{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config_file import config\n",
    "import sys\n",
    "sys.path.append(\"/export/home/nidebroux/sct_custom\")\n",
    "sys.path.append(\"/export/home/nidebroux/modules\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '/gpu:0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from spinalcordtoolbox.image import Image\n",
    "from sklearn.utils import shuffle\n",
    "import tkinter\n",
    "import matplotlib\n",
    "#comment the line below if necessary\n",
    "matplotlib.use('TkAgg')\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from spinalcordtoolbox.deepseg_sc.cnn_models_3d import load_trained_model\n",
    "from generator import get_training_and_validation_generators\n",
    "from utils import fetch_data_files, visualize_data, normalize_data, load_3Dpatches, train_model, get_callbacks\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "main_dir = config[\"main_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PARAMETERS FROM CONFIG FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['data_dict'] = pickle file containing a dictionary with at least the following keys: subject and contrast_foldname\n",
    "# This dict is load as a panda dataframe and used by the function utils.fetch_data_files\n",
    "# IMPORTANT NOTE: the testing dataset is not included in this dataframe\n",
    "DATA_PD = pd.read_pickle(config['data_dict'])\n",
    "\n",
    "DATA_FOLD = config[\"data_dir\"]  # where the preprocess data are stored\n",
    "#MODEL_FOLD = config[\"path2save\"]  # where to store the trained models\n",
    "\n",
    "MEAN_TRAIN_T2, STD_TRAIN_T2 = 871.309, 557.916  # Mean and SD of the training dataset of the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT INPUT IMAGES INTO AN HDF5 FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.8 * len(DATA_PD.index)) # 80% of the dataset is used for the training, 20% for validation\n",
    "idx_train = random.sample(range(len(DATA_PD.index)), len_train)\n",
    "idx_valid = [ii for ii in range(len(DATA_PD.index)) if ii not in idx_train] # the remaining images are used for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_train)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_SEG')\n",
    "validation_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_valid)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_SEG')\n",
    "\n",
    "print(training_files)\n",
    "print(validation_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT 3D PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extracted patches are stored as pickle files (one for training, one for validation).\n",
    "# If these files already exist, we load them directly (i.e. do not re run the patch extraction).\n",
    "pkl_train_fname = DATA_FOLD + 'train_data.pkl'\n",
    "print(pkl_train_fname)\n",
    "if not os.path.isfile(pkl_train_fname):\n",
    "    \n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=config[\"patch_overlap\"]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    with open(pkl_train_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_train, y_train]), fp)\n",
    "else:\n",
    "    with open (pkl_train_fname, 'rb') as fp:\n",
    "        X_train, y_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_valid_fname = DATA_FOLD + 'valid_data.pkl'\n",
    "print(pkl_valid_fname)\n",
    "\n",
    "if not os.path.isfile(pkl_valid_fname):\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    \n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    with open(pkl_valid_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_valid, y_valid]), fp)\n",
    "else:\n",
    "    with open (pkl_valid_fname, 'rb') as fp:\n",
    "        X_valid, y_valid = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of Training patches:\\n\\t' + str(X_train.shape[0]))\n",
    "print('Number of Validation patches:\\n\\t' + str(X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "loss =model.loss\n",
    "optimizer_class_name = model.optimizer.__class__.__name__\n",
    "optimizer_config = model.optimizer.get_config()\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET TRAINING AND VALIDATION GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=config[\"batch_size\"],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "print(train_generator,nb_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "print(validation_generator,nb_valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN FINE-TUNING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning technique and parameters chosen here are based on the results obtained trough the tests below. It depends directly from the training set and should thus be change in function of the used training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "\n",
    "history = train_model(model=model,\n",
    "    path2save=config[\"path2save\"],\n",
    "    model_name=config[\"model_name\"],\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=2*config[\"n_epochs_1\"],\n",
    "    learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "    learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "   )\n",
    "\n",
    "np.save(main_dir + \"parameters_analysis/best_results.npy\",history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the script contains a serie of tests made on the following parameters : depth of the network surgery in case of transfer learning, amount of frozen parameters during unfreezing phase, batch size, overlap size, learning rate (for transfer learning and for unfreezing phase) and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK SURGERY \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here what is the best amount of layers to retrain from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery from conv3d_5\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery from conv3d_6\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "x_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'new_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-6].output)\n",
    "x_2 = keras.layers.BatchNormalization(name = 'new_batch_1')(x_1)\n",
    "x_3 = keras.layers.Activation('relu',name = 'new_activation_1')(x_2)\n",
    "x_4 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'new_conv3D_2',kernel_initializer = init)(x_3)\n",
    "new_output = keras.layers.Activation('sigmoid',name = 'new_activation_2')(x_4)\n",
    "\n",
    "middle_model = keras.models.Model(model.input,new_output,name = \"middle_surgery_model\")\n",
    "\n",
    "for layer in middle_model.layers[:-6]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "middle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(middle_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery from conv3d_7\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "short_1 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'short_conv3D',kernel_initializer = init)(model.layers[-3].output)\n",
    "short_output = keras.layers.Activation('sigmoid',name = 'short_activation')(short_1)\n",
    "\n",
    "short_model = keras.models.Model(model.input,short_output,name = \"short_surgery_model\")\n",
    "\n",
    "for layer in short_model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "short_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(short_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the long model\n",
    "long_history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"long_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/long_surgery.npy\",long_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the middle model\n",
    "middle_history = train_model(model=middle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"middle_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/middle_surgery.npy\",middle_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the short model\n",
    "short_history = train_model(model=short_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"short_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/short_surgery.npy\",short_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone of the model without training\n",
    "for layer in middle_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "clone_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "   \n",
    "clone_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(clone_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_history = train_model(model=clone_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"total_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/clone.npy\",clone_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_surgery = np.load(main_dir+\"parameters_analysis/long_surgery.npy\")\n",
    "middle_surgery = np.load(main_dir+\"parameters_analysis/middle_surgery.npy\")\n",
    "short_surgery = np.load(main_dir+\"parameters_analysis/short_surgery.npy\")\n",
    "clone = np.load(main_dir+\"parameters_analysis/clone.npy\")\n",
    "\n",
    "epochs = range(1,101,4)\n",
    "mean_val_loss = np.zeros((4,25))\n",
    "\n",
    "for i in range(25):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(long_surgery[i:i+4])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(middle_surgery[i:i+4]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(short_surgery[i:i+4]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(clone[i:i+4]))\n",
    "\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'val_loss for big surgery')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'val_loss for middle surgery')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'val_loss for short surgery')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'val_loss for total replacement')\n",
    "plt.title('Comparison between different surgeries ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 4 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH SIZE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = [1,5,10,20,50]\n",
    "val_loss_values = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(5):\n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=batch_size_train[i],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"batch_size_test\",str(nb_train_steps)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_values[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir + \"parameters_analysis/batch_size.npy\",val_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(5,51,5)\n",
    "val_loss_values = np.load(main_dir + \"parameters_analysis/batch_size.npy\")\n",
    "mean_val_loss = np.zeros((5,10))\n",
    "for i in range(10):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_values[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_values[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_values[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_values[3,i:i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(val_loss_values[4,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'batch size = 1 - iteration = 271')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'batch size = 5 - iteration = 54')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'batch size = 10 - iteration = 27')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'batch size = 20 - iteration = 13')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'batch size = 50 - iteration = 5')\n",
    "plt.title('Evolution of the validation loss with different batch sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERLAP TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_array = [None,42,36,24,12]\n",
    "val_loss_overlap = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=overlap_array[i]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"overlap_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_overlap[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir + \"parameters_analysis/overlap.npy\",val_loss_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_overlap = np.load(main_dir+\"parameters_analysis/overlap.npy\")\n",
    "epochs = range(5,51,5)\n",
    "mean_val_loss = np.zeros((5,10))\n",
    "for i in range(10):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_overlap[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_overlap[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_overlap[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_overlap[3,i:i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(val_loss_overlap[4,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'No overlap (= 48)')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'overlap of 42')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'overlap of 36')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'overlap of 24')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'overlap of 12')\n",
    "plt.title('Evolution of the validation loss with different overlap sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING RATE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_array = [0.1,0.01,0.001,0.0001]\n",
    "val_loss_learning = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,learning_array[i])\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"learning_rate_test\",str(learning_array[i])]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_learning[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/learning_rate.npy\",val_loss_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_learning = np.load(main_dir+\"parameters_analysis/learning_rate.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((4,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_learning[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_learning[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_learning[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_learning[3,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'learning rate = 0.1')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'learning rate = 0.01')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'learning rate = 0.001')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'learning rate = 0.0001')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different learning rates ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNFREEZING TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here the amount of layers that we keep frozen during unfreezing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [0,6,10]\n",
    "val_loss_unfreeze = np.zeros((3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=long_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='pretraining_unfreeze_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "\n",
    "for i in range(3):\n",
    "    pretrained_model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/transferLearned_models/best_pretraining_unfreeze_test.h5')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = True\n",
    "    for layer in pretrained_model.layers[:depth[i]]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    pretrained_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(pretrained_model.optimizer.learning_rate,0.00001)\n",
    "    print(pretrained_model.summary())\n",
    "    \n",
    "    history = train_model(model=pretrained_model,\n",
    "        path2save=config[\"path2save_finetuned\"],\n",
    "        model_name=_.join(['unfreeze_test_depth',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_unfreeze[i,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/unfreeze_2.npy\",val_loss_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_unfreeze = np.load(main_dir+\"parameters_analysis/unfreeze_2.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((3,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_unfreeze[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_unfreeze[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_unfreeze[2,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'frozen parameters = 0')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'frozen parameters = 32 112')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'frozen parameters = 94 560')\n",
    "\n",
    "plt.title('Evolution of the validation loss for different unfreezing possibilities')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNFREEZING --> LEARNING RATE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_array = [0.0001,0.00001,0.000001]\n",
    "val_loss_learning = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=long_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='pretraining_unfreeze_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "val_loss_learning[0,:] = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    pretrained_model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/transferLearned_models/best_pretraining_unfreeze_test.h5')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    pretrained_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(pretrained_model.optimizer.learning_rate,learning_array[i])\n",
    "    \n",
    "    history = train_model(model=pretrained_model,\n",
    "        path2save=config[\"path2save_finetuned\"],\n",
    "        model_name=_.join(['unfreeze_test_learning',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_learning[i+1,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/unfreeze_learning_rate.npy\",val_loss_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_learning = np.load(main_dir+\"parameters_analysis/unfreeze_learning_rate.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((4,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_learning[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_learning[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_learning[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_learning[3,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'First training results (before unfreezing)')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'learning rate = 0.0001')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'learning rate = 0.00001')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'learning rate = 0.000001')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different learning rates during unfreezing phase ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIRECT RETRAINING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here which layers we freeze in case of direct retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [0,6,10,16]\n",
    "val_loss_unfreeze = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    for layer in model.layers[:depth[i]]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "    print(model.summary())\n",
    "    \n",
    "    history = train_model(model=model,\n",
    "        path2save=config[\"path2save_retrained\"],\n",
    "        model_name=_.join(['retrained',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_unfreeze[i,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/retraining.npy\",val_loss_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_unfreeze = np.load(main_dir+\"parameters_analysis/retraining.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((4,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_unfreeze[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_unfreeze[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_unfreeze[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_unfreeze[3,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'frozen parameters = 0')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'frozen parameters = 32 112')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'frozen parameters = 94 560')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'frozen parameters = 219 024')\n",
    "\n",
    "plt.title('Evolution of the validation loss for different unfreezing possibilities starting from the original model')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_array = [False,True]\n",
    "flip_array = [False,True]\n",
    "val_loss_augment = np.zeros((2,70))\n",
    "train_loss_augment = np.zeros((2,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=16) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=augment_array[i],\n",
    "                                                    augment_flip=flip_array[i])\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_retraining\"],\n",
    "            model_name='_'.join([\"augment_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=70,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_augment[i,:] = history.history['val_loss']\n",
    "    train_loss_augment[i,:] = history.history['loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/augment_val.npy\",val_loss_augment)\n",
    "np.save(main_dir+\"parameters_analysis/augment_train.npy\",train_loss_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_augment = np.load(main_dir+\"parameters_analysis/augment_val.npy\")\n",
    "loss_augment = np.load(main_dir+\"parameters_analysis/augment_train.npy\")\n",
    "epochs = range(5,71,5)\n",
    "mean_val_loss = np.zeros((2,14))\n",
    "mean_loss = np.zeros((2,14))\n",
    "for i in range(14):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_augment[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_augment[1,i:i+5]))\n",
    "    mean_loss[0,i]=np.abs(np.mean(loss_augment[0,i:i+5]))\n",
    "    mean_loss[1,i]=np.abs(np.mean(loss_augment[1,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'validation loss without data augmentation')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'validation loss with data augmentation')\n",
    "plt.plot(epochs,mean_loss[0,:],'o--',label = 'train loss without data augmentation')\n",
    "plt.plot(epochs,mean_loss[1,:],'o--',label = 'train loss with data augmentation')\n",
    "\n",
    "plt.title('Evolution of the validation and train loss with and without data augmentation')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation/train loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND OVERLAP TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_array = [12,6,3]\n",
    "val_loss_overlap = np.zeros((3,50))\n",
    "loss_overlap = np.zeros((3,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=overlap_array[i]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"second_overlap_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_overlap[i,:] = history.history['val_loss']\n",
    "    loss_overlap[i,:] = history.history['loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/second_overlap.npy\",val_loss_overlap)\n",
    "np.save(main_dir+\"parameters_analysis/second_overlap_train.npy\",loss_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_overlap = np.load(main_dir+\"parameters_analysis/second_overlap.npy\")\n",
    "epochs = range(5,51,5)\n",
    "mean_val_loss = np.zeros((3,10))\n",
    "for i in range(10):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_overlap[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_overlap[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_overlap[2,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'overlap of 12')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'overlap of 6')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'overlap of 3')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different overlap sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFER LEARNING + UNFREEZING VS DIRECT RETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.zeros((2,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=6) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=long_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='final_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=75,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "\n",
    "val_loss[0,:75] = history.history['val_loss']\n",
    "\n",
    "for layer in long_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "K.set_value(long_model.optimizer.learning_rate,0.00001)\n",
    "\n",
    "history = train_model(model=long_model,\n",
    "    path2save=config[\"path2save_finetuned\"],\n",
    "    model_name=\"final_test\",\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=75,\n",
    "    learning_rate_drop=0.5,\n",
    "    learning_rate_patience=10\n",
    "   )\n",
    "val_loss[0,75:] = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "print(model.summary())\n",
    "\n",
    "history = train_model(model=model,\n",
    "    path2save=config[\"path2save_retrained\"],\n",
    "    model_name=\"final_test\",\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=150,\n",
    "    learning_rate_drop=0.5,\n",
    "    learning_rate_patience=10\n",
    "   )\n",
    "val_loss[1,:] = history.history['val_loss']\n",
    "\n",
    "np.save(main_dir+\"parameters_analysis/final_test.npy\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.load(main_dir+\"parameters_analysis/final_test.npy\")\n",
    "epochs = range(5,151,5)\n",
    "mean_val_loss = np.zeros((2,30))\n",
    "for i in range(30):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss[1,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'transfer learning')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'no  transfer learning')\n",
    "\n",
    "plt.title('Evolution of the validation loss with or without transfer learning ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
