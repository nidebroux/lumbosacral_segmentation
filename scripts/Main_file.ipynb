{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config_file import config\n",
    "import sys\n",
    "sys.path.append(\"/export/home/nidebroux/sct_custom\")\n",
    "sys.path.append(\"/export/home/nidebroux/modules\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '/gpu:0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from spinalcordtoolbox.image import Image\n",
    "from sklearn.utils import shuffle\n",
    "import tkinter\n",
    "import matplotlib\n",
    "#comment the line below if necessary\n",
    "matplotlib.use('TkAgg')\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from spinalcordtoolbox.deepseg_sc.cnn_models_3d import load_trained_model\n",
    "from generator import get_training_and_validation_generators\n",
    "from utils import fetch_data_files, visualize_data, normalize_data, load_3Dpatches, train_model, get_callbacks\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "main_dir = config[\"main_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PARAMETERS FROM CONFIG FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['data_dict'] = pickle file containing a dictionary with at least the following keys: subject and contrast_foldname\n",
    "# This dict is load as a panda dataframe and used by the function utils.fetch_data_files\n",
    "# IMPORTANT NOTE: the testing dataset is not included in this dataframe\n",
    "DATA_PD = pd.read_pickle(config['data_dict'])\n",
    "\n",
    "DATA_FOLD = config[\"data_dir\"]  # where the preprocess data are stored\n",
    "#MODEL_FOLD = config[\"path2save\"]  # where to store the trained models\n",
    "\n",
    "MEAN_TRAIN_T2, STD_TRAIN_T2 = 871.309, 557.916  # Mean and SD of the training dataset of the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT INPUT IMAGES INTO AN HDF5 FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.8 * len(DATA_PD.index)) # 80% of the dataset is used for the training, 20% for validation\n",
    "idx_train = random.sample(range(len(DATA_PD.index)), len_train)\n",
    "idx_valid = [ii for ii in range(len(DATA_PD.index)) if ii not in idx_train] # the remaining images are used for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_train)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_SEG')\n",
    "validation_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_valid)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_SEG')\n",
    "\n",
    "print(training_files)\n",
    "print(validation_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT 3D PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extracted patches are stored as pickle files (one for training, one for validation).\n",
    "# If these files already exist, we load them directly (i.e. do not re run the patch extraction).\n",
    "pkl_train_fname = DATA_FOLD + 'train_data.pkl'\n",
    "print(pkl_train_fname)\n",
    "if not os.path.isfile(pkl_train_fname):\n",
    "    \n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=config[\"patch_overlap\"]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    with open(pkl_train_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_train, y_train]), fp)\n",
    "else:\n",
    "    with open (pkl_train_fname, 'rb') as fp:\n",
    "        X_train, y_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pkl_valid_fname = DATA_FOLD + 'valid_data.pkl'\n",
    "print(pkl_valid_fname)\n",
    "\n",
    "if not os.path.isfile(pkl_valid_fname):\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    \n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    with open(pkl_valid_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_valid, y_valid]), fp)\n",
    "else:\n",
    "    with open (pkl_valid_fname, 'rb') as fp:\n",
    "        X_valid, y_valid = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of Training patches:\\n\\t' + str(X_train.shape[0]))\n",
    "print('Number of Validation patches:\\n\\t' + str(X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "loss =model.loss\n",
    "optimizer_class_name = model.optimizer.__class__.__name__\n",
    "optimizer_config = model.optimizer.get_config()\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET TRAINING AND VALIDATION GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=config[\"batch_size\"],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "print(train_generator,nb_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "print(validation_generator,nb_valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN FINE-TUNING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning technique and parameters chosen here are based on the results obtained trough the tests below. It depends directly from the training set and should thus be change in function of the used training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "history = train_model(model=model,\n",
    "    path2save=config[\"path2save\"],\n",
    "    model_name=config[\"model_name\"],\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=2*config[\"n_epochs_1\"],\n",
    "    learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "    learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "   )\n",
    "\n",
    "np.save(main_dir + \"parameters_analysis/best_results.npy\",history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the script contains a serie of tests made on the following parameters : depth of the network surgery in case of transfer learning, amount of frozen parameters during unfreezing phase, batch size, overlap size, learning rate (for transfer learning and for unfreezing phase) and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK SURGERY \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here which layers we should replace in a network surgery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery for contraction path \n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "contr_1 = keras.layers.Conv3D(24,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'contr_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[0].output)\n",
    "contr_2 = keras.layers.BatchNormalization(name = 'contr_batch_1', axis = 1)(contr_1)\n",
    "contr_3 = keras.layers.Activation('relu',name = 'contr_activation_1')(contr_2)\n",
    "contr_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'contr_conv3D_2',padding = 'same',kernel_initializer = init)(contr_3)\n",
    "contr_5 = keras.layers.BatchNormalization(name = 'contr_batch_2',axis=1)(contr_4)\n",
    "for i in range(6,24):\n",
    "    if i ==6 :\n",
    "        contr_6 = model.layers[i](contr_5)\n",
    "    elif i == 7:\n",
    "        l= model.layers[i](contr_6)\n",
    "    elif i == 15 :\n",
    "        l = model.layers[i]([contr_6,l])\n",
    "    else :\n",
    "        l= model.layers[i](l)\n",
    "    \n",
    "contr_model = keras.models.Model(model.input,l,name = \"contraction_model\")\n",
    "\n",
    "for layer in contr_model.layers[6:24]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "contr_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(contr_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery for expansion path\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1',axis=1)(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2',axis=1)(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery with both expansion and contraction\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "contr_1 = keras.layers.Conv3D(24,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'contr_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[0].output)\n",
    "contr_2 = keras.layers.BatchNormalization(name = 'contr_batch_1', axis = 1)(contr_1)\n",
    "contr_3 = keras.layers.Activation('relu',name = 'contr_activation_1')(contr_2)\n",
    "contr_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'contr_conv3D_2',padding = 'same',kernel_initializer = init)(contr_3)\n",
    "contr_5 = keras.layers.BatchNormalization(name = 'contr_batch_2',axis=1)(contr_4)\n",
    "for i in range(6,16):\n",
    "    if i ==6 :\n",
    "        contr_6 = model.layers[i](contr_5)\n",
    "    elif i == 7:\n",
    "        l= model.layers[i](contr_6)\n",
    "    elif i == 15 :\n",
    "        l = model.layers[i]([contr_6,l])\n",
    "    else :\n",
    "        l= model.layers[i](l)\n",
    "        \n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(l)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1',axis=1)(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2',axis=1)(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "    \n",
    "double_model = keras.models.Model(model.input,long_output,name = \"contraction_and_expansion_model\")\n",
    "\n",
    "for layer in double_model.layers[6:16]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "double_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(double_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery of the bottleneck\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "for i in range(14,24):\n",
    "    if i ==14 :\n",
    "        l = model.layers[i](bottle_6)\n",
    "    elif i == 15 :\n",
    "        l = model.layers[i]([model.layers[6].output,l])\n",
    "    else :\n",
    "        l= model.layers[i](l)\n",
    "        \n",
    "bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "i = 0\n",
    "for layer in bottle_model.layers:\n",
    "    if i > 7 and i < 14:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    i = i+1\n",
    "    \n",
    "bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(bottle_model.optimizer.learning_rate,0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the expansion model\n",
    "long_history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"expansion_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/expansion_surgery.npy\",long_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the contraction model\n",
    "contr_history = train_model(model=contr_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"contr_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/contr_surgery.npy\",contr_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the double model\n",
    "double_history = train_model(model=double_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"double_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/double_surgery.npy\",double_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the bottle model\n",
    "bottle_history = train_model(model=bottle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"bottle_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/bottle_surgery.npy\",bottle_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone of the model without training\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "clone_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "   \n",
    "clone_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(clone_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clone_history = train_model(model=clone_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"total_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/clone.npy\",clone_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_surgery = np.load(main_dir+\"parameters_analysis/expansion_surgery.npy\")\n",
    "contr_surgery = np.load(main_dir+\"parameters_analysis/contr_surgery.npy\")\n",
    "double_surgery = np.load(main_dir+\"parameters_analysis/double_surgery.npy\")\n",
    "bottle_surgery = np.load(main_dir+\"parameters_analysis/bottle_surgery.npy\")\n",
    "clone = np.load(main_dir+\"parameters_analysis/clone.npy\")\n",
    "\n",
    "epochs = range(25,101,5)\n",
    "mean_val_loss = np.zeros((5,16))\n",
    "\n",
    "for i in range(16):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(long_surgery[20+5*i:20+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(contr_surgery[20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(double_surgery[20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(bottle_surgery[20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(clone[20+5*i:20+5*i+5]))\n",
    "\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'val_loss for expansion surgery')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'val_loss for contraction surgery')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'val_loss for contraction and expansion surgery')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'val_loss for bottleneck surgery')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'val_loss for total replacement')\n",
    "plt.title('Comparison between different network surgeries ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH SIZE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = [1,5,10,20,50]\n",
    "val_loss_values = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(5):\n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=batch_size_train[i],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "    bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "    bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "    bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "    bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "    bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "    for i in range(14,24):\n",
    "        if i ==14 :\n",
    "            l = model.layers[i](bottle_6)\n",
    "        elif i == 15 :\n",
    "            l = model.layers[i]([model.layers[6].output,l])\n",
    "        else :\n",
    "            l= model.layers[i](l)\n",
    "\n",
    "    bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "    i = 0\n",
    "    for layer in bottle_model.layers:\n",
    "        if i > 7 and i < 14:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "        i = i+1\n",
    "\n",
    "    bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "    #fit the bottle model\n",
    "    history = train_model(model=bottle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"batch_size_test\",str(nb_train_steps)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_values[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir + \"parameters_analysis/batch_size.npy\",val_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(20,51,5)\n",
    "val_loss_values = np.load(main_dir + \"parameters_analysis/batch_size.npy\")\n",
    "mean_val_loss = np.zeros((5,7))\n",
    "for i in range(7):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_values[0,15+5*i:15+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_values[1,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_values[2,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_values[3,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(val_loss_values[4,15+5*i:15+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'batch size = 1 - iteration = 271')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'batch size = 5 - iteration = 54')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'batch size = 10 - iteration = 27')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'batch size = 20 - iteration = 13')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'batch size = 50 - iteration = 5')\n",
    "plt.title('Evolution of the validation loss with different batch sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERLAP TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_array = [None,42,36,24,12]\n",
    "val_loss_overlap = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=overlap_array[i]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "    bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "    bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "    bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "    bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "    bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "    for i in range(14,24):\n",
    "        if i ==14 :\n",
    "            l = model.layers[i](bottle_6)\n",
    "        elif i == 15 :\n",
    "            l = model.layers[i]([model.layers[6].output,l])\n",
    "        else :\n",
    "            l= model.layers[i](l)\n",
    "\n",
    "    bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "    i = 0\n",
    "    for layer in bottle_model.layers:\n",
    "        if i > 7 and i < 14:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "        i = i+1\n",
    "\n",
    "    bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "    #fit the bottle model\n",
    "    history = train_model(model=bottle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"overlap_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_overlap[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir + \"parameters_analysis/overlap.npy\",val_loss_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_overlap = np.load(main_dir+\"parameters_analysis/overlap.npy\")\n",
    "epochs = range(20,51,5)\n",
    "mean_val_loss = np.zeros((5,7))\n",
    "for i in range(7):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_overlap[0,15+5*i:15+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_overlap[1,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_overlap[2,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_overlap[3,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(val_loss_overlap[4,15+5*i:15+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'No overlap (= 48)')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'overlap of 42')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'overlap of 36')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'overlap of 24')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'overlap of 12')\n",
    "plt.title('Evolution of the validation loss with different overlap sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING RATE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_array = [0.1,0.01,0.001,0.0001]\n",
    "val_loss_learning = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "\n",
    "\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "    bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "    bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "    bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "    bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "    bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "    for i in range(14,24):\n",
    "        if i ==14 :\n",
    "            l = model.layers[i](bottle_6)\n",
    "        elif i == 15 :\n",
    "            l = model.layers[i]([model.layers[6].output,l])\n",
    "        else :\n",
    "            l= model.layers[i](l)\n",
    "\n",
    "    bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "    i = 0\n",
    "    for layer in bottle_model.layers:\n",
    "        if i > 7 and i < 14:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "        i = i+1\n",
    "\n",
    "    bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "    #fit the bottle model\n",
    "    history = train_model(model=bottle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"learning_rate_test\",str(learning_array[i])]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_learning[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/learning_rate.npy\",val_loss_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_learning = np.load(main_dir+\"parameters_analysis/learning_rate.npy\")\n",
    "epochs = range(25,101,5)\n",
    "mean_val_loss = np.zeros((4,16))\n",
    "for i in range(16):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_learning[0,20+5*i:20+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_learning[1,20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_learning[2,20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_learning[3,20+5*i:20+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'learning rate = 0.1')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'learning rate = 0.01')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'learning rate = 0.001')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'learning rate = 0.0001')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different learning rates ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNFREEZING TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here which layers we should kept frozen during unfreezing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loss_unfreeze = np.zeros((3,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "for i in range(14,24):\n",
    "    if i ==14 :\n",
    "        l = model.layers[i](bottle_6)\n",
    "    elif i == 15 :\n",
    "        l = model.layers[i]([model.layers[6].output,l])\n",
    "    else :\n",
    "        l= model.layers[i](l)\n",
    "        \n",
    "bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "i = 0\n",
    "for layer in bottle_model.layers:\n",
    "    if i > 7 and i < 14:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    i = i+1\n",
    "    \n",
    "bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=bottle_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='pretraining_unfreeze_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=50,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "\n",
    "for i in range(3):\n",
    "    pretrained_model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/transferLearned_models/best_pretraining_unfreeze_test.h5')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = True\n",
    "    if i==1 : #freeze contraction\n",
    "        for layer in pretrained_model.layers[:7]:\n",
    "            layer.trainable = False\n",
    "    elif i==2 : #freeze expansion\n",
    "        for layer in pretrained_model.layers[14:]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    pretrained_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(pretrained_model.optimizer.learning_rate,0.00001)\n",
    "    print(pretrained_model.summary())\n",
    "    \n",
    "    history = train_model(model=pretrained_model,\n",
    "        path2save=config[\"path2save_finetuned\"],\n",
    "        model_name=_.join(['unfreeze_test_depth',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=50,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_unfreeze[i,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/unfreeze_2.npy\",val_loss_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_unfreeze = np.load(main_dir+\"parameters_analysis/unfreeze_2.npy\")\n",
    "epochs = range(15,51,5)\n",
    "mean_val_loss = np.zeros((3,8))\n",
    "for i in range(8):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_unfreeze[0,10+5*i:10+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_unfreeze[1,10+5*i:10+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_unfreeze[2,10+5*i:10+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'No frozen layers')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'Contraction section frozen')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'Expansion section frozen')\n",
    "\n",
    "plt.title('Evolution of the validation loss for different unfreezing possibilities')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNFREEZING --> LEARNING RATE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_array = [0.0001,0.00001,0.000001]\n",
    "val_loss_learning = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "for i in range(14,24):\n",
    "    if i ==14 :\n",
    "        l = model.layers[i](bottle_6)\n",
    "    elif i == 15 :\n",
    "        l = model.layers[i]([model.layers[6].output,l])\n",
    "    else :\n",
    "        l= model.layers[i](l)\n",
    "        \n",
    "bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "i = 0\n",
    "for layer in bottle_model.layers:\n",
    "    if i > 7 and i < 14:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    i = i+1\n",
    "    \n",
    "bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "#fit the bottle model\n",
    "history = train_model(model=bottle_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='pretraining_unfreeze_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "val_loss_learning[0,:] = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    pretrained_model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/transferLearned_models/best_pretraining_unfreeze_test.h5')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    pretrained_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(pretrained_model.optimizer.learning_rate,learning_array[i])\n",
    "    \n",
    "    history = train_model(model=pretrained_model,\n",
    "        path2save=config[\"path2save_finetuned\"],\n",
    "        model_name=_.join(['unfreeze_test_learning',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_learning[i+1,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/unfreeze_learning_rate.npy\",val_loss_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_learning = np.load(main_dir+\"parameters_analysis/unfreeze_learning_rate.npy\")\n",
    "epochs = range(25,101,5)\n",
    "mean_val_loss = np.zeros((4,16))\n",
    "for i in range(16):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_learning[0,20+5*i:20+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_learning[1,20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_learning[2,20+5*i:20+5*i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_learning[3,20+5*i:20+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'First training results (before unfreezing)')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'learning rate = $10^{-4}$')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'learning rate = $10^{-5}$')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'learning rate = $10^{-6}$')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different learning rates during unfreezing phase ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_array = [False,True]\n",
    "flip_array = [False,True]\n",
    "val_loss_augment = np.zeros((2,70))\n",
    "train_loss_augment = np.zeros((2,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=16) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=augment_array[i],\n",
    "                                                    augment_flip=flip_array[i])\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "    bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "    bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "    bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "    bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "    bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "    for i in range(14,24):\n",
    "        if i ==14 :\n",
    "            l = model.layers[i](bottle_6)\n",
    "        elif i == 15 :\n",
    "            l = model.layers[i]([model.layers[6].output,l])\n",
    "        else :\n",
    "            l= model.layers[i](l)\n",
    "\n",
    "    bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "    i = 0\n",
    "    for layer in bottle_model.layers:\n",
    "        if i > 7 and i < 14:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "        i = i+1\n",
    "\n",
    "    bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "    #fit the bottle model\n",
    "    history = train_model(model=bottle_model,\n",
    "            path2save=config[\"path2save_retraining\"],\n",
    "            model_name='_'.join([\"augment_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=70,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_augment[i,:] = history.history['val_loss']\n",
    "    train_loss_augment[i,:] = history.history['loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/augment_val.npy\",val_loss_augment)\n",
    "np.save(main_dir+\"parameters_analysis/augment_train.npy\",train_loss_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_augment = np.load(main_dir+\"parameters_analysis/augment_val.npy\")\n",
    "loss_augment = np.load(main_dir+\"parameters_analysis/augment_train.npy\")\n",
    "epochs = range(20,71,5)\n",
    "mean_val_loss = np.zeros((2,11))\n",
    "mean_loss = np.zeros((2,11))\n",
    "for i in range(11):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_augment[0,15+5*i:15+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_augment[1,15+5*i:15+5*i+5]))\n",
    "    mean_loss[0,i]=np.abs(np.mean(loss_augment[0,15+5*i:15+5*i+5]))\n",
    "    mean_loss[1,i]=np.abs(np.mean(loss_augment[1,15+5*i:15+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'validation loss without data augmentation')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'validation loss with data augmentation')\n",
    "plt.plot(epochs,mean_loss[0,:],'o--',label = 'training loss without data augmentation')\n",
    "plt.plot(epochs,mean_loss[1,:],'o--',label = 'training loss with data augmentation')\n",
    "\n",
    "plt.title('Evolution of the validation and train loss with and without data augmentation')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation/train loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND OVERLAP TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_array = [12,6,3]\n",
    "val_loss_overlap = np.zeros((3,50))\n",
    "loss_overlap = np.zeros((3,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=overlap_array[i]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "    bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "    bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "    bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "    bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "    bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "    for i in range(14,24):\n",
    "        if i ==14 :\n",
    "            l = model.layers[i](bottle_6)\n",
    "        elif i == 15 :\n",
    "            l = model.layers[i]([model.layers[6].output,l])\n",
    "        else :\n",
    "            l= model.layers[i](l)\n",
    "\n",
    "    bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "    i = 0\n",
    "    for layer in bottle_model.layers:\n",
    "        if i > 7 and i < 14:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "        i = i+1\n",
    "\n",
    "    bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "    #fit the bottle model\n",
    "    history = train_model(model=bottle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"second_overlap_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_overlap[i,:] = history.history['val_loss']\n",
    "    loss_overlap[i,:] = history.history['loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/second_overlap.npy\",val_loss_overlap)\n",
    "np.save(main_dir+\"parameters_analysis/second_overlap_train.npy\",loss_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_overlap = np.load(main_dir+\"parameters_analysis/second_overlap.npy\")\n",
    "epochs = range(20,51,5)\n",
    "mean_val_loss = np.zeros((3,7))\n",
    "for i in range(7):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_overlap[0,15+5*i:15+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_overlap[1,15+5*i:15+5*i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_overlap[2,15+5*i:15+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'overlap of 12')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'overlap of 6')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'overlap of 3')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different overlap sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPACT OF THE NETWORK SURGERY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe here the interest of the network surgery in our fine-tuning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.zeros((2,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=6) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=1,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "bottle_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[7].output)\n",
    "bottle_2 = keras.layers.BatchNormalization(name = 'bottle_batch_1',axis=1)(bottle_1)\n",
    "bottle_3 = keras.layers.Activation('relu',name = 'bottle_activation_1')(bottle_2)\n",
    "bottle_4 = keras.layers.Conv3D(96,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'bottle_conv3D_2',padding = 'same',kernel_initializer = init)(bottle_3)\n",
    "bottle_5 = keras.layers.BatchNormalization(name = 'bottle_batch_2',axis = 1)(bottle_4)\n",
    "bottle_6 = keras.layers.Activation('relu',name = 'bottle_activation_2')(bottle_5)\n",
    "\n",
    "for i in range(14,24):\n",
    "    if i ==14 :\n",
    "        l = model.layers[i](bottle_6)\n",
    "    elif i == 15 :\n",
    "        l = model.layers[i]([model.layers[6].output,l])\n",
    "    else :\n",
    "        l= model.layers[i](l)\n",
    "        \n",
    "bottle_model = keras.models.Model(model.input,l,name = \"bottle_model\")\n",
    "\n",
    "i = 0\n",
    "for layer in bottle_model.layers:\n",
    "    if i > 7 and i < 14:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "    i = i+1\n",
    "    \n",
    "bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(bottle_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "\n",
    "#fit the bottle model\n",
    "history = train_model(model=bottle_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='final_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=75,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "\n",
    "val_loss[0,:75] = history.history['val_loss']\n",
    "\n",
    "for layer in bottle_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for layer in bottle_model.layers[:7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "bottle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "K.set_value(bottle_model.optimizer.learning_rate,0.00001)\n",
    "\n",
    "history = train_model(model=bottle_model,\n",
    "    path2save=config[\"path2save_finetuned\"],\n",
    "    model_name=\"final_test\",\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=75,\n",
    "    learning_rate_drop=0.5,\n",
    "    learning_rate_patience=10\n",
    "   )\n",
    "val_loss[0,75:] = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for layer in model.layers[:7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "print(model.summary())\n",
    "\n",
    "history = train_model(model=model,\n",
    "    path2save=config[\"path2save_retrained\"],\n",
    "    model_name=\"final_test\",\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=150,\n",
    "    learning_rate_drop=0.5,\n",
    "    learning_rate_patience=10\n",
    "   )\n",
    "val_loss[1,:] = history.history['val_loss']\n",
    "\n",
    "np.save(main_dir+\"parameters_analysis/final_test.npy\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.load(main_dir+\"parameters_analysis/final_test.npy\")\n",
    "epochs = range(30,151,5)\n",
    "mean_val_loss = np.zeros((2,25))\n",
    "for i in range(25):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss[0,25+5*i:25+5*i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss[1,25+5*i:25+5*i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'Network surgery')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'No  network surgery')\n",
    "\n",
    "plt.title('Evolution of the validation loss with or without network surgery ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
