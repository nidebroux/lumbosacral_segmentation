{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config_file import config\n",
    "import sys\n",
    "sys.path.append(\"/export/home/nidebroux/sct_custom\")\n",
    "sys.path.append(\"/export/home/nidebroux/modules\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '/gpu:0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from spinalcordtoolbox.image import Image\n",
    "from sklearn.utils import shuffle\n",
    "import tkinter\n",
    "import matplotlib\n",
    "#comment the line below if necessary\n",
    "matplotlib.use('TkAgg')\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from spinalcordtoolbox.deepseg_sc.cnn_models_3d import load_trained_model\n",
    "from generator import get_training_and_validation_generators\n",
    "from utils import fetch_data_files, visualize_data, normalize_data, load_3Dpatches, train_model, get_callbacks\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "main_dir = config[\"main_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PARAMETERS FROM CONFIG FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['data_dict'] = pickle file containing a dictionary with at least the following keys: subject and contrast_foldname\n",
    "# This dict is load as a panda dataframe and used by the function utils.fetch_data_files\n",
    "# IMPORTANT NOTE: the testing dataset is not included in this dataframe\n",
    "DATA_PD = pd.read_pickle(config['data_dict'])\n",
    "\n",
    "DATA_FOLD = config[\"data_dir\"]  # where the preprocess data are stored\n",
    "#MODEL_FOLD = config[\"path2save\"]  # where to store the trained models\n",
    "\n",
    "MEAN_TRAIN_T2, STD_TRAIN_T2 = 871.309, 557.916  # Mean and SD of the training dataset of the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT INPUT IMAGES INTO AN HDF5 FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.8 * len(DATA_PD.index)) # 80% of the dataset is used for the training, 20% for validation\n",
    "idx_train = random.sample(range(len(DATA_PD.index)), len_train)\n",
    "idx_valid = [ii for ii in range(len(DATA_PD.index)) if ii not in idx_train] # the remaining images are used for the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_crop_SEG.nii.gz')]\n",
      "[('/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_crop_SEG.nii.gz'), ('/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz', '/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_crop_SEG.nii.gz')]\n"
     ]
    }
   ],
   "source": [
    "training_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_train)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_SEG')\n",
    "validation_files = fetch_data_files(data_frame=DATA_PD[DATA_PD.index.isin(idx_valid)],\n",
    "                                  data_fold=DATA_FOLD,\n",
    "                                  im_suffixe='_norm',\n",
    "                                  target_suffixe='_crop_SEG')\n",
    "\n",
    "print(training_files)\n",
    "print(validation_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT 3D PATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/train_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# The extracted patches are stored as pickle files (one for training, one for validation).\n",
    "# If these files already exist, we load them directly (i.e. do not re run the patch extraction).\n",
    "pkl_train_fname = DATA_FOLD + 'train_data.pkl'\n",
    "print(pkl_train_fname)\n",
    "if not os.path.isfile(pkl_train_fname):\n",
    "    \n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=config[\"patch_overlap\"]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    with open(pkl_train_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_train, y_train]), fp)\n",
    "else:\n",
    "    with open (pkl_train_fname, 'rb') as fp:\n",
    "        X_train, y_train = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/valid_data.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pkl_valid_fname = DATA_FOLD + 'valid_data.pkl'\n",
    "print(pkl_valid_fname)\n",
    "\n",
    "if not os.path.isfile(pkl_valid_fname):\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    \n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    with open(pkl_valid_fname, 'wb') as fp:\n",
    "        pickle.dump(np.array([X_valid, y_valid]), fp)\n",
    "else:\n",
    "    with open (pkl_valid_fname, 'rb') as fp:\n",
    "        X_valid, y_valid = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training patches:\n",
      "\t271\n",
      "Number of Validation patches:\n",
      "\t38\n"
     ]
    }
   ],
   "source": [
    "print('Number of Training patches:\\n\\t' + str(X_train.shape[0]))\n",
    "print('Number of Validation patches:\\n\\t' + str(X_valid.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "loss =model.loss\n",
    "optimizer_class_name = model.optimizer.__class__.__name__\n",
    "optimizer_config = model.optimizer.get_config()\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET TRAINING AND VALIDATION GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object data_generator at 0x7fdd08e817d0> 271\n"
     ]
    }
   ],
   "source": [
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=config[\"batch_size\"],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "print(train_generator,nb_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object data_generator at 0x7fdcf4124250> 38\n"
     ]
    }
   ],
   "source": [
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "print(validation_generator,nb_valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN FINE-TUNING PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning technique and parameters chosen here are based on the results obtained trough the tests below. It depends directly from the training set and should thus be change in function of the used training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "\n",
    "history = train_model(model=model,\n",
    "    path2save=config[\"path2save\"],\n",
    "    model_name=config[\"model_name\"],\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=2*config[\"n_epochs_1\"],\n",
    "    learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "    learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "   )\n",
    "\n",
    "np.save(main_dir + \"parameters_analysis/best_results.npy\",history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the script contains a serie of tests made on the following parameters : depth of the network surgery in case of transfer learning, amount of frozen parameters during unfreezing phase, batch size, overlap size, learning rate (for transfer learning and for unfreezing phase) and data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK SURGERY \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here what is the best amount of layers to retrain from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery from conv3d_5\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery from conv3d_6\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "x_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'new_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-6].output)\n",
    "x_2 = keras.layers.BatchNormalization(name = 'new_batch_1')(x_1)\n",
    "x_3 = keras.layers.Activation('relu',name = 'new_activation_1')(x_2)\n",
    "x_4 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'new_conv3D_2',kernel_initializer = init)(x_3)\n",
    "new_output = keras.layers.Activation('sigmoid',name = 'new_activation_2')(x_4)\n",
    "\n",
    "middle_model = keras.models.Model(model.input,new_output,name = \"middle_surgery_model\")\n",
    "\n",
    "for layer in middle_model.layers[:-6]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "middle_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(middle_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surgery from conv3d_7\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "short_1 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'short_conv3D',kernel_initializer = init)(model.layers[-3].output)\n",
    "short_output = keras.layers.Activation('sigmoid',name = 'short_activation')(short_1)\n",
    "\n",
    "short_model = keras.models.Model(model.input,short_output,name = \"short_surgery_model\")\n",
    "\n",
    "for layer in short_model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "short_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(short_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "271/271 - 17s - loss: -2.5253e-01 - val_loss: -3.8458e-01 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "271/271 - 16s - loss: -3.7342e-01 - val_loss: -4.4275e-01 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "271/271 - 16s - loss: -3.9122e-01 - val_loss: -3.5356e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "271/271 - 17s - loss: -4.2631e-01 - val_loss: -5.0070e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "271/271 - 17s - loss: -4.4790e-01 - val_loss: -4.0220e-01 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "271/271 - 17s - loss: -4.4723e-01 - val_loss: -4.8898e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "271/271 - 17s - loss: -4.6730e-01 - val_loss: -4.9850e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "271/271 - 17s - loss: -4.7749e-01 - val_loss: -5.1019e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "271/271 - 17s - loss: -4.8746e-01 - val_loss: -3.8670e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "271/271 - 17s - loss: -5.0616e-01 - val_loss: -5.0649e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "271/271 - 17s - loss: -5.0503e-01 - val_loss: -4.9474e-01 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "271/271 - 17s - loss: -5.1997e-01 - val_loss: -4.9004e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "271/271 - 17s - loss: -5.2481e-01 - val_loss: -4.8013e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "271/271 - 17s - loss: -5.3415e-01 - val_loss: -5.1947e-01 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "271/271 - 17s - loss: -5.3450e-01 - val_loss: -5.2012e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "271/271 - 17s - loss: -5.3555e-01 - val_loss: -5.5884e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "271/271 - 17s - loss: -5.3673e-01 - val_loss: -5.0507e-01 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "271/271 - 17s - loss: -5.4840e-01 - val_loss: -4.5931e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "271/271 - 17s - loss: -5.5114e-01 - val_loss: -6.0038e-01 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "271/271 - 17s - loss: -5.5106e-01 - val_loss: -5.4477e-01 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "271/271 - 17s - loss: -5.4988e-01 - val_loss: -5.4555e-01 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "271/271 - 17s - loss: -5.6013e-01 - val_loss: -2.8490e-01 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "271/271 - 17s - loss: -5.5988e-01 - val_loss: -5.5940e-01 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "271/271 - 17s - loss: -5.5982e-01 - val_loss: -4.8263e-01 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "271/271 - 17s - loss: -5.6203e-01 - val_loss: -6.2003e-01 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "271/271 - 17s - loss: -5.5998e-01 - val_loss: -4.5263e-01 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "271/271 - 17s - loss: -5.6945e-01 - val_loss: -5.1698e-01 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "271/271 - 17s - loss: -5.7345e-01 - val_loss: -5.5109e-01 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "271/271 - 17s - loss: -5.7350e-01 - val_loss: -6.1104e-01 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "271/271 - 17s - loss: -5.7271e-01 - val_loss: -4.9861e-01 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "271/271 - 17s - loss: -5.7225e-01 - val_loss: -5.9451e-01 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "271/271 - 17s - loss: -5.8061e-01 - val_loss: -5.5855e-01 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "271/271 - 17s - loss: -5.8500e-01 - val_loss: -5.3373e-01 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "271/271 - 17s - loss: -5.8412e-01 - val_loss: -5.0866e-01 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "271/271 - 17s - loss: -5.8073e-01 - val_loss: -5.2785e-01 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "271/271 - 17s - loss: -5.9709e-01 - val_loss: -5.8491e-01 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "271/271 - 17s - loss: -6.0133e-01 - val_loss: -5.3226e-01 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "271/271 - 17s - loss: -5.9972e-01 - val_loss: -5.5656e-01 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "271/271 - 17s - loss: -5.9836e-01 - val_loss: -5.7773e-01 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "271/271 - 17s - loss: -6.0361e-01 - val_loss: -5.9340e-01 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "271/271 - 17s - loss: -6.0466e-01 - val_loss: -5.7481e-01 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "271/271 - 17s - loss: -6.0229e-01 - val_loss: -5.8462e-01 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "271/271 - 17s - loss: -6.0379e-01 - val_loss: -5.1730e-01 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "271/271 - 17s - loss: -6.0219e-01 - val_loss: -6.1311e-01 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "271/271 - 17s - loss: -6.0730e-01 - val_loss: -5.7082e-01 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "271/271 - 17s - loss: -6.1535e-01 - val_loss: -6.2396e-01 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "271/271 - 17s - loss: -6.1587e-01 - val_loss: -5.9859e-01 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "271/271 - 17s - loss: -6.1828e-01 - val_loss: -5.2473e-01 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "271/271 - 17s - loss: -6.1528e-01 - val_loss: -5.9194e-01 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "271/271 - 17s - loss: -6.1931e-01 - val_loss: -5.8528e-01 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "271/271 - 17s - loss: -6.1628e-01 - val_loss: -5.9605e-01 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "271/271 - 17s - loss: -6.1777e-01 - val_loss: -5.4571e-01 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "271/271 - 17s - loss: -6.1940e-01 - val_loss: -6.5630e-01 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "271/271 - 17s - loss: -6.1799e-01 - val_loss: -5.8415e-01 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "271/271 - 17s - loss: -6.1974e-01 - val_loss: -5.4729e-01 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "271/271 - 17s - loss: -6.1985e-01 - val_loss: -6.4316e-01 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "271/271 - 17s - loss: -6.1835e-01 - val_loss: -5.5927e-01 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "271/271 - 17s - loss: -6.2330e-01 - val_loss: -6.1252e-01 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "271/271 - 17s - loss: -6.2301e-01 - val_loss: -5.6648e-01 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "271/271 - 17s - loss: -6.2331e-01 - val_loss: -5.8968e-01 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "271/271 - 17s - loss: -6.2141e-01 - val_loss: -5.7859e-01 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "271/271 - 17s - loss: -6.2265e-01 - val_loss: -6.0617e-01 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "271/271 - 17s - loss: -6.2203e-01 - val_loss: -6.5035e-01 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "271/271 - 17s - loss: -6.2929e-01 - val_loss: -5.7305e-01 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "271/271 - 17s - loss: -6.3123e-01 - val_loss: -6.5448e-01 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "271/271 - 17s - loss: -6.2966e-01 - val_loss: -5.7050e-01 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "271/271 - 17s - loss: -6.3088e-01 - val_loss: -6.2092e-01 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "271/271 - 17s - loss: -6.3017e-01 - val_loss: -5.7227e-01 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "271/271 - 17s - loss: -6.3127e-01 - val_loss: -5.6055e-01 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "271/271 - 17s - loss: -6.3059e-01 - val_loss: -5.9796e-01 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "271/271 - 17s - loss: -6.3020e-01 - val_loss: -6.1121e-01 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "271/271 - 17s - loss: -6.3197e-01 - val_loss: -6.1698e-01 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "271/271 - 17s - loss: -6.3238e-01 - val_loss: -6.1622e-01 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "271/271 - 17s - loss: -6.3321e-01 - val_loss: -5.9817e-01 - lr: 6.2500e-05\n",
      "Epoch 75/100\n",
      "271/271 - 17s - loss: -6.3330e-01 - val_loss: -6.1057e-01 - lr: 6.2500e-05\n",
      "Epoch 76/100\n",
      "271/271 - 17s - loss: -6.3338e-01 - val_loss: -5.9307e-01 - lr: 6.2500e-05\n",
      "Epoch 77/100\n",
      "271/271 - 17s - loss: -6.3509e-01 - val_loss: -5.8454e-01 - lr: 6.2500e-05\n",
      "Epoch 78/100\n",
      "271/271 - 17s - loss: -6.3489e-01 - val_loss: -5.9411e-01 - lr: 6.2500e-05\n",
      "Epoch 79/100\n",
      "271/271 - 17s - loss: -6.3620e-01 - val_loss: -6.3432e-01 - lr: 6.2500e-05\n",
      "Epoch 80/100\n",
      "271/271 - 17s - loss: -6.3603e-01 - val_loss: -5.9001e-01 - lr: 6.2500e-05\n",
      "Epoch 81/100\n",
      "271/271 - 17s - loss: -6.3539e-01 - val_loss: -6.4133e-01 - lr: 6.2500e-05\n",
      "Epoch 82/100\n",
      "271/271 - 17s - loss: -6.3379e-01 - val_loss: -5.9312e-01 - lr: 6.2500e-05\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "271/271 - 17s - loss: -6.3568e-01 - val_loss: -6.1431e-01 - lr: 6.2500e-05\n",
      "Epoch 84/100\n",
      "271/271 - 17s - loss: -6.3704e-01 - val_loss: -6.2677e-01 - lr: 3.1250e-05\n",
      "Epoch 85/100\n",
      "271/271 - 17s - loss: -6.3801e-01 - val_loss: -5.9102e-01 - lr: 3.1250e-05\n",
      "Epoch 86/100\n",
      "271/271 - 17s - loss: -6.3883e-01 - val_loss: -5.7768e-01 - lr: 3.1250e-05\n",
      "Epoch 87/100\n",
      "271/271 - 17s - loss: -6.3597e-01 - val_loss: -6.4593e-01 - lr: 3.1250e-05\n",
      "Epoch 88/100\n",
      "271/271 - 17s - loss: -6.3630e-01 - val_loss: -5.9255e-01 - lr: 3.1250e-05\n",
      "Epoch 89/100\n",
      "271/271 - 17s - loss: -6.3984e-01 - val_loss: -6.4047e-01 - lr: 3.1250e-05\n",
      "Epoch 90/100\n",
      "271/271 - 17s - loss: -6.3893e-01 - val_loss: -5.5172e-01 - lr: 3.1250e-05\n",
      "Epoch 91/100\n",
      "271/271 - 17s - loss: -6.3734e-01 - val_loss: -6.1362e-01 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "271/271 - 17s - loss: -6.3844e-01 - val_loss: -6.3214e-01 - lr: 3.1250e-05\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "271/271 - 17s - loss: -6.3634e-01 - val_loss: -6.1163e-01 - lr: 3.1250e-05\n",
      "Epoch 94/100\n",
      "271/271 - 17s - loss: -6.3727e-01 - val_loss: -5.6424e-01 - lr: 1.5625e-05\n",
      "Epoch 95/100\n",
      "271/271 - 17s - loss: -6.3579e-01 - val_loss: -6.5033e-01 - lr: 1.5625e-05\n",
      "Epoch 96/100\n",
      "271/271 - 17s - loss: -6.3708e-01 - val_loss: -6.2647e-01 - lr: 1.5625e-05\n",
      "Epoch 97/100\n",
      "271/271 - 17s - loss: -6.3752e-01 - val_loss: -5.8697e-01 - lr: 1.5625e-05\n",
      "Epoch 98/100\n",
      "271/271 - 17s - loss: -6.3850e-01 - val_loss: -6.1642e-01 - lr: 1.5625e-05\n",
      "Epoch 99/100\n",
      "271/271 - 17s - loss: -6.3877e-01 - val_loss: -5.7751e-01 - lr: 1.5625e-05\n",
      "Epoch 100/100\n",
      "271/271 - 17s - loss: -6.3987e-01 - val_loss: -6.2354e-01 - lr: 1.5625e-05\n"
     ]
    }
   ],
   "source": [
    "#fit the long model\n",
    "long_history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"long_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/long_surgery.npy\",long_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "271/271 - 10s - loss: -2.2929e-01 - val_loss: -2.7286e-01 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "271/271 - 10s - loss: -2.9437e-01 - val_loss: -3.3093e-01 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "271/271 - 9s - loss: -3.0390e-01 - val_loss: -3.5035e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "271/271 - 9s - loss: -3.2746e-01 - val_loss: -3.3819e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "271/271 - 10s - loss: -3.2941e-01 - val_loss: -4.1459e-01 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "271/271 - 9s - loss: -3.4385e-01 - val_loss: -3.3247e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "271/271 - 9s - loss: -3.4003e-01 - val_loss: -3.5968e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "271/271 - 10s - loss: -3.4934e-01 - val_loss: -4.2398e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "271/271 - 9s - loss: -3.4986e-01 - val_loss: -3.7006e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "271/271 - 9s - loss: -3.6116e-01 - val_loss: -3.9322e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "271/271 - 9s - loss: -3.6051e-01 - val_loss: -3.3979e-01 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "271/271 - 9s - loss: -3.6198e-01 - val_loss: -3.6012e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "271/271 - 9s - loss: -3.5958e-01 - val_loss: -3.5745e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "271/271 - 10s - loss: -3.7683e-01 - val_loss: -4.1080e-01 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "271/271 - 9s - loss: -3.6324e-01 - val_loss: -3.7648e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "271/271 - 9s - loss: -3.7727e-01 - val_loss: -4.0159e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "271/271 - 9s - loss: -3.7698e-01 - val_loss: -3.8986e-01 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "271/271 - 9s - loss: -3.8197e-01 - val_loss: -4.0431e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "271/271 - 9s - loss: -3.8922e-01 - val_loss: -4.0960e-01 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "271/271 - 9s - loss: -3.8739e-01 - val_loss: -3.9167e-01 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "271/271 - 9s - loss: -3.9442e-01 - val_loss: -3.8248e-01 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "271/271 - 9s - loss: -4.0274e-01 - val_loss: -3.7551e-01 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "271/271 - 9s - loss: -4.0023e-01 - val_loss: -3.4600e-01 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "271/271 - 9s - loss: -4.0503e-01 - val_loss: -3.3141e-01 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "271/271 - 9s - loss: -4.0136e-01 - val_loss: -3.7185e-01 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "271/271 - 9s - loss: -4.0472e-01 - val_loss: -3.9661e-01 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "271/271 - 9s - loss: -4.0595e-01 - val_loss: -3.4131e-01 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "271/271 - 9s - loss: -4.0715e-01 - val_loss: -2.8641e-01 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "271/271 - 9s - loss: -4.1132e-01 - val_loss: -4.4299e-01 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "271/271 - 9s - loss: -4.1155e-01 - val_loss: -3.8436e-01 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "271/271 - 9s - loss: -4.1543e-01 - val_loss: -4.0123e-01 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "271/271 - 10s - loss: -4.1747e-01 - val_loss: -4.4308e-01 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "271/271 - 9s - loss: -4.1652e-01 - val_loss: -3.8154e-01 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "271/271 - 9s - loss: -4.1712e-01 - val_loss: -4.3771e-01 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "271/271 - 9s - loss: -4.1758e-01 - val_loss: -4.0362e-01 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "271/271 - 9s - loss: -4.1956e-01 - val_loss: -4.0742e-01 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "271/271 - 9s - loss: -4.1955e-01 - val_loss: -3.7171e-01 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "271/271 - 9s - loss: -4.2426e-01 - val_loss: -3.8414e-01 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "271/271 - 9s - loss: -4.2475e-01 - val_loss: -3.7250e-01 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "271/271 - 9s - loss: -4.2871e-01 - val_loss: -3.8280e-01 - lr: 1.2500e-04\n",
      "Epoch 41/100\n",
      "271/271 - 10s - loss: -4.2598e-01 - val_loss: -4.6885e-01 - lr: 1.2500e-04\n",
      "Epoch 42/100\n",
      "271/271 - 9s - loss: -4.2722e-01 - val_loss: -3.9901e-01 - lr: 1.2500e-04\n",
      "Epoch 43/100\n",
      "271/271 - 9s - loss: -4.2459e-01 - val_loss: -4.0767e-01 - lr: 1.2500e-04\n",
      "Epoch 44/100\n",
      "271/271 - 9s - loss: -4.2821e-01 - val_loss: -4.2288e-01 - lr: 1.2500e-04\n",
      "Epoch 45/100\n",
      "271/271 - 9s - loss: -4.2725e-01 - val_loss: -4.1961e-01 - lr: 1.2500e-04\n",
      "Epoch 46/100\n",
      "271/271 - 9s - loss: -4.2909e-01 - val_loss: -4.5305e-01 - lr: 1.2500e-04\n",
      "Epoch 47/100\n",
      "271/271 - 9s - loss: -4.2676e-01 - val_loss: -4.0573e-01 - lr: 1.2500e-04\n",
      "Epoch 48/100\n",
      "271/271 - 9s - loss: -4.2925e-01 - val_loss: -4.1403e-01 - lr: 1.2500e-04\n",
      "Epoch 49/100\n",
      "271/271 - 9s - loss: -4.3472e-01 - val_loss: -4.4069e-01 - lr: 1.2500e-04\n",
      "Epoch 50/100\n",
      "271/271 - 9s - loss: -4.2828e-01 - val_loss: -4.1058e-01 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "271/271 - 9s - loss: -4.3095e-01 - val_loss: -4.3380e-01 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "271/271 - 9s - loss: -4.3519e-01 - val_loss: -4.3544e-01 - lr: 6.2500e-05\n",
      "Epoch 53/100\n",
      "271/271 - 9s - loss: -4.3493e-01 - val_loss: -4.3590e-01 - lr: 6.2500e-05\n",
      "Epoch 54/100\n",
      "271/271 - 9s - loss: -4.3295e-01 - val_loss: -4.1698e-01 - lr: 6.2500e-05\n",
      "Epoch 55/100\n",
      "271/271 - 9s - loss: -4.3419e-01 - val_loss: -4.1940e-01 - lr: 6.2500e-05\n",
      "Epoch 56/100\n",
      "271/271 - 10s - loss: -4.3254e-01 - val_loss: -4.0525e-01 - lr: 6.2500e-05\n",
      "Epoch 57/100\n",
      "271/271 - 9s - loss: -4.3128e-01 - val_loss: -4.2055e-01 - lr: 6.2500e-05\n",
      "Epoch 58/100\n",
      "271/271 - 9s - loss: -4.3215e-01 - val_loss: -4.1624e-01 - lr: 6.2500e-05\n",
      "Epoch 59/100\n",
      "271/271 - 9s - loss: -4.3789e-01 - val_loss: -4.3665e-01 - lr: 6.2500e-05\n",
      "Epoch 60/100\n",
      "271/271 - 9s - loss: -4.3298e-01 - val_loss: -4.1306e-01 - lr: 6.2500e-05\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "271/271 - 9s - loss: -4.3877e-01 - val_loss: -4.0634e-01 - lr: 6.2500e-05\n",
      "Epoch 62/100\n",
      "271/271 - 9s - loss: -4.3749e-01 - val_loss: -4.5233e-01 - lr: 3.1250e-05\n",
      "Epoch 63/100\n",
      "271/271 - 9s - loss: -4.2984e-01 - val_loss: -4.1408e-01 - lr: 3.1250e-05\n",
      "Epoch 64/100\n",
      "271/271 - 9s - loss: -4.3672e-01 - val_loss: -4.4599e-01 - lr: 3.1250e-05\n",
      "Epoch 65/100\n",
      "271/271 - 9s - loss: -4.3866e-01 - val_loss: -4.2297e-01 - lr: 3.1250e-05\n",
      "Epoch 66/100\n",
      "271/271 - 9s - loss: -4.3679e-01 - val_loss: -3.9589e-01 - lr: 3.1250e-05\n",
      "Epoch 67/100\n",
      "271/271 - 9s - loss: -4.4045e-01 - val_loss: -4.0547e-01 - lr: 3.1250e-05\n",
      "Epoch 68/100\n",
      "271/271 - 9s - loss: -4.3988e-01 - val_loss: -4.4597e-01 - lr: 3.1250e-05\n",
      "Epoch 69/100\n",
      "271/271 - 9s - loss: -4.3959e-01 - val_loss: -3.8729e-01 - lr: 3.1250e-05\n",
      "Epoch 70/100\n",
      "271/271 - 9s - loss: -4.3816e-01 - val_loss: -4.0299e-01 - lr: 3.1250e-05\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "271/271 - 9s - loss: -4.3865e-01 - val_loss: -3.9710e-01 - lr: 3.1250e-05\n",
      "Epoch 72/100\n",
      "271/271 - 10s - loss: -4.3711e-01 - val_loss: -5.0429e-01 - lr: 1.5625e-05\n",
      "Epoch 73/100\n",
      "271/271 - 9s - loss: -4.4230e-01 - val_loss: -4.0773e-01 - lr: 1.5625e-05\n",
      "Epoch 74/100\n",
      "271/271 - 9s - loss: -4.3608e-01 - val_loss: -4.3230e-01 - lr: 1.5625e-05\n",
      "Epoch 75/100\n",
      "271/271 - 9s - loss: -4.3744e-01 - val_loss: -4.2196e-01 - lr: 1.5625e-05\n",
      "Epoch 76/100\n",
      "271/271 - 9s - loss: -4.3674e-01 - val_loss: -3.7692e-01 - lr: 1.5625e-05\n",
      "Epoch 77/100\n",
      "271/271 - 9s - loss: -4.3823e-01 - val_loss: -4.4259e-01 - lr: 1.5625e-05\n",
      "Epoch 78/100\n",
      "271/271 - 9s - loss: -4.3973e-01 - val_loss: -4.3814e-01 - lr: 1.5625e-05\n",
      "Epoch 79/100\n",
      "271/271 - 9s - loss: -4.3903e-01 - val_loss: -3.6427e-01 - lr: 1.5625e-05\n",
      "Epoch 80/100\n",
      "271/271 - 9s - loss: -4.4075e-01 - val_loss: -4.8352e-01 - lr: 1.5625e-05\n",
      "Epoch 81/100\n",
      "271/271 - 9s - loss: -4.3637e-01 - val_loss: -4.5437e-01 - lr: 1.5625e-05\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "271/271 - 9s - loss: -4.3459e-01 - val_loss: -3.9730e-01 - lr: 1.5625e-05\n",
      "Epoch 83/100\n",
      "271/271 - 9s - loss: -4.4069e-01 - val_loss: -4.0254e-01 - lr: 7.8125e-06\n",
      "Epoch 84/100\n",
      "271/271 - 9s - loss: -4.3874e-01 - val_loss: -4.2265e-01 - lr: 7.8125e-06\n",
      "Epoch 85/100\n",
      "271/271 - 9s - loss: -4.3968e-01 - val_loss: -4.0883e-01 - lr: 7.8125e-06\n",
      "Epoch 86/100\n",
      "271/271 - 9s - loss: -4.3553e-01 - val_loss: -4.3389e-01 - lr: 7.8125e-06\n",
      "Epoch 87/100\n",
      "271/271 - 9s - loss: -4.3869e-01 - val_loss: -4.1272e-01 - lr: 7.8125e-06\n",
      "Epoch 88/100\n",
      "271/271 - 9s - loss: -4.4027e-01 - val_loss: -4.1393e-01 - lr: 7.8125e-06\n",
      "Epoch 89/100\n",
      "271/271 - 9s - loss: -4.3926e-01 - val_loss: -4.3594e-01 - lr: 7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "271/271 - 9s - loss: -4.3938e-01 - val_loss: -4.1610e-01 - lr: 7.8125e-06\n",
      "Epoch 91/100\n",
      "271/271 - 9s - loss: -4.3838e-01 - val_loss: -4.2144e-01 - lr: 7.8125e-06\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "271/271 - 9s - loss: -4.4005e-01 - val_loss: -4.2899e-01 - lr: 7.8125e-06\n",
      "Epoch 93/100\n",
      "271/271 - 9s - loss: -4.4105e-01 - val_loss: -4.1375e-01 - lr: 3.9063e-06\n",
      "Epoch 94/100\n",
      "271/271 - 9s - loss: -4.4044e-01 - val_loss: -4.0720e-01 - lr: 3.9063e-06\n",
      "Epoch 95/100\n",
      "271/271 - 9s - loss: -4.4047e-01 - val_loss: -4.4945e-01 - lr: 3.9063e-06\n",
      "Epoch 96/100\n",
      "271/271 - 9s - loss: -4.4206e-01 - val_loss: -3.9712e-01 - lr: 3.9063e-06\n",
      "Epoch 97/100\n",
      "271/271 - 9s - loss: -4.3891e-01 - val_loss: -4.4289e-01 - lr: 3.9063e-06\n",
      "Epoch 98/100\n",
      "271/271 - 9s - loss: -4.3660e-01 - val_loss: -4.1761e-01 - lr: 3.9063e-06\n",
      "Epoch 99/100\n",
      "271/271 - 9s - loss: -4.3736e-01 - val_loss: -3.8341e-01 - lr: 3.9063e-06\n",
      "Epoch 100/100\n",
      "271/271 - 9s - loss: -4.3957e-01 - val_loss: -4.9045e-01 - lr: 3.9063e-06\n"
     ]
    }
   ],
   "source": [
    "#fit the middle model\n",
    "middle_history = train_model(model=middle_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"middle_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/middle_surgery.npy\",middle_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "271/271 - 7s - loss: -1.0338e-01 - val_loss: -9.9860e-02 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "271/271 - 6s - loss: -1.2411e-01 - val_loss: -1.1583e-01 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "271/271 - 6s - loss: -1.3932e-01 - val_loss: -1.2746e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "271/271 - 6s - loss: -1.5197e-01 - val_loss: -1.4523e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "271/271 - 6s - loss: -1.5819e-01 - val_loss: -1.5943e-01 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "271/271 - 6s - loss: -1.5808e-01 - val_loss: -1.3233e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "271/271 - 6s - loss: -1.6588e-01 - val_loss: -1.7146e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "271/271 - 6s - loss: -1.6767e-01 - val_loss: -1.6437e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "271/271 - 6s - loss: -1.6967e-01 - val_loss: -1.5643e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "271/271 - 6s - loss: -1.7543e-01 - val_loss: -1.7140e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "271/271 - 6s - loss: -1.7362e-01 - val_loss: -1.7003e-01 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "271/271 - 6s - loss: -1.7699e-01 - val_loss: -1.7521e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "271/271 - 6s - loss: -1.7731e-01 - val_loss: -1.7366e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "271/271 - 6s - loss: -1.8319e-01 - val_loss: -1.7516e-01 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "271/271 - 6s - loss: -1.8229e-01 - val_loss: -1.6658e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "271/271 - 6s - loss: -1.8095e-01 - val_loss: -1.8397e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "271/271 - 6s - loss: -1.8264e-01 - val_loss: -1.6886e-01 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "271/271 - 6s - loss: -1.8777e-01 - val_loss: -1.8348e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "271/271 - 6s - loss: -1.8536e-01 - val_loss: -1.7223e-01 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "271/271 - 6s - loss: -1.8958e-01 - val_loss: -1.8742e-01 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "271/271 - 6s - loss: -1.8977e-01 - val_loss: -1.8138e-01 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "271/271 - 6s - loss: -1.9144e-01 - val_loss: -1.6730e-01 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "271/271 - 6s - loss: -1.9155e-01 - val_loss: -1.8554e-01 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "271/271 - 6s - loss: -1.9546e-01 - val_loss: -1.8455e-01 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "271/271 - 6s - loss: -1.9613e-01 - val_loss: -1.8508e-01 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "271/271 - 6s - loss: -1.9569e-01 - val_loss: -1.8630e-01 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "271/271 - 6s - loss: -1.9590e-01 - val_loss: -1.8739e-01 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "271/271 - 6s - loss: -1.9500e-01 - val_loss: -1.8322e-01 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "271/271 - 6s - loss: -1.9347e-01 - val_loss: -1.8367e-01 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "271/271 - 6s - loss: -1.9673e-01 - val_loss: -1.8152e-01 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "271/271 - 6s - loss: -1.9768e-01 - val_loss: -1.8926e-01 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "271/271 - 6s - loss: -2.0099e-01 - val_loss: -1.8420e-01 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "271/271 - 6s - loss: -1.9209e-01 - val_loss: -1.9207e-01 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "271/271 - 6s - loss: -2.0157e-01 - val_loss: -1.8539e-01 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "271/271 - 6s - loss: -1.9876e-01 - val_loss: -1.9069e-01 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "271/271 - 6s - loss: -1.9461e-01 - val_loss: -1.8534e-01 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "271/271 - 6s - loss: -1.9786e-01 - val_loss: -1.8715e-01 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "271/271 - 6s - loss: -1.9658e-01 - val_loss: -1.9086e-01 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "271/271 - 6s - loss: -1.9912e-01 - val_loss: -1.8036e-01 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "271/271 - 6s - loss: -1.9803e-01 - val_loss: -1.9393e-01 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "271/271 - 6s - loss: -1.9866e-01 - val_loss: -1.9478e-01 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "271/271 - 6s - loss: -1.9852e-01 - val_loss: -1.8969e-01 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "271/271 - 6s - loss: -1.9657e-01 - val_loss: -1.7037e-01 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "271/271 - 6s - loss: -2.0010e-01 - val_loss: -1.8958e-01 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "271/271 - 6s - loss: -2.0107e-01 - val_loss: -2.0121e-01 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "271/271 - 6s - loss: -1.9986e-01 - val_loss: -1.8982e-01 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "271/271 - 6s - loss: -1.9942e-01 - val_loss: -1.8817e-01 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "271/271 - 6s - loss: -1.9692e-01 - val_loss: -1.9047e-01 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "271/271 - 6s - loss: -1.9815e-01 - val_loss: -1.8600e-01 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "271/271 - 6s - loss: -2.0561e-01 - val_loss: -1.8314e-01 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "271/271 - 6s - loss: -2.0290e-01 - val_loss: -1.9942e-01 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "271/271 - 6s - loss: -1.9758e-01 - val_loss: -1.8775e-01 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "271/271 - 6s - loss: -2.0542e-01 - val_loss: -1.8479e-01 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "271/271 - 6s - loss: -1.9949e-01 - val_loss: -2.1143e-01 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "271/271 - 6s - loss: -2.0173e-01 - val_loss: -1.9123e-01 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "271/271 - 6s - loss: -1.9963e-01 - val_loss: -1.9428e-01 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "271/271 - 6s - loss: -2.0016e-01 - val_loss: -1.8334e-01 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "271/271 - 6s - loss: -2.0137e-01 - val_loss: -1.9962e-01 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "271/271 - 6s - loss: -2.0194e-01 - val_loss: -1.8518e-01 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "271/271 - 6s - loss: -2.0484e-01 - val_loss: -1.9172e-01 - lr: 5.0000e-04\n",
      "Epoch 61/100\n",
      "271/271 - 6s - loss: -2.0057e-01 - val_loss: -1.9008e-01 - lr: 5.0000e-04\n",
      "Epoch 62/100\n",
      "271/271 - 6s - loss: -2.0051e-01 - val_loss: -1.9745e-01 - lr: 5.0000e-04\n",
      "Epoch 63/100\n",
      "271/271 - 6s - loss: -2.0140e-01 - val_loss: -1.8767e-01 - lr: 5.0000e-04\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "271/271 - 6s - loss: -2.0604e-01 - val_loss: -1.9119e-01 - lr: 5.0000e-04\n",
      "Epoch 65/100\n",
      "271/271 - 6s - loss: -2.0261e-01 - val_loss: -1.9211e-01 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "271/271 - 6s - loss: -2.0014e-01 - val_loss: -1.9072e-01 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "271/271 - 6s - loss: -2.0090e-01 - val_loss: -1.9255e-01 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "271/271 - 6s - loss: -2.0527e-01 - val_loss: -1.8646e-01 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "271/271 - 6s - loss: -2.0458e-01 - val_loss: -2.0160e-01 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "271/271 - 6s - loss: -1.9787e-01 - val_loss: -1.8514e-01 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "271/271 - 6s - loss: -2.0046e-01 - val_loss: -2.0040e-01 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "271/271 - 6s - loss: -2.0631e-01 - val_loss: -1.8539e-01 - lr: 2.5000e-04\n",
      "Epoch 73/100\n",
      "271/271 - 6s - loss: -2.0276e-01 - val_loss: -1.9154e-01 - lr: 2.5000e-04\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "271/271 - 6s - loss: -2.0315e-01 - val_loss: -2.0291e-01 - lr: 2.5000e-04\n",
      "Epoch 75/100\n",
      "271/271 - 6s - loss: -2.0209e-01 - val_loss: -1.9273e-01 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "271/271 - 6s - loss: -2.0377e-01 - val_loss: -1.9363e-01 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "271/271 - 6s - loss: -2.0550e-01 - val_loss: -1.9553e-01 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "271/271 - 6s - loss: -2.0441e-01 - val_loss: -1.9513e-01 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "271/271 - 6s - loss: -2.0085e-01 - val_loss: -1.7771e-01 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "271/271 - 6s - loss: -2.0630e-01 - val_loss: -2.0896e-01 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "271/271 - 6s - loss: -2.0236e-01 - val_loss: -1.8875e-01 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "271/271 - 6s - loss: -2.0310e-01 - val_loss: -1.9705e-01 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "271/271 - 6s - loss: -2.0695e-01 - val_loss: -1.8823e-01 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "271/271 - 6s - loss: -2.0436e-01 - val_loss: -2.0273e-01 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "271/271 - 6s - loss: -2.0567e-01 - val_loss: -1.7771e-01 - lr: 6.2500e-05\n",
      "Epoch 86/100\n",
      "271/271 - 6s - loss: -2.0469e-01 - val_loss: -2.1364e-01 - lr: 6.2500e-05\n",
      "Epoch 87/100\n",
      "271/271 - 6s - loss: -2.0639e-01 - val_loss: -1.8634e-01 - lr: 6.2500e-05\n",
      "Epoch 88/100\n",
      "271/271 - 6s - loss: -2.1134e-01 - val_loss: -1.9425e-01 - lr: 6.2500e-05\n",
      "Epoch 89/100\n",
      "271/271 - 6s - loss: -2.0292e-01 - val_loss: -1.9535e-01 - lr: 6.2500e-05\n",
      "Epoch 90/100\n",
      "271/271 - 6s - loss: -2.0126e-01 - val_loss: -1.9499e-01 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "271/271 - 6s - loss: -2.0434e-01 - val_loss: -1.8641e-01 - lr: 6.2500e-05\n",
      "Epoch 92/100\n",
      "271/271 - 6s - loss: -2.0324e-01 - val_loss: -1.9221e-01 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "271/271 - 6s - loss: -2.0357e-01 - val_loss: -1.9089e-01 - lr: 6.2500e-05\n",
      "Epoch 94/100\n",
      "271/271 - 6s - loss: -2.0495e-01 - val_loss: -2.0460e-01 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "271/271 - 6s - loss: -2.0317e-01 - val_loss: -1.9311e-01 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "271/271 - 6s - loss: -2.0316e-01 - val_loss: -1.9134e-01 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "271/271 - 6s - loss: -2.0390e-01 - val_loss: -2.0176e-01 - lr: 3.1250e-05\n",
      "Epoch 98/100\n",
      "271/271 - 6s - loss: -2.0097e-01 - val_loss: -1.8474e-01 - lr: 3.1250e-05\n",
      "Epoch 99/100\n",
      "271/271 - 6s - loss: -2.0230e-01 - val_loss: -2.0468e-01 - lr: 3.1250e-05\n",
      "Epoch 100/100\n",
      "271/271 - 6s - loss: -2.0375e-01 - val_loss: -1.9122e-01 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "#fit the short model\n",
    "short_history = train_model(model=short_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"short_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/short_surgery.npy\",short_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone of the model without training\n",
    "for layer in middle_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "clone_model = tf.keras.models.clone_model(model)\n",
    "\n",
    "   \n",
    "clone_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    \n",
    "K.set_value(clone_model.optimizer.learning_rate,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "271/271 - 21s - loss: -3.5441e-01 - val_loss: -1.4051e-01 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "271/271 - 21s - loss: -4.8608e-01 - val_loss: -1.8485e-02 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "271/271 - 22s - loss: -5.6707e-01 - val_loss: -2.1636e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "271/271 - 22s - loss: -5.9354e-01 - val_loss: -3.5034e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "271/271 - 22s - loss: -6.0958e-01 - val_loss: -4.0237e-02 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "271/271 - 22s - loss: -6.1179e-01 - val_loss: -1.8814e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "271/271 - 22s - loss: -6.1240e-01 - val_loss: -2.4260e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "271/271 - 22s - loss: -6.3322e-01 - val_loss: -2.6582e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "271/271 - 22s - loss: -6.3374e-01 - val_loss: -3.5987e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "271/271 - 22s - loss: -6.4054e-01 - val_loss: -1.1434e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "271/271 - 22s - loss: -6.4976e-01 - val_loss: -1.0557e-02 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "271/271 - 22s - loss: -6.5705e-01 - val_loss: -5.2792e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "271/271 - 22s - loss: -6.6678e-01 - val_loss: -2.1932e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "271/271 - 22s - loss: -6.6686e-01 - val_loss: -6.8945e-03 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "271/271 - 22s - loss: -6.6408e-01 - val_loss: -1.6454e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "271/271 - 22s - loss: -6.7231e-01 - val_loss: -2.6692e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "271/271 - 22s - loss: -6.7888e-01 - val_loss: -4.1957e-02 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "271/271 - 22s - loss: -6.7936e-01 - val_loss: -2.5463e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "271/271 - 22s - loss: -6.8445e-01 - val_loss: -1.5702e-01 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "271/271 - 22s - loss: -6.8162e-01 - val_loss: -4.4964e-01 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "271/271 - 22s - loss: -7.0151e-01 - val_loss: -1.9055e-01 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "271/271 - 22s - loss: -6.9538e-01 - val_loss: -4.9094e-01 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "271/271 - 22s - loss: -7.1150e-01 - val_loss: -1.5376e-01 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "271/271 - 22s - loss: -7.1755e-01 - val_loss: -5.1557e-01 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "271/271 - 22s - loss: -7.1595e-01 - val_loss: -1.6182e-01 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "271/271 - 22s - loss: -7.2445e-01 - val_loss: -5.5385e-01 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "271/271 - 22s - loss: -7.2187e-01 - val_loss: -6.0005e-01 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "271/271 - 22s - loss: -7.2663e-01 - val_loss: -1.7577e-01 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "271/271 - 22s - loss: -7.2172e-01 - val_loss: -4.4674e-01 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "271/271 - 22s - loss: -7.2519e-01 - val_loss: -3.8306e-01 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "271/271 - 22s - loss: -7.3036e-01 - val_loss: -1.1913e-01 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "271/271 - 22s - loss: -7.2623e-01 - val_loss: -2.1569e-01 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "271/271 - 22s - loss: -7.3478e-01 - val_loss: -1.7326e-01 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "271/271 - 22s - loss: -7.3181e-01 - val_loss: -1.6237e-01 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "271/271 - 22s - loss: -7.3964e-01 - val_loss: -5.7065e-01 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "271/271 - 22s - loss: -7.3496e-01 - val_loss: -2.3917e-01 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "271/271 - 22s - loss: -7.3823e-01 - val_loss: -2.0095e-01 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "271/271 - 22s - loss: -7.4402e-01 - val_loss: -2.4218e-01 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "271/271 - 22s - loss: -7.4909e-01 - val_loss: -5.3093e-01 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "271/271 - 22s - loss: -7.5212e-01 - val_loss: -3.8918e-01 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "271/271 - 22s - loss: -7.4898e-01 - val_loss: -3.6342e-01 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "271/271 - 22s - loss: -7.5742e-01 - val_loss: -6.2027e-01 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "271/271 - 22s - loss: -7.5584e-01 - val_loss: -5.1721e-01 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "271/271 - 22s - loss: -7.5283e-01 - val_loss: -5.4644e-01 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "271/271 - 22s - loss: -7.6123e-01 - val_loss: -6.1859e-01 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "271/271 - 22s - loss: -7.5784e-01 - val_loss: -5.9687e-01 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "271/271 - 22s - loss: -7.5865e-01 - val_loss: -5.5131e-01 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "271/271 - 22s - loss: -7.5978e-01 - val_loss: -5.4382e-01 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "271/271 - 22s - loss: -7.6107e-01 - val_loss: -5.9069e-01 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "271/271 - 22s - loss: -7.6002e-01 - val_loss: -6.2903e-01 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "271/271 - 22s - loss: -7.6634e-01 - val_loss: -5.8058e-01 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "271/271 - 22s - loss: -7.6501e-01 - val_loss: -5.0239e-01 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "271/271 - 22s - loss: -7.6567e-01 - val_loss: -4.0731e-01 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "271/271 - 22s - loss: -7.6362e-01 - val_loss: -5.8280e-01 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "271/271 - 22s - loss: -7.6902e-01 - val_loss: -5.3216e-01 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "271/271 - 22s - loss: -7.7073e-01 - val_loss: -5.3150e-01 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "271/271 - 22s - loss: -7.7070e-01 - val_loss: -1.7908e-01 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "271/271 - 22s - loss: -7.7000e-01 - val_loss: -5.8538e-01 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "271/271 - 22s - loss: -7.6643e-01 - val_loss: -6.4479e-01 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "271/271 - 22s - loss: -7.7349e-01 - val_loss: -5.2646e-01 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "271/271 - 22s - loss: -7.7073e-01 - val_loss: -5.3728e-01 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "271/271 - 22s - loss: -7.7309e-01 - val_loss: -5.4986e-01 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "271/271 - 22s - loss: -7.7731e-01 - val_loss: -6.2585e-01 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "271/271 - 22s - loss: -7.7631e-01 - val_loss: -5.4884e-01 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "271/271 - 22s - loss: -7.7611e-01 - val_loss: -4.3675e-01 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "271/271 - 22s - loss: -7.7444e-01 - val_loss: -5.9547e-01 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "271/271 - 22s - loss: -7.7763e-01 - val_loss: -6.0730e-01 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "271/271 - 22s - loss: -7.7784e-01 - val_loss: -5.3515e-01 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "271/271 - 22s - loss: -7.7840e-01 - val_loss: -6.0074e-01 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "271/271 - 22s - loss: -7.8431e-01 - val_loss: -5.6808e-01 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "271/271 - 22s - loss: -7.8334e-01 - val_loss: -5.5434e-01 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "271/271 - 22s - loss: -7.8923e-01 - val_loss: -5.6338e-01 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "271/271 - 22s - loss: -7.9042e-01 - val_loss: -6.4411e-01 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "271/271 - 22s - loss: -7.8816e-01 - val_loss: -4.9120e-01 - lr: 1.2500e-04\n",
      "Epoch 75/100\n",
      "271/271 - 22s - loss: -7.8873e-01 - val_loss: -5.7158e-01 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "271/271 - 22s - loss: -7.9073e-01 - val_loss: -6.1168e-01 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "271/271 - 22s - loss: -7.9012e-01 - val_loss: -6.3860e-01 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "271/271 - 22s - loss: -7.9324e-01 - val_loss: -6.0369e-01 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "271/271 - 22s - loss: -7.8956e-01 - val_loss: -6.4975e-01 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "271/271 - 22s - loss: -7.9266e-01 - val_loss: -6.2940e-01 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "271/271 - 22s - loss: -7.9306e-01 - val_loss: -6.0535e-01 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "271/271 - 22s - loss: -7.9302e-01 - val_loss: -5.5010e-01 - lr: 1.2500e-04\n",
      "Epoch 83/100\n",
      "271/271 - 22s - loss: -7.9013e-01 - val_loss: -5.7714e-01 - lr: 1.2500e-04\n",
      "Epoch 84/100\n",
      "271/271 - 22s - loss: -7.9957e-01 - val_loss: -4.3388e-01 - lr: 1.2500e-04\n",
      "Epoch 85/100\n",
      "271/271 - 22s - loss: -7.9405e-01 - val_loss: -4.3716e-01 - lr: 1.2500e-04\n",
      "Epoch 86/100\n",
      "271/271 - 22s - loss: -7.9444e-01 - val_loss: -6.4519e-01 - lr: 1.2500e-04\n",
      "Epoch 87/100\n",
      "271/271 - 22s - loss: -7.9115e-01 - val_loss: -6.1489e-01 - lr: 1.2500e-04\n",
      "Epoch 88/100\n",
      "271/271 - 22s - loss: -8.0189e-01 - val_loss: -6.0302e-01 - lr: 1.2500e-04\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "271/271 - 22s - loss: -7.9678e-01 - val_loss: -6.4219e-01 - lr: 1.2500e-04\n",
      "Epoch 90/100\n",
      "271/271 - 22s - loss: -7.9775e-01 - val_loss: -6.4126e-01 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "271/271 - 22s - loss: -7.9940e-01 - val_loss: -5.9790e-01 - lr: 6.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "271/271 - 22s - loss: -8.0374e-01 - val_loss: -6.1535e-01 - lr: 6.2500e-05\n",
      "Epoch 93/100\n",
      "271/271 - 22s - loss: -8.0143e-01 - val_loss: -6.8996e-01 - lr: 6.2500e-05\n",
      "Epoch 94/100\n",
      "271/271 - 22s - loss: -8.0299e-01 - val_loss: -5.6547e-01 - lr: 6.2500e-05\n",
      "Epoch 95/100\n",
      "271/271 - 22s - loss: -8.0057e-01 - val_loss: -6.7116e-01 - lr: 6.2500e-05\n",
      "Epoch 96/100\n",
      "271/271 - 22s - loss: -8.0243e-01 - val_loss: -5.7743e-01 - lr: 6.2500e-05\n",
      "Epoch 97/100\n",
      "271/271 - 22s - loss: -8.0300e-01 - val_loss: -6.0879e-01 - lr: 6.2500e-05\n",
      "Epoch 98/100\n",
      "271/271 - 22s - loss: -8.0439e-01 - val_loss: -6.1190e-01 - lr: 6.2500e-05\n",
      "Epoch 99/100\n",
      "271/271 - 22s - loss: -8.0450e-01 - val_loss: -6.0249e-01 - lr: 6.2500e-05\n",
      "Epoch 100/100\n",
      "271/271 - 22s - loss: -8.0371e-01 - val_loss: -6.7637e-01 - lr: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "clone_history = train_model(model=clone_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name=\"total_surgery\",\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=config[\"learning_rate_patience\"]\n",
    "           )\n",
    "np.save(main_dir + \"parameters_analysis/clone.npy\",clone_history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_surgery = np.load(main_dir+\"parameters_analysis/long_surgery.npy\")\n",
    "middle_surgery = np.load(main_dir+\"parameters_analysis/middle_surgery.npy\")\n",
    "short_surgery = np.load(main_dir+\"parameters_analysis/short_surgery.npy\")\n",
    "clone = np.load(main_dir+\"parameters_analysis/clone.npy\")\n",
    "\n",
    "epochs = range(1,101,4)\n",
    "mean_val_loss = np.zeros((4,25))\n",
    "\n",
    "for i in range(25):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(long_surgery[i:i+4])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(middle_surgery[i:i+4]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(short_surgery[i:i+4]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(clone[i:i+4]))\n",
    "\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'val_loss for big surgery')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'val_loss for middle surgery')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'val_loss for short surgery')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'val_loss for total replacement')\n",
    "plt.title('Comparison between different surgeries ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 4 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BATCH SIZE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = [1,5,10,20,50]\n",
    "val_loss_values = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-231f379d3183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     train_generator, nb_train_steps = get_training_and_validation_generators(\n\u001b[0;32m----> 4\u001b[0;31m                                                     \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                     \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=batch_size_train[i],\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=1,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"batch_size_test\",str(nb_train_steps)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_values[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir + \"parameters_analysis/batch_size.npy\",val_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(5,51,5)\n",
    "val_loss_values = np.load(main_dir + \"parameters_analysis/batch_size.npy\")\n",
    "mean_val_loss = np.zeros((5,10))\n",
    "for i in range(10):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_values[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_values[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_values[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_values[3,i:i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(val_loss_values[4,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'batch size = 1 - iteration = 271')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'batch size = 5 - iteration = 54')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'batch size = 10 - iteration = 27')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'batch size = 20 - iteration = 13')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'batch size = 50 - iteration = 5')\n",
    "plt.title('Evolution of the validation loss with different batch sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERLAP TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_array = [None,42,36,24,12]\n",
    "val_loss_overlap = np.zeros((5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "28/28 - 7s - loss: -1.7699e-01 - val_loss: -3.5477e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "28/28 - 6s - loss: -3.2058e-01 - val_loss: -3.2708e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "28/28 - 6s - loss: -3.5845e-01 - val_loss: -4.1459e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "28/28 - 6s - loss: -3.9210e-01 - val_loss: -4.1338e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "28/28 - 6s - loss: -4.0589e-01 - val_loss: -2.7574e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "28/28 - 6s - loss: -3.7042e-01 - val_loss: -4.4397e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "28/28 - 6s - loss: -3.6895e-01 - val_loss: -3.7375e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "28/28 - 6s - loss: -3.7854e-01 - val_loss: -4.3916e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "28/28 - 6s - loss: -4.2790e-01 - val_loss: -4.6294e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "28/28 - 6s - loss: -4.4971e-01 - val_loss: -4.6350e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "28/28 - 6s - loss: -4.4793e-01 - val_loss: -4.4690e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "28/28 - 6s - loss: -4.5766e-01 - val_loss: -3.4888e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "28/28 - 6s - loss: -4.3617e-01 - val_loss: -4.5582e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "28/28 - 6s - loss: -4.3935e-01 - val_loss: -3.7658e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "28/28 - 6s - loss: -4.3950e-01 - val_loss: -5.0103e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "28/28 - 6s - loss: -4.7230e-01 - val_loss: -4.7062e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "28/28 - 6s - loss: -4.8758e-01 - val_loss: -4.9634e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "28/28 - 6s - loss: -4.8029e-01 - val_loss: -4.8250e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "28/28 - 6s - loss: -4.7959e-01 - val_loss: -4.6572e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "28/28 - 6s - loss: -4.8280e-01 - val_loss: -4.7961e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "28/28 - 6s - loss: -5.0898e-01 - val_loss: -4.6012e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "28/28 - 6s - loss: -4.9277e-01 - val_loss: -4.6715e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "28/28 - 6s - loss: -4.7646e-01 - val_loss: -5.5972e-01 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "28/28 - 6s - loss: -5.0122e-01 - val_loss: -5.0386e-01 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "28/28 - 6s - loss: -4.8785e-01 - val_loss: -4.2303e-01 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "28/28 - 6s - loss: -4.8074e-01 - val_loss: -4.7068e-01 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "28/28 - 6s - loss: -5.1302e-01 - val_loss: -5.4816e-01 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "28/28 - 6s - loss: -4.9962e-01 - val_loss: -4.6662e-01 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "28/28 - 6s - loss: -4.9538e-01 - val_loss: -5.2321e-01 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "28/28 - 6s - loss: -4.8159e-01 - val_loss: -4.6874e-01 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "28/28 - 6s - loss: -4.9102e-01 - val_loss: -5.0852e-01 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "28/28 - 6s - loss: -4.7590e-01 - val_loss: -5.0230e-01 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "28/28 - 6s - loss: -5.1690e-01 - val_loss: -4.8388e-01 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "28/28 - 6s - loss: -5.2329e-01 - val_loss: -5.0254e-01 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "28/28 - 6s - loss: -5.2242e-01 - val_loss: -5.2549e-01 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "28/28 - 6s - loss: -5.1616e-01 - val_loss: -4.8999e-01 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "28/28 - 6s - loss: -5.2769e-01 - val_loss: -4.5429e-01 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "28/28 - 6s - loss: -5.1786e-01 - val_loss: -4.7431e-01 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "28/28 - 6s - loss: -5.3474e-01 - val_loss: -4.4577e-01 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "28/28 - 6s - loss: -5.3492e-01 - val_loss: -4.4541e-01 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "28/28 - 6s - loss: -5.0528e-01 - val_loss: -4.3738e-01 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "28/28 - 6s - loss: -5.1310e-01 - val_loss: -4.3837e-01 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "28/28 - 6s - loss: -5.1970e-01 - val_loss: -5.2304e-01 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "28/28 - 6s - loss: -5.3548e-01 - val_loss: -4.5465e-01 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "28/28 - 6s - loss: -5.4143e-01 - val_loss: -5.6576e-01 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "28/28 - 6s - loss: -5.3977e-01 - val_loss: -4.8865e-01 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "28/28 - 6s - loss: -5.4390e-01 - val_loss: -5.2714e-01 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "28/28 - 6s - loss: -5.5406e-01 - val_loss: -5.1240e-01 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "28/28 - 6s - loss: -5.3696e-01 - val_loss: -4.2857e-01 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "28/28 - 6s - loss: -5.4918e-01 - val_loss: -5.4904e-01 - lr: 2.5000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "31/31 - 7s - loss: -1.5798e-01 - val_loss: -3.1368e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "31/31 - 7s - loss: -3.1441e-01 - val_loss: -3.7680e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "31/31 - 7s - loss: -2.1061e-01 - val_loss: -3.3683e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "31/31 - 7s - loss: -3.5432e-01 - val_loss: -4.0782e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "31/31 - 7s - loss: -3.9683e-01 - val_loss: -4.3210e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "31/31 - 7s - loss: -4.1523e-01 - val_loss: -4.1564e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "31/31 - 7s - loss: -4.2091e-01 - val_loss: -4.6306e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "31/31 - 7s - loss: -4.2512e-01 - val_loss: -4.3461e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "31/31 - 7s - loss: -4.5110e-01 - val_loss: -4.3663e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "31/31 - 7s - loss: -4.2974e-01 - val_loss: -4.2245e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "31/31 - 7s - loss: -4.1662e-01 - val_loss: -4.8634e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "31/31 - 7s - loss: -4.4888e-01 - val_loss: -4.5527e-01 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50\n",
      "31/31 - 7s - loss: -4.7203e-01 - val_loss: -4.7033e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "31/31 - 7s - loss: -4.6753e-01 - val_loss: -4.8656e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "31/31 - 7s - loss: -4.8277e-01 - val_loss: -4.8621e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "31/31 - 7s - loss: -4.7478e-01 - val_loss: -5.3991e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "31/31 - 7s - loss: -4.9439e-01 - val_loss: -4.7636e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "31/31 - 7s - loss: -4.8476e-01 - val_loss: -4.7169e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "31/31 - 7s - loss: -5.0020e-01 - val_loss: -4.5982e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "31/31 - 7s - loss: -4.9027e-01 - val_loss: -4.5931e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "31/31 - 7s - loss: -4.8913e-01 - val_loss: -4.8662e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "31/31 - 7s - loss: -4.9457e-01 - val_loss: -4.7242e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "31/31 - 7s - loss: -4.8149e-01 - val_loss: -4.6659e-01 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "31/31 - 7s - loss: -4.9082e-01 - val_loss: -4.8824e-01 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "31/31 - 7s - loss: -5.1387e-01 - val_loss: -4.8592e-01 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "31/31 - 7s - loss: -5.0985e-01 - val_loss: -4.1520e-01 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "31/31 - 7s - loss: -5.2487e-01 - val_loss: -4.8312e-01 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "31/31 - 7s - loss: -5.2758e-01 - val_loss: -4.9160e-01 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "31/31 - 7s - loss: -5.2977e-01 - val_loss: -4.0286e-01 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "31/31 - 7s - loss: -5.2577e-01 - val_loss: -5.4246e-01 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "31/31 - 7s - loss: -5.2744e-01 - val_loss: -4.6706e-01 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "31/31 - 7s - loss: -5.3503e-01 - val_loss: -5.5006e-01 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "31/31 - 7s - loss: -5.4254e-01 - val_loss: -4.9686e-01 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "31/31 - 7s - loss: -5.3659e-01 - val_loss: -4.6760e-01 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "31/31 - 7s - loss: -5.4122e-01 - val_loss: -5.1788e-01 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "31/31 - 7s - loss: -5.4167e-01 - val_loss: -5.4408e-01 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "31/31 - 7s - loss: -5.3501e-01 - val_loss: -5.3323e-01 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "31/31 - 7s - loss: -5.4546e-01 - val_loss: -5.0004e-01 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "31/31 - 7s - loss: -5.3220e-01 - val_loss: -4.8859e-01 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "31/31 - 7s - loss: -5.5227e-01 - val_loss: -5.3572e-01 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "31/31 - 7s - loss: -5.5985e-01 - val_loss: -4.9657e-01 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "31/31 - 7s - loss: -5.4576e-01 - val_loss: -4.9100e-01 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "31/31 - 7s - loss: -5.5462e-01 - val_loss: -5.3341e-01 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "31/31 - 7s - loss: -5.5620e-01 - val_loss: -5.5054e-01 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "31/31 - 7s - loss: -5.5742e-01 - val_loss: -4.8921e-01 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "31/31 - 7s - loss: -5.7136e-01 - val_loss: -5.8724e-01 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "31/31 - 7s - loss: -5.5390e-01 - val_loss: -5.4729e-01 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "31/31 - 7s - loss: -5.6999e-01 - val_loss: -5.0175e-01 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "31/31 - 7s - loss: -5.5626e-01 - val_loss: -5.9901e-01 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "31/31 - 7s - loss: -5.6023e-01 - val_loss: -4.9394e-01 - lr: 2.5000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "36/36 - 8s - loss: -2.2836e-01 - val_loss: -3.5441e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "36/36 - 8s - loss: -3.2826e-01 - val_loss: -4.1590e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "36/36 - 8s - loss: -3.6508e-01 - val_loss: -4.0057e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "36/36 - 8s - loss: -3.9401e-01 - val_loss: -4.6467e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "36/36 - 8s - loss: -4.1296e-01 - val_loss: -4.3267e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "36/36 - 8s - loss: -4.1418e-01 - val_loss: -3.9913e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "36/36 - 8s - loss: -4.4565e-01 - val_loss: -4.5207e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "36/36 - 8s - loss: -4.5652e-01 - val_loss: -4.9895e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "36/36 - 8s - loss: -4.5434e-01 - val_loss: -4.3573e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "36/36 - 8s - loss: -4.5842e-01 - val_loss: -5.0067e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "36/36 - 8s - loss: -4.6498e-01 - val_loss: -5.0454e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "36/36 - 8s - loss: -4.6816e-01 - val_loss: -4.3326e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "36/36 - 8s - loss: -4.9165e-01 - val_loss: -5.2444e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "36/36 - 8s - loss: -5.0166e-01 - val_loss: -4.7696e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "36/36 - 8s - loss: -4.8094e-01 - val_loss: -4.1656e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "36/36 - 8s - loss: -5.0376e-01 - val_loss: -5.4911e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "36/36 - 8s - loss: -4.9780e-01 - val_loss: -3.8968e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "36/36 - 8s - loss: -5.0408e-01 - val_loss: -4.1260e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "36/36 - 8s - loss: -5.0195e-01 - val_loss: -3.8672e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "36/36 - 8s - loss: -4.9434e-01 - val_loss: -5.2570e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "36/36 - 8s - loss: -5.0579e-01 - val_loss: -4.0701e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "36/36 - 8s - loss: -5.0157e-01 - val_loss: -4.8017e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "36/36 - 8s - loss: -5.1897e-01 - val_loss: -4.1169e-01 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "36/36 - 8s - loss: -5.0137e-01 - val_loss: -4.9351e-01 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "36/36 - 8s - loss: -5.0147e-01 - val_loss: -5.1440e-01 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "36/36 - 8s - loss: -5.2296e-01 - val_loss: -5.1129e-01 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "36/36 - 8s - loss: -5.3191e-01 - val_loss: -4.7390e-01 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "36/36 - 8s - loss: -5.3671e-01 - val_loss: -5.5277e-01 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "36/36 - 8s - loss: -5.3857e-01 - val_loss: -5.0579e-01 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "36/36 - 8s - loss: -5.4818e-01 - val_loss: -5.7638e-01 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "36/36 - 8s - loss: -5.4508e-01 - val_loss: -4.6574e-01 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "36/36 - 8s - loss: -5.4936e-01 - val_loss: -4.8463e-01 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "36/36 - 8s - loss: -5.5108e-01 - val_loss: -5.1056e-01 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "36/36 - 8s - loss: -5.4432e-01 - val_loss: -4.7168e-01 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "36/36 - 8s - loss: -5.4862e-01 - val_loss: -5.1031e-01 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "36/36 - 8s - loss: -5.5719e-01 - val_loss: -4.4023e-01 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "36/36 - 8s - loss: -5.5345e-01 - val_loss: -4.7535e-01 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "36/36 - 8s - loss: -5.4486e-01 - val_loss: -4.3286e-01 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "36/36 - 8s - loss: -5.4743e-01 - val_loss: -5.3633e-01 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "36/36 - 8s - loss: -5.5984e-01 - val_loss: -4.8365e-01 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "36/36 - 8s - loss: -5.7023e-01 - val_loss: -5.0437e-01 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "36/36 - 8s - loss: -5.6454e-01 - val_loss: -5.0802e-01 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "36/36 - 8s - loss: -5.7006e-01 - val_loss: -5.8131e-01 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "36/36 - 8s - loss: -5.6806e-01 - val_loss: -5.0328e-01 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "36/36 - 8s - loss: -5.7190e-01 - val_loss: -4.9180e-01 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "36/36 - 8s - loss: -5.5721e-01 - val_loss: -5.3165e-01 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "36/36 - 8s - loss: -5.6867e-01 - val_loss: -5.5470e-01 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "36/36 - 8s - loss: -5.7462e-01 - val_loss: -5.5378e-01 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "36/36 - 8s - loss: -5.7526e-01 - val_loss: -5.2636e-01 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "36/36 - 8s - loss: -5.7842e-01 - val_loss: -5.7188e-01 - lr: 2.5000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "54/54 - 12s - loss: -2.2705e-01 - val_loss: -3.5754e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "54/54 - 12s - loss: -3.5761e-01 - val_loss: -4.1796e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "54/54 - 12s - loss: -4.2370e-01 - val_loss: -4.2057e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "54/54 - 11s - loss: -4.3350e-01 - val_loss: -4.0629e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "54/54 - 12s - loss: -4.1956e-01 - val_loss: -3.9165e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "54/54 - 12s - loss: -4.5738e-01 - val_loss: -4.4392e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "54/54 - 12s - loss: -4.8837e-01 - val_loss: -5.0829e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "54/54 - 12s - loss: -4.8464e-01 - val_loss: -4.7687e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "54/54 - 12s - loss: -4.8269e-01 - val_loss: -5.0862e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "54/54 - 12s - loss: -4.9765e-01 - val_loss: -3.9019e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "54/54 - 12s - loss: -4.5050e-01 - val_loss: -2.9579e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "54/54 - 12s - loss: -4.4579e-01 - val_loss: -5.3860e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "54/54 - 12s - loss: -4.9589e-01 - val_loss: -4.6756e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "54/54 - 12s - loss: -5.0790e-01 - val_loss: -4.5883e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "54/54 - 12s - loss: -5.1628e-01 - val_loss: -5.1214e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "54/54 - 11s - loss: -5.2837e-01 - val_loss: -4.6957e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "54/54 - 12s - loss: -5.3103e-01 - val_loss: -5.4085e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "54/54 - 12s - loss: -5.4256e-01 - val_loss: -5.2304e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "54/54 - 12s - loss: -5.4639e-01 - val_loss: -5.0698e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "54/54 - 12s - loss: -5.5421e-01 - val_loss: -4.7806e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "54/54 - 12s - loss: -5.4183e-01 - val_loss: -4.6142e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "54/54 - 12s - loss: -5.4377e-01 - val_loss: -5.0131e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "54/54 - 12s - loss: -5.3580e-01 - val_loss: -5.4468e-01 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "54/54 - 12s - loss: -5.4268e-01 - val_loss: -4.8757e-01 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "54/54 - 11s - loss: -5.3545e-01 - val_loss: -4.3821e-01 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "54/54 - 12s - loss: -5.4692e-01 - val_loss: -5.4231e-01 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "54/54 - 12s - loss: -5.6362e-01 - val_loss: -5.2412e-01 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "54/54 - 12s - loss: -5.5905e-01 - val_loss: -5.2665e-01 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "54/54 - 12s - loss: -5.5744e-01 - val_loss: -5.1114e-01 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "54/54 - 12s - loss: -5.5813e-01 - val_loss: -4.5974e-01 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "54/54 - 12s - loss: -5.6079e-01 - val_loss: -5.2533e-01 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "54/54 - 12s - loss: -5.7008e-01 - val_loss: -5.3852e-01 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "54/54 - 12s - loss: -5.7874e-01 - val_loss: -5.8176e-01 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "54/54 - 12s - loss: -5.7311e-01 - val_loss: -4.8048e-01 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "54/54 - 12s - loss: -5.7456e-01 - val_loss: -5.3115e-01 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "54/54 - 12s - loss: -5.7273e-01 - val_loss: -5.1625e-01 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "54/54 - 12s - loss: -5.6437e-01 - val_loss: -4.9961e-01 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "54/54 - 12s - loss: -5.5789e-01 - val_loss: -4.3350e-01 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "54/54 - 12s - loss: -5.7424e-01 - val_loss: -5.6454e-01 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "54/54 - 12s - loss: -5.8117e-01 - val_loss: -4.9455e-01 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "54/54 - 12s - loss: -5.9502e-01 - val_loss: -5.8149e-01 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "54/54 - 12s - loss: -5.8472e-01 - val_loss: -5.3646e-01 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "54/54 - 12s - loss: -5.8307e-01 - val_loss: -4.6645e-01 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "54/54 - 11s - loss: -5.9771e-01 - val_loss: -5.3899e-01 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "54/54 - 11s - loss: -5.9860e-01 - val_loss: -4.2661e-01 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "54/54 - 12s - loss: -5.8803e-01 - val_loss: -5.5140e-01 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "54/54 - 12s - loss: -6.1421e-01 - val_loss: -4.5795e-01 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "54/54 - 12s - loss: -5.9759e-01 - val_loss: -4.6364e-01 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "54/54 - 12s - loss: -6.0304e-01 - val_loss: -5.2292e-01 - lr: 5.0000e-04\n",
      "Epoch 50/50\n",
      "54/54 - 11s - loss: -6.0541e-01 - val_loss: -5.6746e-01 - lr: 5.0000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "106/106 - 22s - loss: -2.9188e-01 - val_loss: -4.2236e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "106/106 - 22s - loss: -4.0694e-01 - val_loss: -4.3560e-01 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "106/106 - 22s - loss: -4.3785e-01 - val_loss: -4.8567e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "106/106 - 22s - loss: -4.7056e-01 - val_loss: -4.7201e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "106/106 - 22s - loss: -4.9761e-01 - val_loss: -4.9519e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "106/106 - 22s - loss: -5.0940e-01 - val_loss: -5.1542e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "106/106 - 22s - loss: -5.0399e-01 - val_loss: -4.9883e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "106/106 - 22s - loss: -5.2284e-01 - val_loss: -5.2324e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "106/106 - 22s - loss: -5.1248e-01 - val_loss: -5.0754e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "106/106 - 22s - loss: -5.2795e-01 - val_loss: -4.6839e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "106/106 - 22s - loss: -5.2910e-01 - val_loss: -4.8304e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "106/106 - 22s - loss: -5.3713e-01 - val_loss: -5.6546e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "106/106 - 22s - loss: -5.4820e-01 - val_loss: -4.5903e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "106/106 - 22s - loss: -5.5197e-01 - val_loss: -3.5909e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "106/106 - 22s - loss: -5.3565e-01 - val_loss: -5.1122e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "106/106 - 22s - loss: -5.6121e-01 - val_loss: -5.0932e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "106/106 - 22s - loss: -5.7075e-01 - val_loss: -5.3351e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "106/106 - 22s - loss: -5.6410e-01 - val_loss: -5.3595e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "106/106 - 22s - loss: -5.6653e-01 - val_loss: -4.9806e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "106/106 - 22s - loss: -5.6942e-01 - val_loss: -4.4655e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "106/106 - 22s - loss: -5.6700e-01 - val_loss: -5.5987e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "106/106 - 22s - loss: -5.7968e-01 - val_loss: -5.4241e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "106/106 - 22s - loss: -5.9218e-01 - val_loss: -5.3450e-01 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "106/106 - 22s - loss: -5.9187e-01 - val_loss: -5.6627e-01 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "106/106 - 22s - loss: -5.9635e-01 - val_loss: -5.1968e-01 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "106/106 - 22s - loss: -6.0135e-01 - val_loss: -4.9626e-01 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "106/106 - 22s - loss: -6.0309e-01 - val_loss: -5.8720e-01 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "106/106 - 22s - loss: -6.0976e-01 - val_loss: -5.1231e-01 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "106/106 - 22s - loss: -6.1114e-01 - val_loss: -5.2072e-01 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "106/106 - 22s - loss: -6.0949e-01 - val_loss: -5.1857e-01 - lr: 5.0000e-04\n",
      "Epoch 31/50\n",
      "106/106 - 22s - loss: -6.1019e-01 - val_loss: -5.2630e-01 - lr: 5.0000e-04\n",
      "Epoch 32/50\n",
      "106/106 - 22s - loss: -6.0852e-01 - val_loss: -5.5917e-01 - lr: 5.0000e-04\n",
      "Epoch 33/50\n",
      "106/106 - 22s - loss: -6.1667e-01 - val_loss: -5.5099e-01 - lr: 5.0000e-04\n",
      "Epoch 34/50\n",
      "106/106 - 22s - loss: -6.1580e-01 - val_loss: -4.8187e-01 - lr: 5.0000e-04\n",
      "Epoch 35/50\n",
      "106/106 - 22s - loss: -6.1814e-01 - val_loss: -5.4079e-01 - lr: 5.0000e-04\n",
      "Epoch 36/50\n",
      "106/106 - 22s - loss: -6.1391e-01 - val_loss: -5.5561e-01 - lr: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "106/106 - 22s - loss: -6.1977e-01 - val_loss: -5.3691e-01 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "106/106 - 22s - loss: -6.2548e-01 - val_loss: -5.6193e-01 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "106/106 - 22s - loss: -6.2936e-01 - val_loss: -4.0439e-01 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "106/106 - 22s - loss: -6.3146e-01 - val_loss: -5.6361e-01 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "106/106 - 22s - loss: -6.3074e-01 - val_loss: -5.2460e-01 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "106/106 - 22s - loss: -6.3772e-01 - val_loss: -4.6878e-01 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "106/106 - 22s - loss: -6.3821e-01 - val_loss: -5.6481e-01 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "106/106 - 22s - loss: -6.3670e-01 - val_loss: -5.3474e-01 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "106/106 - 22s - loss: -6.4205e-01 - val_loss: -5.4176e-01 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "106/106 - 22s - loss: -6.3789e-01 - val_loss: -5.0030e-01 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "106/106 - 22s - loss: -6.3915e-01 - val_loss: -5.8714e-01 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "106/106 - 22s - loss: -6.4478e-01 - val_loss: -5.0718e-01 - lr: 1.2500e-04\n",
      "Epoch 49/50\n",
      "106/106 - 22s - loss: -6.4590e-01 - val_loss: -5.5803e-01 - lr: 1.2500e-04\n",
      "Epoch 50/50\n",
      "106/106 - 22s - loss: -6.4664e-01 - val_loss: -5.3633e-01 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=overlap_array[i]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"overlap_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_overlap[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir + \"parameters_analysis/overlap.npy\",val_loss_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_overlap = np.load(main_dir+\"parameters_analysis/overlap.npy\")\n",
    "epochs = range(5,51,5)\n",
    "mean_val_loss = np.zeros((5,10))\n",
    "for i in range(10):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_overlap[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_overlap[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_overlap[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_overlap[3,i:i+5]))\n",
    "    mean_val_loss[4,i]=np.abs(np.mean(val_loss_overlap[4,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'No overlap (= 48)')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'overlap of 42')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'overlap of 36')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'overlap of 24')\n",
    "plt.plot(epochs,mean_val_loss[4,:],'o--',label = 'overlap of 12')\n",
    "plt.title('Evolution of the validation loss with different overlap sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING RATE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_array = [0.1,0.01,0.001,0.0001]\n",
    "val_loss_learning = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "Epoch 1/100\n",
      "106/106 - 22s - loss: -1.3483e-01 - val_loss: -1.2635e-01 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "106/106 - 22s - loss: -3.0346e-01 - val_loss: -2.9098e-01 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "106/106 - 22s - loss: -3.9472e-01 - val_loss: -4.1539e-01 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "106/106 - 22s - loss: -4.2020e-01 - val_loss: -5.1610e-01 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "106/106 - 22s - loss: -4.5449e-01 - val_loss: -4.3913e-01 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "106/106 - 22s - loss: -4.4940e-01 - val_loss: -4.5728e-01 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "106/106 - 22s - loss: -4.7191e-01 - val_loss: -4.1286e-01 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "106/106 - 22s - loss: -4.8432e-01 - val_loss: -4.7425e-01 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "106/106 - 22s - loss: -4.8928e-01 - val_loss: -5.2733e-01 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "106/106 - 22s - loss: -4.9209e-01 - val_loss: -5.1685e-01 - lr: 0.1000\n",
      "Epoch 11/100\n",
      "106/106 - 22s - loss: -4.9974e-01 - val_loss: -5.6049e-01 - lr: 0.1000\n",
      "Epoch 12/100\n",
      "106/106 - 22s - loss: -5.1017e-01 - val_loss: -5.4035e-01 - lr: 0.1000\n",
      "Epoch 13/100\n",
      "106/106 - 22s - loss: -5.2111e-01 - val_loss: -5.8245e-01 - lr: 0.1000\n",
      "Epoch 14/100\n",
      "106/106 - 22s - loss: -5.1052e-01 - val_loss: -5.2039e-01 - lr: 0.1000\n",
      "Epoch 15/100\n",
      "106/106 - 22s - loss: -5.1671e-01 - val_loss: -5.5845e-01 - lr: 0.1000\n",
      "Epoch 16/100\n",
      "106/106 - 22s - loss: -5.2345e-01 - val_loss: -5.3776e-01 - lr: 0.1000\n",
      "Epoch 17/100\n",
      "106/106 - 22s - loss: -5.3333e-01 - val_loss: -5.8037e-01 - lr: 0.1000\n",
      "Epoch 18/100\n",
      "106/106 - 22s - loss: -5.3789e-01 - val_loss: -5.5387e-01 - lr: 0.1000\n",
      "Epoch 19/100\n",
      "106/106 - 22s - loss: -5.2929e-01 - val_loss: -4.1094e-01 - lr: 0.1000\n",
      "Epoch 20/100\n",
      "106/106 - 22s - loss: -5.3027e-01 - val_loss: -4.8053e-01 - lr: 0.1000\n",
      "Epoch 21/100\n",
      "106/106 - 22s - loss: -5.2859e-01 - val_loss: -5.6470e-01 - lr: 0.1000\n",
      "Epoch 22/100\n",
      "106/106 - 22s - loss: -5.3838e-01 - val_loss: -5.1963e-01 - lr: 0.1000\n",
      "Epoch 23/100\n",
      "106/106 - 22s - loss: -5.4086e-01 - val_loss: -6.1753e-01 - lr: 0.1000\n",
      "Epoch 24/100\n",
      "106/106 - 22s - loss: -5.3247e-01 - val_loss: -5.2291e-01 - lr: 0.1000\n",
      "Epoch 25/100\n",
      "106/106 - 22s - loss: -5.4159e-01 - val_loss: -2.7378e-01 - lr: 0.1000\n",
      "Epoch 26/100\n",
      "106/106 - 22s - loss: -5.3321e-01 - val_loss: -5.7434e-01 - lr: 0.1000\n",
      "Epoch 27/100\n",
      "106/106 - 22s - loss: -5.4526e-01 - val_loss: -5.3907e-01 - lr: 0.1000\n",
      "Epoch 28/100\n",
      "106/106 - 22s - loss: -5.4068e-01 - val_loss: -5.6235e-01 - lr: 0.1000\n",
      "Epoch 29/100\n",
      "106/106 - 22s - loss: -5.4770e-01 - val_loss: -5.8232e-01 - lr: 0.1000\n",
      "Epoch 30/100\n",
      "106/106 - 22s - loss: -5.4556e-01 - val_loss: -5.6794e-01 - lr: 0.1000\n",
      "Epoch 31/100\n",
      "106/106 - 22s - loss: -5.4587e-01 - val_loss: -5.4634e-01 - lr: 0.1000\n",
      "Epoch 32/100\n",
      "106/106 - 22s - loss: -5.5727e-01 - val_loss: -6.0621e-01 - lr: 0.1000\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "106/106 - 22s - loss: -5.3409e-01 - val_loss: -6.0588e-01 - lr: 0.1000\n",
      "Epoch 34/100\n",
      "106/106 - 22s - loss: -5.6281e-01 - val_loss: -5.6575e-01 - lr: 0.0500\n",
      "Epoch 35/100\n",
      "106/106 - 22s - loss: -5.6460e-01 - val_loss: -5.9077e-01 - lr: 0.0500\n",
      "Epoch 36/100\n",
      "106/106 - 22s - loss: -5.6313e-01 - val_loss: -6.1729e-01 - lr: 0.0500\n",
      "Epoch 37/100\n",
      "106/106 - 22s - loss: -5.6884e-01 - val_loss: -6.0283e-01 - lr: 0.0500\n",
      "Epoch 38/100\n",
      "106/106 - 22s - loss: -5.6692e-01 - val_loss: -6.0889e-01 - lr: 0.0500\n",
      "Epoch 39/100\n",
      "106/106 - 22s - loss: -5.6977e-01 - val_loss: -5.8135e-01 - lr: 0.0500\n",
      "Epoch 40/100\n",
      "106/106 - 22s - loss: -5.7389e-01 - val_loss: -5.6263e-01 - lr: 0.0500\n",
      "Epoch 41/100\n",
      "106/106 - 22s - loss: -5.6207e-01 - val_loss: -6.3648e-01 - lr: 0.0500\n",
      "Epoch 42/100\n",
      "106/106 - 22s - loss: -5.6950e-01 - val_loss: -6.1913e-01 - lr: 0.0500\n",
      "Epoch 43/100\n",
      "106/106 - 22s - loss: -5.6922e-01 - val_loss: -6.1302e-01 - lr: 0.0500\n",
      "Epoch 44/100\n",
      "106/106 - 22s - loss: -5.7419e-01 - val_loss: -5.9767e-01 - lr: 0.0500\n",
      "Epoch 45/100\n",
      "106/106 - 22s - loss: -5.7292e-01 - val_loss: -5.9214e-01 - lr: 0.0500\n",
      "Epoch 46/100\n",
      "106/106 - 22s - loss: -5.7878e-01 - val_loss: -6.3781e-01 - lr: 0.0500\n",
      "Epoch 47/100\n",
      "106/106 - 22s - loss: -5.7848e-01 - val_loss: -5.1087e-01 - lr: 0.0500\n",
      "Epoch 48/100\n",
      "106/106 - 22s - loss: -5.7181e-01 - val_loss: -5.0527e-01 - lr: 0.0500\n",
      "Epoch 49/100\n",
      "106/106 - 22s - loss: -5.7314e-01 - val_loss: -5.8993e-01 - lr: 0.0500\n",
      "Epoch 50/100\n",
      "106/106 - 22s - loss: -5.7979e-01 - val_loss: -6.0633e-01 - lr: 0.0500\n",
      "Epoch 51/100\n",
      "106/106 - 22s - loss: -5.7360e-01 - val_loss: -5.6559e-01 - lr: 0.0500\n",
      "Epoch 52/100\n",
      "106/106 - 22s - loss: -5.8217e-01 - val_loss: -6.2668e-01 - lr: 0.0500\n",
      "Epoch 53/100\n",
      "106/106 - 22s - loss: -5.7652e-01 - val_loss: -5.8793e-01 - lr: 0.0500\n",
      "Epoch 54/100\n",
      "106/106 - 22s - loss: -5.8401e-01 - val_loss: -5.9107e-01 - lr: 0.0500\n",
      "Epoch 55/100\n",
      "106/106 - 22s - loss: -5.8503e-01 - val_loss: -6.2578e-01 - lr: 0.0500\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "106/106 - 22s - loss: -5.8176e-01 - val_loss: -6.0003e-01 - lr: 0.0500\n",
      "Epoch 57/100\n",
      "106/106 - 22s - loss: -5.8857e-01 - val_loss: -6.1163e-01 - lr: 0.0250\n",
      "Epoch 58/100\n",
      "106/106 - 22s - loss: -5.9226e-01 - val_loss: -5.5205e-01 - lr: 0.0250\n",
      "Epoch 59/100\n",
      "106/106 - 22s - loss: -5.8832e-01 - val_loss: -6.1931e-01 - lr: 0.0250\n",
      "Epoch 60/100\n",
      "106/106 - 22s - loss: -5.9076e-01 - val_loss: -6.2202e-01 - lr: 0.0250\n",
      "Epoch 61/100\n",
      "106/106 - 22s - loss: -5.9288e-01 - val_loss: -6.7277e-01 - lr: 0.0250\n",
      "Epoch 62/100\n",
      "106/106 - 22s - loss: -5.9736e-01 - val_loss: -5.9016e-01 - lr: 0.0250\n",
      "Epoch 63/100\n",
      "106/106 - 22s - loss: -5.9185e-01 - val_loss: -6.0796e-01 - lr: 0.0250\n",
      "Epoch 64/100\n",
      "106/106 - 22s - loss: -5.9737e-01 - val_loss: -6.8365e-01 - lr: 0.0250\n",
      "Epoch 65/100\n",
      "106/106 - 22s - loss: -5.9369e-01 - val_loss: -6.1113e-01 - lr: 0.0250\n",
      "Epoch 66/100\n",
      "106/106 - 22s - loss: -5.9697e-01 - val_loss: -6.2712e-01 - lr: 0.0250\n",
      "Epoch 67/100\n",
      "106/106 - 22s - loss: -6.0047e-01 - val_loss: -6.5178e-01 - lr: 0.0250\n",
      "Epoch 68/100\n",
      "106/106 - 22s - loss: -5.9402e-01 - val_loss: -6.4793e-01 - lr: 0.0250\n",
      "Epoch 69/100\n",
      "106/106 - 22s - loss: -5.9500e-01 - val_loss: -5.6553e-01 - lr: 0.0250\n",
      "Epoch 70/100\n",
      "106/106 - 22s - loss: -5.9649e-01 - val_loss: -6.1294e-01 - lr: 0.0250\n",
      "Epoch 71/100\n",
      "106/106 - 22s - loss: -6.0101e-01 - val_loss: -6.2279e-01 - lr: 0.0250\n",
      "Epoch 72/100\n",
      "106/106 - 22s - loss: -5.9802e-01 - val_loss: -6.2133e-01 - lr: 0.0250\n",
      "Epoch 73/100\n",
      "106/106 - 22s - loss: -5.9898e-01 - val_loss: -6.3203e-01 - lr: 0.0250\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.012500000186264515.\n",
      "106/106 - 22s - loss: -6.0345e-01 - val_loss: -6.2003e-01 - lr: 0.0250\n",
      "Epoch 75/100\n",
      "106/106 - 22s - loss: -6.0704e-01 - val_loss: -6.2667e-01 - lr: 0.0125\n",
      "Epoch 76/100\n",
      "106/106 - 22s - loss: -6.0663e-01 - val_loss: -6.6662e-01 - lr: 0.0125\n",
      "Epoch 77/100\n",
      "106/106 - 22s - loss: -6.1016e-01 - val_loss: -6.2810e-01 - lr: 0.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "106/106 - 22s - loss: -6.0991e-01 - val_loss: -6.1299e-01 - lr: 0.0125\n",
      "Epoch 79/100\n",
      "106/106 - 22s - loss: -6.0704e-01 - val_loss: -6.4674e-01 - lr: 0.0125\n",
      "Epoch 80/100\n",
      "106/106 - 22s - loss: -6.0527e-01 - val_loss: -6.1678e-01 - lr: 0.0125\n",
      "Epoch 81/100\n",
      "106/106 - 22s - loss: -6.0653e-01 - val_loss: -6.7362e-01 - lr: 0.0125\n",
      "Epoch 82/100\n",
      "106/106 - 22s - loss: -6.0937e-01 - val_loss: -6.9708e-01 - lr: 0.0125\n",
      "Epoch 83/100\n",
      "106/106 - 22s - loss: -6.0795e-01 - val_loss: -6.4699e-01 - lr: 0.0125\n",
      "Epoch 84/100\n",
      "106/106 - 22s - loss: -6.0917e-01 - val_loss: -6.1964e-01 - lr: 0.0125\n",
      "Epoch 85/100\n",
      "106/106 - 22s - loss: -6.1005e-01 - val_loss: -6.3073e-01 - lr: 0.0125\n",
      "Epoch 86/100\n",
      "106/106 - 22s - loss: -6.0761e-01 - val_loss: -6.3345e-01 - lr: 0.0125\n",
      "Epoch 87/100\n",
      "106/106 - 22s - loss: -6.1034e-01 - val_loss: -6.3456e-01 - lr: 0.0125\n",
      "Epoch 88/100\n",
      "106/106 - 22s - loss: -6.1523e-01 - val_loss: -5.9279e-01 - lr: 0.0125\n",
      "Epoch 89/100\n",
      "106/106 - 22s - loss: -6.0745e-01 - val_loss: -6.7046e-01 - lr: 0.0125\n",
      "Epoch 90/100\n",
      "106/106 - 22s - loss: -6.1236e-01 - val_loss: -6.4354e-01 - lr: 0.0125\n",
      "Epoch 91/100\n",
      "106/106 - 22s - loss: -6.1541e-01 - val_loss: -6.4591e-01 - lr: 0.0125\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0062500000931322575.\n",
      "106/106 - 22s - loss: -6.1279e-01 - val_loss: -6.8096e-01 - lr: 0.0125\n",
      "Epoch 93/100\n",
      "106/106 - 22s - loss: -6.1879e-01 - val_loss: -6.7473e-01 - lr: 0.0063\n",
      "Epoch 94/100\n",
      "106/106 - 22s - loss: -6.1416e-01 - val_loss: -5.6618e-01 - lr: 0.0063\n",
      "Epoch 95/100\n",
      "106/106 - 22s - loss: -6.1871e-01 - val_loss: -6.1104e-01 - lr: 0.0063\n",
      "Epoch 96/100\n",
      "106/106 - 22s - loss: -6.1458e-01 - val_loss: -6.8706e-01 - lr: 0.0063\n",
      "Epoch 97/100\n",
      "106/106 - 22s - loss: -6.1384e-01 - val_loss: -6.3504e-01 - lr: 0.0063\n",
      "Epoch 98/100\n",
      "106/106 - 22s - loss: -6.2092e-01 - val_loss: -6.4617e-01 - lr: 0.0063\n",
      "Epoch 99/100\n",
      "106/106 - 22s - loss: -6.1603e-01 - val_loss: -6.4619e-01 - lr: 0.0063\n",
      "Epoch 100/100\n",
      "106/106 - 22s - loss: -6.1588e-01 - val_loss: -6.1787e-01 - lr: 0.0063\n",
      "Epoch 1/100\n",
      "106/106 - 22s - loss: -2.5931e-01 - val_loss: -4.0926e-01 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "106/106 - 22s - loss: -3.8822e-01 - val_loss: -4.7985e-01 - lr: 0.0100\n",
      "Epoch 3/100\n",
      "106/106 - 22s - loss: -4.1617e-01 - val_loss: -3.5038e-01 - lr: 0.0100\n",
      "Epoch 4/100\n",
      "106/106 - 22s - loss: -4.7248e-01 - val_loss: -2.8103e-01 - lr: 0.0100\n",
      "Epoch 5/100\n",
      "106/106 - 22s - loss: -4.7812e-01 - val_loss: -5.3007e-01 - lr: 0.0100\n",
      "Epoch 6/100\n",
      "106/106 - 22s - loss: -4.9771e-01 - val_loss: -5.4326e-01 - lr: 0.0100\n",
      "Epoch 7/100\n",
      "106/106 - 22s - loss: -5.1168e-01 - val_loss: -5.0212e-01 - lr: 0.0100\n",
      "Epoch 8/100\n",
      "106/106 - 22s - loss: -5.0392e-01 - val_loss: -5.1725e-01 - lr: 0.0100\n",
      "Epoch 9/100\n",
      "106/106 - 22s - loss: -5.1781e-01 - val_loss: -5.6869e-01 - lr: 0.0100\n",
      "Epoch 10/100\n",
      "106/106 - 22s - loss: -5.3038e-01 - val_loss: -5.6153e-01 - lr: 0.0100\n",
      "Epoch 11/100\n",
      "106/106 - 22s - loss: -5.2830e-01 - val_loss: -5.3936e-01 - lr: 0.0100\n",
      "Epoch 12/100\n",
      "106/106 - 22s - loss: -5.2513e-01 - val_loss: -6.1221e-01 - lr: 0.0100\n",
      "Epoch 13/100\n",
      "106/106 - 22s - loss: -5.3658e-01 - val_loss: -5.0868e-01 - lr: 0.0100\n",
      "Epoch 14/100\n",
      "106/106 - 22s - loss: -5.4327e-01 - val_loss: -5.4226e-01 - lr: 0.0100\n",
      "Epoch 15/100\n",
      "106/106 - 22s - loss: -5.4165e-01 - val_loss: -5.9158e-01 - lr: 0.0100\n",
      "Epoch 16/100\n",
      "106/106 - 22s - loss: -5.4471e-01 - val_loss: -2.5207e-01 - lr: 0.0100\n",
      "Epoch 17/100\n",
      "106/106 - 22s - loss: -5.4173e-01 - val_loss: -5.5529e-01 - lr: 0.0100\n",
      "Epoch 18/100\n",
      "106/106 - 22s - loss: -5.5158e-01 - val_loss: -5.4569e-01 - lr: 0.0100\n",
      "Epoch 19/100\n",
      "106/106 - 22s - loss: -5.4312e-01 - val_loss: -5.4997e-01 - lr: 0.0100\n",
      "Epoch 20/100\n",
      "106/106 - 22s - loss: -5.5817e-01 - val_loss: -5.2731e-01 - lr: 0.0100\n",
      "Epoch 21/100\n",
      "106/106 - 22s - loss: -5.5701e-01 - val_loss: -5.7281e-01 - lr: 0.0100\n",
      "Epoch 22/100\n",
      "106/106 - 22s - loss: -5.6988e-01 - val_loss: -6.4160e-01 - lr: 0.0100\n",
      "Epoch 23/100\n",
      "106/106 - 22s - loss: -5.5961e-01 - val_loss: -5.9641e-01 - lr: 0.0100\n",
      "Epoch 24/100\n",
      "106/106 - 22s - loss: -5.6915e-01 - val_loss: -2.8645e-01 - lr: 0.0100\n",
      "Epoch 25/100\n",
      "106/106 - 22s - loss: -5.5345e-01 - val_loss: -5.5657e-01 - lr: 0.0100\n",
      "Epoch 26/100\n",
      "106/106 - 22s - loss: -5.6451e-01 - val_loss: -5.6081e-01 - lr: 0.0100\n",
      "Epoch 27/100\n",
      "106/106 - 22s - loss: -5.6811e-01 - val_loss: -6.0857e-01 - lr: 0.0100\n",
      "Epoch 28/100\n",
      "106/106 - 22s - loss: -5.7179e-01 - val_loss: -6.1209e-01 - lr: 0.0100\n",
      "Epoch 29/100\n",
      "106/106 - 22s - loss: -5.7523e-01 - val_loss: -5.2156e-01 - lr: 0.0100\n",
      "Epoch 30/100\n",
      "106/106 - 22s - loss: -5.7638e-01 - val_loss: -4.9109e-01 - lr: 0.0100\n",
      "Epoch 31/100\n",
      "106/106 - 22s - loss: -5.7318e-01 - val_loss: -5.4633e-01 - lr: 0.0100\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "106/106 - 22s - loss: -5.6666e-01 - val_loss: -5.8382e-01 - lr: 0.0100\n",
      "Epoch 33/100\n",
      "106/106 - 22s - loss: -5.8734e-01 - val_loss: -5.6587e-01 - lr: 0.0050\n",
      "Epoch 34/100\n",
      "106/106 - 22s - loss: -5.8700e-01 - val_loss: -6.0735e-01 - lr: 0.0050\n",
      "Epoch 35/100\n",
      "106/106 - 22s - loss: -5.8622e-01 - val_loss: -6.0713e-01 - lr: 0.0050\n",
      "Epoch 36/100\n",
      "106/106 - 22s - loss: -5.8870e-01 - val_loss: -6.1023e-01 - lr: 0.0050\n",
      "Epoch 37/100\n",
      "106/106 - 22s - loss: -5.9087e-01 - val_loss: -6.1781e-01 - lr: 0.0050\n",
      "Epoch 38/100\n",
      "106/106 - 22s - loss: -5.8288e-01 - val_loss: -5.8483e-01 - lr: 0.0050\n",
      "Epoch 39/100\n",
      "106/106 - 22s - loss: -5.9891e-01 - val_loss: -6.0777e-01 - lr: 0.0050\n",
      "Epoch 40/100\n",
      "106/106 - 22s - loss: -5.9173e-01 - val_loss: -6.3519e-01 - lr: 0.0050\n",
      "Epoch 41/100\n",
      "106/106 - 22s - loss: -5.8901e-01 - val_loss: -6.1872e-01 - lr: 0.0050\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "106/106 - 22s - loss: -5.8903e-01 - val_loss: -6.3242e-01 - lr: 0.0050\n",
      "Epoch 43/100\n",
      "106/106 - 22s - loss: -6.0110e-01 - val_loss: -6.0822e-01 - lr: 0.0025\n",
      "Epoch 44/100\n",
      "106/106 - 22s - loss: -6.0408e-01 - val_loss: -6.1739e-01 - lr: 0.0025\n",
      "Epoch 45/100\n",
      "106/106 - 22s - loss: -6.0553e-01 - val_loss: -6.2629e-01 - lr: 0.0025\n",
      "Epoch 46/100\n",
      "106/106 - 22s - loss: -5.9940e-01 - val_loss: -6.5345e-01 - lr: 0.0025\n",
      "Epoch 47/100\n",
      "106/106 - 22s - loss: -6.0678e-01 - val_loss: -5.6654e-01 - lr: 0.0025\n",
      "Epoch 48/100\n",
      "106/106 - 22s - loss: -6.0307e-01 - val_loss: -6.5868e-01 - lr: 0.0025\n",
      "Epoch 49/100\n",
      "106/106 - 22s - loss: -6.0687e-01 - val_loss: -6.4280e-01 - lr: 0.0025\n",
      "Epoch 50/100\n",
      "106/106 - 22s - loss: -6.0494e-01 - val_loss: -6.5068e-01 - lr: 0.0025\n",
      "Epoch 51/100\n",
      "106/106 - 22s - loss: -6.0620e-01 - val_loss: -6.4256e-01 - lr: 0.0025\n",
      "Epoch 52/100\n",
      "106/106 - 22s - loss: -6.0520e-01 - val_loss: -6.1967e-01 - lr: 0.0025\n",
      "Epoch 53/100\n",
      "106/106 - 22s - loss: -6.1353e-01 - val_loss: -6.2957e-01 - lr: 0.0025\n",
      "Epoch 54/100\n",
      "106/106 - 22s - loss: -6.1055e-01 - val_loss: -6.2760e-01 - lr: 0.0025\n",
      "Epoch 55/100\n",
      "106/106 - 22s - loss: -6.1077e-01 - val_loss: -4.7184e-01 - lr: 0.0025\n",
      "Epoch 56/100\n",
      "106/106 - 22s - loss: -6.0832e-01 - val_loss: -5.4818e-01 - lr: 0.0025\n",
      "Epoch 57/100\n",
      "106/106 - 22s - loss: -6.1020e-01 - val_loss: -5.7281e-01 - lr: 0.0025\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "106/106 - 22s - loss: -6.1174e-01 - val_loss: -6.5543e-01 - lr: 0.0025\n",
      "Epoch 59/100\n",
      "106/106 - 22s - loss: -6.2042e-01 - val_loss: -6.1398e-01 - lr: 0.0012\n",
      "Epoch 60/100\n",
      "106/106 - 22s - loss: -6.1035e-01 - val_loss: -6.3450e-01 - lr: 0.0012\n",
      "Epoch 61/100\n",
      "106/106 - 22s - loss: -6.1664e-01 - val_loss: -6.3692e-01 - lr: 0.0012\n",
      "Epoch 62/100\n",
      "106/106 - 22s - loss: -6.1988e-01 - val_loss: -6.7115e-01 - lr: 0.0012\n",
      "Epoch 63/100\n",
      "106/106 - 22s - loss: -6.1446e-01 - val_loss: -6.5224e-01 - lr: 0.0012\n",
      "Epoch 64/100\n",
      "106/106 - 22s - loss: -6.1645e-01 - val_loss: -6.1513e-01 - lr: 0.0012\n",
      "Epoch 65/100\n",
      "106/106 - 22s - loss: -6.1742e-01 - val_loss: -6.5978e-01 - lr: 0.0012\n",
      "Epoch 66/100\n",
      "106/106 - 22s - loss: -6.2213e-01 - val_loss: -6.5389e-01 - lr: 0.0012\n",
      "Epoch 67/100\n",
      "106/106 - 22s - loss: -6.1579e-01 - val_loss: -6.0134e-01 - lr: 0.0012\n",
      "Epoch 68/100\n",
      "106/106 - 22s - loss: -6.2384e-01 - val_loss: -6.9088e-01 - lr: 0.0012\n",
      "Epoch 69/100\n",
      "106/106 - 22s - loss: -6.1592e-01 - val_loss: -6.3162e-01 - lr: 0.0012\n",
      "Epoch 70/100\n",
      "106/106 - 22s - loss: -6.1489e-01 - val_loss: -6.2015e-01 - lr: 0.0012\n",
      "Epoch 71/100\n",
      "106/106 - 22s - loss: -6.2200e-01 - val_loss: -6.7138e-01 - lr: 0.0012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "106/106 - 22s - loss: -6.1610e-01 - val_loss: -6.5540e-01 - lr: 0.0012\n",
      "Epoch 73/100\n",
      "106/106 - 22s - loss: -6.2050e-01 - val_loss: -5.0968e-01 - lr: 0.0012\n",
      "Epoch 74/100\n",
      "106/106 - 22s - loss: -6.2171e-01 - val_loss: -6.5452e-01 - lr: 0.0012\n",
      "Epoch 75/100\n",
      "106/106 - 22s - loss: -6.1896e-01 - val_loss: -6.6029e-01 - lr: 0.0012\n",
      "Epoch 76/100\n",
      "106/106 - 22s - loss: -6.2075e-01 - val_loss: -6.3658e-01 - lr: 0.0012\n",
      "Epoch 77/100\n",
      "106/106 - 22s - loss: -6.2102e-01 - val_loss: -5.9845e-01 - lr: 0.0012\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "106/106 - 22s - loss: -6.2121e-01 - val_loss: -6.7748e-01 - lr: 0.0012\n",
      "Epoch 79/100\n",
      "106/106 - 22s - loss: -6.2314e-01 - val_loss: -5.8916e-01 - lr: 6.2500e-04\n",
      "Epoch 80/100\n",
      "106/106 - 22s - loss: -6.2097e-01 - val_loss: -6.4477e-01 - lr: 6.2500e-04\n",
      "Epoch 81/100\n",
      "106/106 - 22s - loss: -6.2586e-01 - val_loss: -6.4935e-01 - lr: 6.2500e-04\n",
      "Epoch 82/100\n",
      "106/106 - 22s - loss: -6.2570e-01 - val_loss: -6.4474e-01 - lr: 6.2500e-04\n",
      "Epoch 83/100\n",
      "106/106 - 22s - loss: -6.2329e-01 - val_loss: -6.2303e-01 - lr: 6.2500e-04\n",
      "Epoch 84/100\n",
      "106/106 - 22s - loss: -6.2155e-01 - val_loss: -6.1051e-01 - lr: 6.2500e-04\n",
      "Epoch 85/100\n",
      "106/106 - 22s - loss: -6.2715e-01 - val_loss: -6.4118e-01 - lr: 6.2500e-04\n",
      "Epoch 86/100\n",
      "106/106 - 22s - loss: -6.2399e-01 - val_loss: -6.3605e-01 - lr: 6.2500e-04\n",
      "Epoch 87/100\n",
      "106/106 - 22s - loss: -6.2379e-01 - val_loss: -6.3481e-01 - lr: 6.2500e-04\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "106/106 - 22s - loss: -6.2395e-01 - val_loss: -6.6670e-01 - lr: 6.2500e-04\n",
      "Epoch 89/100\n",
      "106/106 - 22s - loss: -6.2191e-01 - val_loss: -7.0076e-01 - lr: 3.1250e-04\n",
      "Epoch 90/100\n",
      "106/106 - 22s - loss: -6.2892e-01 - val_loss: -6.2988e-01 - lr: 3.1250e-04\n",
      "Epoch 91/100\n",
      "106/106 - 22s - loss: -6.2440e-01 - val_loss: -6.1942e-01 - lr: 3.1250e-04\n",
      "Epoch 92/100\n",
      "106/106 - 22s - loss: -6.2909e-01 - val_loss: -6.1431e-01 - lr: 3.1250e-04\n",
      "Epoch 93/100\n",
      "106/106 - 22s - loss: -6.2962e-01 - val_loss: -6.4549e-01 - lr: 3.1250e-04\n",
      "Epoch 94/100\n",
      "106/106 - 22s - loss: -6.2999e-01 - val_loss: -6.7044e-01 - lr: 3.1250e-04\n",
      "Epoch 95/100\n",
      "106/106 - 22s - loss: -6.2819e-01 - val_loss: -6.6349e-01 - lr: 3.1250e-04\n",
      "Epoch 96/100\n",
      "106/106 - 22s - loss: -6.2691e-01 - val_loss: -6.3188e-01 - lr: 3.1250e-04\n",
      "Epoch 97/100\n",
      "106/106 - 22s - loss: -6.2910e-01 - val_loss: -6.7466e-01 - lr: 3.1250e-04\n",
      "Epoch 98/100\n",
      "106/106 - 22s - loss: -6.2772e-01 - val_loss: -6.2804e-01 - lr: 3.1250e-04\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "106/106 - 22s - loss: -6.2957e-01 - val_loss: -6.3211e-01 - lr: 3.1250e-04\n",
      "Epoch 100/100\n",
      "106/106 - 22s - loss: -6.2933e-01 - val_loss: -6.7157e-01 - lr: 1.5625e-04\n",
      "Epoch 1/100\n",
      "106/106 - 22s - loss: -3.2521e-01 - val_loss: -4.4551e-01 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "106/106 - 22s - loss: -3.9912e-01 - val_loss: -4.7438e-01 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "106/106 - 22s - loss: -4.4161e-01 - val_loss: -4.4196e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "106/106 - 22s - loss: -4.4466e-01 - val_loss: -4.3499e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "106/106 - 22s - loss: -4.7386e-01 - val_loss: -5.4049e-01 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "106/106 - 22s - loss: -4.6815e-01 - val_loss: -4.5928e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "106/106 - 22s - loss: -4.8941e-01 - val_loss: -5.1677e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "106/106 - 22s - loss: -4.9515e-01 - val_loss: -5.2955e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "106/106 - 22s - loss: -4.9918e-01 - val_loss: -4.6481e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "106/106 - 22s - loss: -5.0404e-01 - val_loss: -5.3793e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "106/106 - 22s - loss: -5.3039e-01 - val_loss: -5.3418e-01 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "106/106 - 22s - loss: -5.3507e-01 - val_loss: -4.5487e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "106/106 - 22s - loss: -5.1710e-01 - val_loss: -5.4805e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "106/106 - 22s - loss: -5.1401e-01 - val_loss: -5.0896e-01 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "106/106 - 22s - loss: -5.4351e-01 - val_loss: -5.6646e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "106/106 - 22s - loss: -5.3773e-01 - val_loss: -5.9963e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "106/106 - 22s - loss: -5.4884e-01 - val_loss: -5.8061e-01 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "106/106 - 22s - loss: -5.5094e-01 - val_loss: -6.1133e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "106/106 - 22s - loss: -5.3992e-01 - val_loss: -5.9060e-01 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "106/106 - 22s - loss: -5.5900e-01 - val_loss: -5.6930e-01 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "106/106 - 22s - loss: -5.5517e-01 - val_loss: -5.1630e-01 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "106/106 - 22s - loss: -5.4920e-01 - val_loss: -5.5170e-01 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "106/106 - 22s - loss: -5.6098e-01 - val_loss: -6.1521e-01 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "106/106 - 22s - loss: -5.6810e-01 - val_loss: -6.5342e-01 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "106/106 - 22s - loss: -5.5275e-01 - val_loss: -6.0614e-01 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "106/106 - 22s - loss: -5.6659e-01 - val_loss: -5.3492e-01 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "106/106 - 22s - loss: -5.6783e-01 - val_loss: -6.2058e-01 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "106/106 - 22s - loss: -5.6843e-01 - val_loss: -5.8591e-01 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "106/106 - 22s - loss: -5.7161e-01 - val_loss: -6.1240e-01 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "106/106 - 22s - loss: -5.7748e-01 - val_loss: -5.1949e-01 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "106/106 - 22s - loss: -5.6174e-01 - val_loss: -6.1889e-01 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "106/106 - 22s - loss: -5.7790e-01 - val_loss: -6.2647e-01 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "106/106 - 22s - loss: -5.7370e-01 - val_loss: -5.8465e-01 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "106/106 - 22s - loss: -5.7991e-01 - val_loss: -6.0749e-01 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "106/106 - 22s - loss: -5.8874e-01 - val_loss: -5.9224e-01 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "106/106 - 22s - loss: -5.9151e-01 - val_loss: -6.2999e-01 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "106/106 - 22s - loss: -5.9054e-01 - val_loss: -5.7256e-01 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "106/106 - 22s - loss: -5.9059e-01 - val_loss: -5.5776e-01 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "106/106 - 22s - loss: -5.9353e-01 - val_loss: -6.7232e-01 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "106/106 - 22s - loss: -5.9335e-01 - val_loss: -5.8122e-01 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "106/106 - 22s - loss: -5.9610e-01 - val_loss: -6.7708e-01 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "106/106 - 22s - loss: -5.9749e-01 - val_loss: -5.7449e-01 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "106/106 - 22s - loss: -5.9564e-01 - val_loss: -5.9785e-01 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "106/106 - 22s - loss: -6.0562e-01 - val_loss: -6.3501e-01 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "106/106 - 22s - loss: -6.0157e-01 - val_loss: -5.9250e-01 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "106/106 - 22s - loss: -6.0003e-01 - val_loss: -6.1299e-01 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "106/106 - 22s - loss: -6.0200e-01 - val_loss: -6.2400e-01 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "106/106 - 22s - loss: -6.0536e-01 - val_loss: -6.3313e-01 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "106/106 - 22s - loss: -5.9609e-01 - val_loss: -6.3146e-01 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "106/106 - 22s - loss: -6.0730e-01 - val_loss: -6.2297e-01 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "106/106 - 22s - loss: -6.0454e-01 - val_loss: -6.3020e-01 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "106/106 - 22s - loss: -6.1878e-01 - val_loss: -6.0000e-01 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "106/106 - 22s - loss: -6.1507e-01 - val_loss: -6.5831e-01 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "106/106 - 22s - loss: -6.1235e-01 - val_loss: -6.3895e-01 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "106/106 - 22s - loss: -6.1068e-01 - val_loss: -6.2985e-01 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "106/106 - 22s - loss: -6.1508e-01 - val_loss: -6.6680e-01 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "106/106 - 22s - loss: -6.2090e-01 - val_loss: -6.2678e-01 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "106/106 - 22s - loss: -6.1934e-01 - val_loss: -6.5306e-01 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "106/106 - 22s - loss: -6.2508e-01 - val_loss: -6.1757e-01 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "106/106 - 22s - loss: -6.1089e-01 - val_loss: -6.4224e-01 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "106/106 - 22s - loss: -6.2236e-01 - val_loss: -6.1617e-01 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "106/106 - 22s - loss: -6.2418e-01 - val_loss: -6.8056e-01 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "106/106 - 22s - loss: -6.2282e-01 - val_loss: -6.8584e-01 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "106/106 - 22s - loss: -6.2639e-01 - val_loss: -6.1851e-01 - lr: 1.2500e-04\n",
      "Epoch 65/100\n",
      "106/106 - 22s - loss: -6.2520e-01 - val_loss: -6.3868e-01 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "106/106 - 22s - loss: -6.2727e-01 - val_loss: -6.5931e-01 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "106/106 - 22s - loss: -6.2669e-01 - val_loss: -6.6967e-01 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "106/106 - 22s - loss: -6.2961e-01 - val_loss: -6.5280e-01 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "106/106 - 22s - loss: -6.2499e-01 - val_loss: -6.5423e-01 - lr: 1.2500e-04\n",
      "Epoch 70/100\n",
      "106/106 - 22s - loss: -6.2924e-01 - val_loss: -6.7414e-01 - lr: 1.2500e-04\n",
      "Epoch 71/100\n",
      "106/106 - 22s - loss: -6.3349e-01 - val_loss: -6.5850e-01 - lr: 1.2500e-04\n",
      "Epoch 72/100\n",
      "106/106 - 22s - loss: -6.2962e-01 - val_loss: -6.8249e-01 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "106/106 - 22s - loss: -6.2712e-01 - val_loss: -5.9217e-01 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "106/106 - 22s - loss: -6.2891e-01 - val_loss: -6.4308e-01 - lr: 6.2500e-05\n",
      "Epoch 75/100\n",
      "106/106 - 22s - loss: -6.3573e-01 - val_loss: -7.1197e-01 - lr: 6.2500e-05\n",
      "Epoch 76/100\n",
      "106/106 - 22s - loss: -6.3148e-01 - val_loss: -6.5746e-01 - lr: 6.2500e-05\n",
      "Epoch 77/100\n",
      "106/106 - 22s - loss: -6.3448e-01 - val_loss: -6.4219e-01 - lr: 6.2500e-05\n",
      "Epoch 78/100\n",
      "106/106 - 22s - loss: -6.3305e-01 - val_loss: -6.4389e-01 - lr: 6.2500e-05\n",
      "Epoch 79/100\n",
      "106/106 - 22s - loss: -6.3266e-01 - val_loss: -6.9003e-01 - lr: 6.2500e-05\n",
      "Epoch 80/100\n",
      "106/106 - 22s - loss: -6.3242e-01 - val_loss: -6.4966e-01 - lr: 6.2500e-05\n",
      "Epoch 81/100\n",
      "106/106 - 22s - loss: -6.3317e-01 - val_loss: -6.5472e-01 - lr: 6.2500e-05\n",
      "Epoch 82/100\n",
      "106/106 - 22s - loss: -6.3487e-01 - val_loss: -6.6436e-01 - lr: 6.2500e-05\n",
      "Epoch 83/100\n",
      "106/106 - 22s - loss: -6.3486e-01 - val_loss: -6.5743e-01 - lr: 6.2500e-05\n",
      "Epoch 84/100\n",
      "106/106 - 22s - loss: -6.3324e-01 - val_loss: -6.6662e-01 - lr: 6.2500e-05\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "106/106 - 22s - loss: -6.3594e-01 - val_loss: -6.7893e-01 - lr: 6.2500e-05\n",
      "Epoch 86/100\n",
      "106/106 - 22s - loss: -6.3572e-01 - val_loss: -6.6127e-01 - lr: 3.1250e-05\n",
      "Epoch 87/100\n",
      "106/106 - 22s - loss: -6.3265e-01 - val_loss: -6.5346e-01 - lr: 3.1250e-05\n",
      "Epoch 88/100\n",
      "106/106 - 22s - loss: -6.3701e-01 - val_loss: -6.6613e-01 - lr: 3.1250e-05\n",
      "Epoch 89/100\n",
      "106/106 - 22s - loss: -6.3896e-01 - val_loss: -6.7098e-01 - lr: 3.1250e-05\n",
      "Epoch 90/100\n",
      "106/106 - 22s - loss: -6.3502e-01 - val_loss: -6.8091e-01 - lr: 3.1250e-05\n",
      "Epoch 91/100\n",
      "106/106 - 22s - loss: -6.3564e-01 - val_loss: -6.9282e-01 - lr: 3.1250e-05\n",
      "Epoch 92/100\n",
      "106/106 - 22s - loss: -6.3947e-01 - val_loss: -6.9985e-01 - lr: 3.1250e-05\n",
      "Epoch 93/100\n",
      "106/106 - 22s - loss: -6.3630e-01 - val_loss: -6.6879e-01 - lr: 3.1250e-05\n",
      "Epoch 94/100\n",
      "106/106 - 22s - loss: -6.3952e-01 - val_loss: -6.4501e-01 - lr: 3.1250e-05\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "106/106 - 22s - loss: -6.3968e-01 - val_loss: -6.8542e-01 - lr: 3.1250e-05\n",
      "Epoch 96/100\n",
      "106/106 - 22s - loss: -6.3627e-01 - val_loss: -6.6579e-01 - lr: 1.5625e-05\n",
      "Epoch 97/100\n",
      "106/106 - 22s - loss: -6.3375e-01 - val_loss: -6.6731e-01 - lr: 1.5625e-05\n",
      "Epoch 98/100\n",
      "106/106 - 22s - loss: -6.4094e-01 - val_loss: -6.7288e-01 - lr: 1.5625e-05\n",
      "Epoch 99/100\n",
      "106/106 - 22s - loss: -6.3671e-01 - val_loss: -6.5842e-01 - lr: 1.5625e-05\n",
      "Epoch 100/100\n",
      "106/106 - 22s - loss: -6.4249e-01 - val_loss: -6.4376e-01 - lr: 1.5625e-05\n",
      "Epoch 1/100\n",
      "106/106 - 22s - loss: -2.8445e-01 - val_loss: -3.4748e-01 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "106/106 - 22s - loss: -3.9432e-01 - val_loss: -4.1203e-01 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "106/106 - 22s - loss: -4.1606e-01 - val_loss: -4.7828e-01 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "106/106 - 22s - loss: -4.5243e-01 - val_loss: -4.8203e-01 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "106/106 - 22s - loss: -4.5734e-01 - val_loss: -4.2324e-01 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "106/106 - 22s - loss: -4.7445e-01 - val_loss: -4.9966e-01 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "106/106 - 22s - loss: -4.8112e-01 - val_loss: -3.4930e-01 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "106/106 - 22s - loss: -4.9258e-01 - val_loss: -4.8494e-01 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "106/106 - 22s - loss: -4.9514e-01 - val_loss: -5.4623e-01 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "106/106 - 22s - loss: -5.0622e-01 - val_loss: -5.3448e-01 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "106/106 - 22s - loss: -5.0490e-01 - val_loss: -4.6634e-01 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "106/106 - 22s - loss: -5.0781e-01 - val_loss: -5.8331e-01 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "106/106 - 22s - loss: -5.2240e-01 - val_loss: -5.4494e-01 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "106/106 - 22s - loss: -5.2725e-01 - val_loss: -5.2862e-01 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "106/106 - 22s - loss: -5.2614e-01 - val_loss: -6.0590e-01 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "106/106 - 22s - loss: -5.3369e-01 - val_loss: -5.2102e-01 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "106/106 - 22s - loss: -5.2791e-01 - val_loss: -6.4804e-01 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "106/106 - 22s - loss: -5.1795e-01 - val_loss: -5.5559e-01 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "106/106 - 22s - loss: -5.3526e-01 - val_loss: -5.7809e-01 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "106/106 - 22s - loss: -5.2707e-01 - val_loss: -5.2111e-01 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "106/106 - 22s - loss: -5.4428e-01 - val_loss: -5.7963e-01 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "106/106 - 22s - loss: -5.4849e-01 - val_loss: -5.8136e-01 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "106/106 - 22s - loss: -5.4823e-01 - val_loss: -4.7781e-01 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "106/106 - 22s - loss: -5.5508e-01 - val_loss: -5.7955e-01 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "106/106 - 22s - loss: -5.5595e-01 - val_loss: -6.1786e-01 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "106/106 - 22s - loss: -5.5191e-01 - val_loss: -5.4474e-01 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "106/106 - 22s - loss: -5.3466e-01 - val_loss: -6.0450e-01 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "106/106 - 22s - loss: -5.6921e-01 - val_loss: -6.1910e-01 - lr: 5.0000e-05\n",
      "Epoch 29/100\n",
      "106/106 - 22s - loss: -5.7236e-01 - val_loss: -5.9424e-01 - lr: 5.0000e-05\n",
      "Epoch 30/100\n",
      "106/106 - 22s - loss: -5.7178e-01 - val_loss: -5.9580e-01 - lr: 5.0000e-05\n",
      "Epoch 31/100\n",
      "106/106 - 22s - loss: -5.7782e-01 - val_loss: -6.3532e-01 - lr: 5.0000e-05\n",
      "Epoch 32/100\n",
      "106/106 - 22s - loss: -5.7928e-01 - val_loss: -6.2424e-01 - lr: 5.0000e-05\n",
      "Epoch 33/100\n",
      "106/106 - 22s - loss: -5.6897e-01 - val_loss: -5.4581e-01 - lr: 5.0000e-05\n",
      "Epoch 34/100\n",
      "106/106 - 22s - loss: -5.7460e-01 - val_loss: -6.0387e-01 - lr: 5.0000e-05\n",
      "Epoch 35/100\n",
      "106/106 - 22s - loss: -5.7447e-01 - val_loss: -6.1645e-01 - lr: 5.0000e-05\n",
      "Epoch 36/100\n",
      "106/106 - 22s - loss: -5.6753e-01 - val_loss: -6.2785e-01 - lr: 5.0000e-05\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "106/106 - 22s - loss: -5.8579e-01 - val_loss: -5.6299e-01 - lr: 5.0000e-05\n",
      "Epoch 38/100\n",
      "106/106 - 22s - loss: -5.7989e-01 - val_loss: -6.4138e-01 - lr: 2.5000e-05\n",
      "Epoch 39/100\n",
      "106/106 - 22s - loss: -5.8339e-01 - val_loss: -5.8478e-01 - lr: 2.5000e-05\n",
      "Epoch 40/100\n",
      "106/106 - 22s - loss: -5.8375e-01 - val_loss: -6.4727e-01 - lr: 2.5000e-05\n",
      "Epoch 41/100\n",
      "106/106 - 22s - loss: -5.8365e-01 - val_loss: -6.1930e-01 - lr: 2.5000e-05\n",
      "Epoch 42/100\n",
      "106/106 - 22s - loss: -5.8872e-01 - val_loss: -6.3364e-01 - lr: 2.5000e-05\n",
      "Epoch 43/100\n",
      "106/106 - 22s - loss: -5.8716e-01 - val_loss: -6.0320e-01 - lr: 2.5000e-05\n",
      "Epoch 44/100\n",
      "106/106 - 22s - loss: -5.8565e-01 - val_loss: -6.2162e-01 - lr: 2.5000e-05\n",
      "Epoch 45/100\n",
      "106/106 - 22s - loss: -5.9122e-01 - val_loss: -5.6887e-01 - lr: 2.5000e-05\n",
      "Epoch 46/100\n",
      "106/106 - 22s - loss: -5.8855e-01 - val_loss: -6.0529e-01 - lr: 2.5000e-05\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "106/106 - 22s - loss: -5.9270e-01 - val_loss: -5.9719e-01 - lr: 2.5000e-05\n",
      "Epoch 48/100\n",
      "106/106 - 22s - loss: -5.8825e-01 - val_loss: -6.9214e-01 - lr: 1.2500e-05\n",
      "Epoch 49/100\n",
      "106/106 - 22s - loss: -5.9509e-01 - val_loss: -6.0507e-01 - lr: 1.2500e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "106/106 - 22s - loss: -5.9301e-01 - val_loss: -6.1770e-01 - lr: 1.2500e-05\n",
      "Epoch 51/100\n",
      "106/106 - 22s - loss: -5.9571e-01 - val_loss: -6.4333e-01 - lr: 1.2500e-05\n",
      "Epoch 52/100\n",
      "106/106 - 22s - loss: -5.9139e-01 - val_loss: -6.4831e-01 - lr: 1.2500e-05\n",
      "Epoch 53/100\n",
      "106/106 - 22s - loss: -5.9676e-01 - val_loss: -6.3475e-01 - lr: 1.2500e-05\n",
      "Epoch 54/100\n",
      "106/106 - 22s - loss: -6.0056e-01 - val_loss: -5.8326e-01 - lr: 1.2500e-05\n",
      "Epoch 55/100\n",
      "106/106 - 22s - loss: -5.9645e-01 - val_loss: -6.7171e-01 - lr: 1.2500e-05\n",
      "Epoch 56/100\n",
      "106/106 - 22s - loss: -5.9786e-01 - val_loss: -5.9151e-01 - lr: 1.2500e-05\n",
      "Epoch 57/100\n",
      "106/106 - 22s - loss: -5.9368e-01 - val_loss: -6.0745e-01 - lr: 1.2500e-05\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "106/106 - 22s - loss: -5.9573e-01 - val_loss: -6.5398e-01 - lr: 1.2500e-05\n",
      "Epoch 59/100\n",
      "106/106 - 22s - loss: -6.0220e-01 - val_loss: -6.5018e-01 - lr: 6.2500e-06\n",
      "Epoch 60/100\n",
      "106/106 - 22s - loss: -5.9989e-01 - val_loss: -6.5663e-01 - lr: 6.2500e-06\n",
      "Epoch 61/100\n",
      "106/106 - 22s - loss: -6.0057e-01 - val_loss: -6.4843e-01 - lr: 6.2500e-06\n",
      "Epoch 62/100\n",
      "106/106 - 22s - loss: -6.0019e-01 - val_loss: -6.4894e-01 - lr: 6.2500e-06\n",
      "Epoch 63/100\n",
      "106/106 - 22s - loss: -5.9899e-01 - val_loss: -6.3248e-01 - lr: 6.2500e-06\n",
      "Epoch 64/100\n",
      "106/106 - 22s - loss: -6.0225e-01 - val_loss: -6.5886e-01 - lr: 6.2500e-06\n",
      "Epoch 65/100\n",
      "106/106 - 22s - loss: -6.0217e-01 - val_loss: -6.1441e-01 - lr: 6.2500e-06\n",
      "Epoch 66/100\n",
      "106/106 - 22s - loss: -5.9728e-01 - val_loss: -6.5964e-01 - lr: 6.2500e-06\n",
      "Epoch 67/100\n",
      "106/106 - 22s - loss: -5.9996e-01 - val_loss: -6.2379e-01 - lr: 6.2500e-06\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "106/106 - 22s - loss: -5.9920e-01 - val_loss: -6.3575e-01 - lr: 6.2500e-06\n",
      "Epoch 69/100\n",
      "106/106 - 22s - loss: -5.9903e-01 - val_loss: -6.5053e-01 - lr: 3.1250e-06\n",
      "Epoch 70/100\n",
      "106/106 - 22s - loss: -5.9901e-01 - val_loss: -6.2872e-01 - lr: 3.1250e-06\n",
      "Epoch 71/100\n",
      "106/106 - 22s - loss: -5.9935e-01 - val_loss: -6.3579e-01 - lr: 3.1250e-06\n",
      "Epoch 72/100\n",
      "106/106 - 22s - loss: -6.0120e-01 - val_loss: -6.5188e-01 - lr: 3.1250e-06\n",
      "Epoch 73/100\n",
      "106/106 - 22s - loss: -6.0154e-01 - val_loss: -6.1155e-01 - lr: 3.1250e-06\n",
      "Epoch 74/100\n",
      "106/106 - 22s - loss: -6.0039e-01 - val_loss: -6.4928e-01 - lr: 3.1250e-06\n",
      "Epoch 75/100\n",
      "106/106 - 22s - loss: -6.0122e-01 - val_loss: -6.0449e-01 - lr: 3.1250e-06\n",
      "Epoch 76/100\n",
      "106/106 - 22s - loss: -6.0119e-01 - val_loss: -6.4437e-01 - lr: 3.1250e-06\n",
      "Epoch 77/100\n",
      "106/106 - 22s - loss: -6.0443e-01 - val_loss: -5.9155e-01 - lr: 3.1250e-06\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "106/106 - 22s - loss: -6.0273e-01 - val_loss: -6.7577e-01 - lr: 3.1250e-06\n",
      "Epoch 79/100\n",
      "106/106 - 22s - loss: -6.0446e-01 - val_loss: -6.3134e-01 - lr: 1.5625e-06\n",
      "Epoch 80/100\n",
      "106/106 - 22s - loss: -6.0143e-01 - val_loss: -6.5597e-01 - lr: 1.5625e-06\n",
      "Epoch 81/100\n",
      "106/106 - 22s - loss: -6.0697e-01 - val_loss: -6.5862e-01 - lr: 1.5625e-06\n",
      "Epoch 82/100\n",
      "106/106 - 22s - loss: -5.9860e-01 - val_loss: -6.2886e-01 - lr: 1.5625e-06\n",
      "Epoch 83/100\n",
      "106/106 - 22s - loss: -5.9899e-01 - val_loss: -6.0404e-01 - lr: 1.5625e-06\n",
      "Epoch 84/100\n",
      "106/106 - 22s - loss: -6.0730e-01 - val_loss: -6.7823e-01 - lr: 1.5625e-06\n",
      "Epoch 85/100\n",
      "106/106 - 22s - loss: -5.9892e-01 - val_loss: -6.6004e-01 - lr: 1.5625e-06\n",
      "Epoch 86/100\n",
      "106/106 - 22s - loss: -6.0114e-01 - val_loss: -6.0010e-01 - lr: 1.5625e-06\n",
      "Epoch 87/100\n",
      "106/106 - 22s - loss: -6.0166e-01 - val_loss: -6.5524e-01 - lr: 1.5625e-06\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-07.\n",
      "106/106 - 22s - loss: -6.0096e-01 - val_loss: -6.4679e-01 - lr: 1.5625e-06\n",
      "Epoch 89/100\n",
      "106/106 - 22s - loss: -6.0260e-01 - val_loss: -6.5090e-01 - lr: 7.8125e-07\n",
      "Epoch 90/100\n",
      "106/106 - 22s - loss: -6.0442e-01 - val_loss: -6.2083e-01 - lr: 7.8125e-07\n",
      "Epoch 91/100\n",
      "106/106 - 22s - loss: -6.0417e-01 - val_loss: -6.3587e-01 - lr: 7.8125e-07\n",
      "Epoch 92/100\n",
      "106/106 - 22s - loss: -6.0294e-01 - val_loss: -6.1599e-01 - lr: 7.8125e-07\n",
      "Epoch 93/100\n",
      "106/106 - 22s - loss: -6.1200e-01 - val_loss: -6.7230e-01 - lr: 7.8125e-07\n",
      "Epoch 94/100\n",
      "106/106 - 22s - loss: -6.0266e-01 - val_loss: -6.2277e-01 - lr: 7.8125e-07\n",
      "Epoch 95/100\n",
      "106/106 - 22s - loss: -6.0345e-01 - val_loss: -6.4341e-01 - lr: 7.8125e-07\n",
      "Epoch 96/100\n",
      "106/106 - 22s - loss: -6.0451e-01 - val_loss: -6.4813e-01 - lr: 7.8125e-07\n",
      "Epoch 97/100\n",
      "106/106 - 22s - loss: -6.0158e-01 - val_loss: -6.3617e-01 - lr: 7.8125e-07\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-07.\n",
      "106/106 - 22s - loss: -6.0412e-01 - val_loss: -6.3019e-01 - lr: 7.8125e-07\n",
      "Epoch 99/100\n",
      "106/106 - 22s - loss: -6.0783e-01 - val_loss: -6.3922e-01 - lr: 3.9062e-07\n",
      "Epoch 100/100\n",
      "106/106 - 22s - loss: -6.0729e-01 - val_loss: -6.3060e-01 - lr: 3.9062e-07\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,learning_array[i])\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"learning_rate_test\",str(learning_array[i])]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=100,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_learning[i,:] = history.history['val_loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/learning_rate.npy\",val_loss_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_learning = np.load(main_dir+\"parameters_analysis/learning_rate.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((4,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_learning[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_learning[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_learning[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_learning[3,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'learning rate = 0.1')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'learning rate = 0.01')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'learning rate = 0.001')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'learning rate = 0.0001')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different learning rates ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNFREEZING TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here the amount of layers that we keep frozen during unfreezing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [0,6,10]\n",
    "val_loss_unfreeze = np.zeros((3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "Epoch 1/100\n",
      "100/100 - 22s - loss: -2.9788e-01 - val_loss: -3.8092e-01 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "100/100 - 21s - loss: -3.9046e-01 - val_loss: -4.3624e-01 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "100/100 - 21s - loss: -4.2420e-01 - val_loss: -4.2136e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "100/100 - 21s - loss: -4.5544e-01 - val_loss: -5.0656e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "100/100 - 21s - loss: -4.5583e-01 - val_loss: -4.9677e-01 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "100/100 - 21s - loss: -4.8026e-01 - val_loss: -4.3482e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "100/100 - 21s - loss: -4.7958e-01 - val_loss: -5.1774e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "100/100 - 21s - loss: -4.9625e-01 - val_loss: -6.4258e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "100/100 - 21s - loss: -5.0226e-01 - val_loss: -4.9117e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "100/100 - 21s - loss: -5.1073e-01 - val_loss: -5.8798e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "100/100 - 21s - loss: -5.1075e-01 - val_loss: -4.5036e-01 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "100/100 - 21s - loss: -5.2247e-01 - val_loss: -5.0261e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "100/100 - 21s - loss: -5.2689e-01 - val_loss: -5.5186e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "100/100 - 21s - loss: -5.3383e-01 - val_loss: -5.7001e-01 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "100/100 - 21s - loss: -5.3801e-01 - val_loss: -4.8729e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "100/100 - 21s - loss: -5.3724e-01 - val_loss: -5.5364e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "100/100 - 21s - loss: -5.3330e-01 - val_loss: -5.7990e-01 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "100/100 - 21s - loss: -5.3866e-01 - val_loss: -5.3667e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "100/100 - 21s - loss: -5.4805e-01 - val_loss: -5.6363e-01 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "100/100 - 21s - loss: -5.6867e-01 - val_loss: -5.8623e-01 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "100/100 - 21s - loss: -5.7318e-01 - val_loss: -5.9092e-01 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "100/100 - 21s - loss: -5.7353e-01 - val_loss: -5.5875e-01 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "100/100 - 21s - loss: -5.6862e-01 - val_loss: -6.2325e-01 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "100/100 - 21s - loss: -5.6853e-01 - val_loss: -6.0621e-01 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "100/100 - 21s - loss: -5.6727e-01 - val_loss: -5.6192e-01 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "100/100 - 21s - loss: -5.7162e-01 - val_loss: -6.2372e-01 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "100/100 - 21s - loss: -5.8139e-01 - val_loss: -6.3503e-01 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "100/100 - 21s - loss: -5.8490e-01 - val_loss: -5.7202e-01 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "100/100 - 21s - loss: -5.8947e-01 - val_loss: -6.0058e-01 - lr: 2.5000e-04\n",
      "Epoch 30/100\n",
      "100/100 - 21s - loss: -5.8935e-01 - val_loss: -6.1504e-01 - lr: 2.5000e-04\n",
      "Epoch 31/100\n",
      "100/100 - 21s - loss: -5.8907e-01 - val_loss: -6.1376e-01 - lr: 2.5000e-04\n",
      "Epoch 32/100\n",
      "100/100 - 21s - loss: -5.9127e-01 - val_loss: -6.0604e-01 - lr: 2.5000e-04\n",
      "Epoch 33/100\n",
      "100/100 - 21s - loss: -5.9620e-01 - val_loss: -6.5448e-01 - lr: 2.5000e-04\n",
      "Epoch 34/100\n",
      "100/100 - 21s - loss: -5.9274e-01 - val_loss: -5.8802e-01 - lr: 2.5000e-04\n",
      "Epoch 35/100\n",
      "100/100 - 21s - loss: -5.9806e-01 - val_loss: -6.1531e-01 - lr: 2.5000e-04\n",
      "Epoch 36/100\n",
      "100/100 - 21s - loss: -5.9830e-01 - val_loss: -6.3654e-01 - lr: 2.5000e-04\n",
      "Epoch 37/100\n",
      "100/100 - 21s - loss: -5.9832e-01 - val_loss: -5.7156e-01 - lr: 2.5000e-04\n",
      "Epoch 38/100\n",
      "100/100 - 21s - loss: -5.9627e-01 - val_loss: -6.3751e-01 - lr: 2.5000e-04\n",
      "Epoch 39/100\n",
      "100/100 - 21s - loss: -5.9154e-01 - val_loss: -6.0973e-01 - lr: 2.5000e-04\n",
      "Epoch 40/100\n",
      "100/100 - 21s - loss: -5.9485e-01 - val_loss: -6.5531e-01 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "100/100 - 21s - loss: -5.9517e-01 - val_loss: -5.6139e-01 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "100/100 - 21s - loss: -6.0588e-01 - val_loss: -6.3020e-01 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "100/100 - 21s - loss: -6.0798e-01 - val_loss: -5.9906e-01 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "100/100 - 21s - loss: -6.0165e-01 - val_loss: -6.2005e-01 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "100/100 - 21s - loss: -6.0631e-01 - val_loss: -6.1875e-01 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "100/100 - 21s - loss: -6.0994e-01 - val_loss: -6.6018e-01 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "100/100 - 21s - loss: -6.1501e-01 - val_loss: -5.7190e-01 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "100/100 - 21s - loss: -6.1081e-01 - val_loss: -6.0626e-01 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "100/100 - 21s - loss: -6.1361e-01 - val_loss: -6.7247e-01 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "100/100 - 21s - loss: -6.1203e-01 - val_loss: -5.9690e-01 - lr: 2.5000e-04\n",
      "Epoch 51/100\n",
      "100/100 - 21s - loss: -6.1212e-01 - val_loss: -6.5695e-01 - lr: 2.5000e-04\n",
      "Epoch 52/100\n",
      "100/100 - 21s - loss: -6.1265e-01 - val_loss: -6.6076e-01 - lr: 2.5000e-04\n",
      "Epoch 53/100\n",
      "100/100 - 21s - loss: -6.1535e-01 - val_loss: -5.5798e-01 - lr: 2.5000e-04\n",
      "Epoch 54/100\n",
      "100/100 - 21s - loss: -6.1612e-01 - val_loss: -5.8350e-01 - lr: 2.5000e-04\n",
      "Epoch 55/100\n",
      "100/100 - 21s - loss: -6.1634e-01 - val_loss: -6.7292e-01 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "100/100 - 21s - loss: -6.1686e-01 - val_loss: -6.0968e-01 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "100/100 - 21s - loss: -6.1986e-01 - val_loss: -6.3697e-01 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "100/100 - 21s - loss: -6.2027e-01 - val_loss: -6.0084e-01 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "100/100 - 21s - loss: -6.1980e-01 - val_loss: -6.9198e-01 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "100/100 - 21s - loss: -6.2225e-01 - val_loss: -5.9256e-01 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "100/100 - 21s - loss: -6.2364e-01 - val_loss: -6.9380e-01 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "100/100 - 21s - loss: -6.1944e-01 - val_loss: -6.1129e-01 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "100/100 - 21s - loss: -6.2331e-01 - val_loss: -6.3305e-01 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "100/100 - 21s - loss: -6.1798e-01 - val_loss: -5.9096e-01 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "100/100 - 21s - loss: -6.2682e-01 - val_loss: -6.3208e-01 - lr: 2.5000e-04\n",
      "Epoch 66/100\n",
      "100/100 - 21s - loss: -6.1662e-01 - val_loss: -6.4619e-01 - lr: 2.5000e-04\n",
      "Epoch 67/100\n",
      "100/100 - 21s - loss: -6.2476e-01 - val_loss: -6.3015e-01 - lr: 2.5000e-04\n",
      "Epoch 68/100\n",
      "100/100 - 21s - loss: -6.2788e-01 - val_loss: -6.5631e-01 - lr: 2.5000e-04\n",
      "Epoch 69/100\n",
      "100/100 - 21s - loss: -6.2196e-01 - val_loss: -6.3854e-01 - lr: 2.5000e-04\n",
      "Epoch 70/100\n",
      "100/100 - 21s - loss: -6.2602e-01 - val_loss: -6.0947e-01 - lr: 2.5000e-04\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "100/100 - 21s - loss: -6.3103e-01 - val_loss: -6.2270e-01 - lr: 2.5000e-04\n",
      "Epoch 72/100\n",
      "100/100 - 21s - loss: -6.3767e-01 - val_loss: -6.2557e-01 - lr: 1.2500e-04\n",
      "Epoch 73/100\n",
      "100/100 - 21s - loss: -6.3666e-01 - val_loss: -6.8938e-01 - lr: 1.2500e-04\n",
      "Epoch 74/100\n",
      "100/100 - 21s - loss: -6.3251e-01 - val_loss: -6.6219e-01 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100\n",
      "100/100 - 21s - loss: -6.3365e-01 - val_loss: -6.1399e-01 - lr: 1.2500e-04\n",
      "Epoch 76/100\n",
      "100/100 - 21s - loss: -6.3067e-01 - val_loss: -6.6841e-01 - lr: 1.2500e-04\n",
      "Epoch 77/100\n",
      "100/100 - 21s - loss: -6.3228e-01 - val_loss: -6.5645e-01 - lr: 1.2500e-04\n",
      "Epoch 78/100\n",
      "100/100 - 21s - loss: -6.3460e-01 - val_loss: -6.0313e-01 - lr: 1.2500e-04\n",
      "Epoch 79/100\n",
      "100/100 - 21s - loss: -6.3447e-01 - val_loss: -6.2857e-01 - lr: 1.2500e-04\n",
      "Epoch 80/100\n",
      "100/100 - 21s - loss: -6.3577e-01 - val_loss: -6.7679e-01 - lr: 1.2500e-04\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "100/100 - 21s - loss: -6.3886e-01 - val_loss: -6.8448e-01 - lr: 1.2500e-04\n",
      "Epoch 82/100\n",
      "100/100 - 21s - loss: -6.4157e-01 - val_loss: -6.3709e-01 - lr: 6.2500e-05\n",
      "Epoch 83/100\n",
      "100/100 - 21s - loss: -6.4235e-01 - val_loss: -6.5970e-01 - lr: 6.2500e-05\n",
      "Epoch 84/100\n",
      "100/100 - 21s - loss: -6.3932e-01 - val_loss: -6.5694e-01 - lr: 6.2500e-05\n",
      "Epoch 85/100\n",
      "100/100 - 21s - loss: -6.4043e-01 - val_loss: -6.3806e-01 - lr: 6.2500e-05\n",
      "Epoch 86/100\n",
      "100/100 - 21s - loss: -6.4154e-01 - val_loss: -6.5133e-01 - lr: 6.2500e-05\n",
      "Epoch 87/100\n",
      "100/100 - 21s - loss: -6.4253e-01 - val_loss: -6.4269e-01 - lr: 6.2500e-05\n",
      "Epoch 88/100\n",
      "100/100 - 21s - loss: -6.4156e-01 - val_loss: -6.4304e-01 - lr: 6.2500e-05\n",
      "Epoch 89/100\n",
      "100/100 - 21s - loss: -6.4273e-01 - val_loss: -6.3825e-01 - lr: 6.2500e-05\n",
      "Epoch 90/100\n",
      "100/100 - 21s - loss: -6.4442e-01 - val_loss: -6.4346e-01 - lr: 6.2500e-05\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "100/100 - 21s - loss: -6.4531e-01 - val_loss: -6.6354e-01 - lr: 6.2500e-05\n",
      "Epoch 92/100\n",
      "100/100 - 21s - loss: -6.4496e-01 - val_loss: -6.3762e-01 - lr: 3.1250e-05\n",
      "Epoch 93/100\n",
      "100/100 - 21s - loss: -6.4841e-01 - val_loss: -6.3822e-01 - lr: 3.1250e-05\n",
      "Epoch 94/100\n",
      "100/100 - 21s - loss: -6.4387e-01 - val_loss: -6.9089e-01 - lr: 3.1250e-05\n",
      "Epoch 95/100\n",
      "100/100 - 21s - loss: -6.4523e-01 - val_loss: -6.4906e-01 - lr: 3.1250e-05\n",
      "Epoch 96/100\n",
      "100/100 - 21s - loss: -6.4291e-01 - val_loss: -6.4261e-01 - lr: 3.1250e-05\n",
      "Epoch 97/100\n",
      "100/100 - 21s - loss: -6.4782e-01 - val_loss: -6.4622e-01 - lr: 3.1250e-05\n",
      "Epoch 98/100\n",
      "100/100 - 21s - loss: -6.4578e-01 - val_loss: -7.1673e-01 - lr: 3.1250e-05\n",
      "Epoch 99/100\n",
      "100/100 - 21s - loss: -6.4686e-01 - val_loss: -6.1367e-01 - lr: 3.1250e-05\n",
      "Epoch 100/100\n",
      "100/100 - 21s - loss: -6.4622e-01 - val_loss: -6.8616e-01 - lr: 3.1250e-05\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"long_surgery_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_1 (Conv3D)          (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_batch_1 (BatchNormalizatio (None, 48, 64, 64, 4 192         long_conv3D_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_1 (Activation)  (None, 48, 64, 64, 4 0           long_batch_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_2 (Conv3D)          (None, 48, 64, 64, 4 62256       long_activation_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "long_batch_2 (BatchNormalizatio (None, 48, 64, 64, 4 192         long_conv3D_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_2 (Activation)  (None, 48, 64, 64, 4 0           long_batch_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_3 (Conv3D)          (None, 1, 64, 64, 48 49          long_activation_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_3 (Activation)  (None, 1, 64, 64, 48 0           long_conv3D_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 468,193\n",
      "Non-trainable params: 624\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "100/100 - 37s - loss: -3.6909e-01 - val_loss: -4.0729e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "100/100 - 37s - loss: -5.4525e-01 - val_loss: -4.8946e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "100/100 - 37s - loss: -5.7683e-01 - val_loss: -5.5508e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "100/100 - 37s - loss: -5.9105e-01 - val_loss: -6.4125e-01 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "100/100 - 37s - loss: -6.0362e-01 - val_loss: -5.2673e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "100/100 - 37s - loss: -6.1339e-01 - val_loss: -6.0864e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "100/100 - 37s - loss: -6.1356e-01 - val_loss: -6.1135e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "100/100 - 37s - loss: -6.2229e-01 - val_loss: -6.3226e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "100/100 - 37s - loss: -6.2728e-01 - val_loss: -6.4263e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "100/100 - 37s - loss: -6.2913e-01 - val_loss: -6.4372e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "100/100 - 37s - loss: -6.3428e-01 - val_loss: -6.3728e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "100/100 - 37s - loss: -6.3377e-01 - val_loss: -6.4996e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "100/100 - 37s - loss: -6.3493e-01 - val_loss: -6.6189e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "100/100 - 37s - loss: -6.4181e-01 - val_loss: -6.5659e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "100/100 - 37s - loss: -6.4505e-01 - val_loss: -6.1942e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "100/100 - 37s - loss: -6.4884e-01 - val_loss: -6.7436e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "100/100 - 37s - loss: -6.4460e-01 - val_loss: -6.7284e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "100/100 - 37s - loss: -6.5328e-01 - val_loss: -6.4831e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "100/100 - 37s - loss: -6.4745e-01 - val_loss: -6.5506e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "100/100 - 37s - loss: -6.5092e-01 - val_loss: -6.9916e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "100/100 - 37s - loss: -6.5256e-01 - val_loss: -7.0037e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "100/100 - 37s - loss: -6.5655e-01 - val_loss: -6.8623e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "100/100 - 37s - loss: -6.5967e-01 - val_loss: -5.8546e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "100/100 - 37s - loss: -6.6030e-01 - val_loss: -6.8422e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "100/100 - 37s - loss: -6.5637e-01 - val_loss: -6.5694e-01 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "100/100 - 37s - loss: -6.6365e-01 - val_loss: -5.9960e-01 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "100/100 - 37s - loss: -6.5772e-01 - val_loss: -6.5646e-01 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "100/100 - 37s - loss: -6.6559e-01 - val_loss: -6.3371e-01 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "100/100 - 37s - loss: -6.6503e-01 - val_loss: -6.6855e-01 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "100/100 - 37s - loss: -6.5898e-01 - val_loss: -6.8971e-01 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "100/100 - 37s - loss: -6.6833e-01 - val_loss: -6.7008e-01 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "100/100 - 37s - loss: -6.7147e-01 - val_loss: -6.8870e-01 - lr: 5.0000e-06\n",
      "Epoch 33/100\n",
      "100/100 - 37s - loss: -6.6935e-01 - val_loss: -6.4730e-01 - lr: 5.0000e-06\n",
      "Epoch 34/100\n",
      "100/100 - 37s - loss: -6.7731e-01 - val_loss: -6.8538e-01 - lr: 5.0000e-06\n",
      "Epoch 35/100\n",
      "100/100 - 37s - loss: -6.7170e-01 - val_loss: -6.6913e-01 - lr: 5.0000e-06\n",
      "Epoch 36/100\n",
      "100/100 - 37s - loss: -6.7616e-01 - val_loss: -6.6983e-01 - lr: 5.0000e-06\n",
      "Epoch 37/100\n",
      "100/100 - 37s - loss: -6.7235e-01 - val_loss: -6.5317e-01 - lr: 5.0000e-06\n",
      "Epoch 38/100\n",
      "100/100 - 37s - loss: -6.7176e-01 - val_loss: -6.6766e-01 - lr: 5.0000e-06\n",
      "Epoch 39/100\n",
      "100/100 - 37s - loss: -6.7013e-01 - val_loss: -6.7648e-01 - lr: 5.0000e-06\n",
      "Epoch 40/100\n",
      "100/100 - 37s - loss: -6.6877e-01 - val_loss: -6.6405e-01 - lr: 5.0000e-06\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "100/100 - 37s - loss: -6.7263e-01 - val_loss: -6.1976e-01 - lr: 5.0000e-06\n",
      "Epoch 42/100\n",
      "100/100 - 37s - loss: -6.7296e-01 - val_loss: -7.4362e-01 - lr: 2.5000e-06\n",
      "Epoch 43/100\n",
      "100/100 - 37s - loss: -6.7240e-01 - val_loss: -6.6649e-01 - lr: 2.5000e-06\n",
      "Epoch 44/100\n",
      "100/100 - 37s - loss: -6.7281e-01 - val_loss: -7.1625e-01 - lr: 2.5000e-06\n",
      "Epoch 45/100\n",
      "100/100 - 37s - loss: -6.6844e-01 - val_loss: -6.9177e-01 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "100/100 - 37s - loss: -6.7224e-01 - val_loss: -6.8892e-01 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "100/100 - 37s - loss: -6.7676e-01 - val_loss: -6.8992e-01 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "100/100 - 37s - loss: -6.7176e-01 - val_loss: -6.7833e-01 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "100/100 - 37s - loss: -6.7559e-01 - val_loss: -6.2053e-01 - lr: 2.5000e-06\n",
      "Epoch 50/100\n",
      "100/100 - 37s - loss: -6.7372e-01 - val_loss: -7.1930e-01 - lr: 2.5000e-06\n",
      "Epoch 51/100\n",
      "100/100 - 37s - loss: -6.7629e-01 - val_loss: -6.3778e-01 - lr: 2.5000e-06\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "100/100 - 37s - loss: -6.7330e-01 - val_loss: -6.6494e-01 - lr: 2.5000e-06\n",
      "Epoch 53/100\n",
      "100/100 - 37s - loss: -6.7983e-01 - val_loss: -6.6882e-01 - lr: 1.2500e-06\n",
      "Epoch 54/100\n",
      "100/100 - 37s - loss: -6.7607e-01 - val_loss: -6.9021e-01 - lr: 1.2500e-06\n",
      "Epoch 55/100\n",
      "100/100 - 37s - loss: -6.7747e-01 - val_loss: -6.8419e-01 - lr: 1.2500e-06\n",
      "Epoch 56/100\n",
      "100/100 - 37s - loss: -6.7629e-01 - val_loss: -6.7581e-01 - lr: 1.2500e-06\n",
      "Epoch 57/100\n",
      "100/100 - 37s - loss: -6.7767e-01 - val_loss: -6.4815e-01 - lr: 1.2500e-06\n",
      "Epoch 58/100\n",
      "100/100 - 37s - loss: -6.7737e-01 - val_loss: -6.9824e-01 - lr: 1.2500e-06\n",
      "Epoch 59/100\n",
      "100/100 - 37s - loss: -6.7631e-01 - val_loss: -6.8599e-01 - lr: 1.2500e-06\n",
      "Epoch 60/100\n",
      "100/100 - 37s - loss: -6.8150e-01 - val_loss: -6.6921e-01 - lr: 1.2500e-06\n",
      "Epoch 61/100\n",
      "100/100 - 37s - loss: -6.7429e-01 - val_loss: -6.9636e-01 - lr: 1.2500e-06\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "100/100 - 37s - loss: -6.7524e-01 - val_loss: -6.3325e-01 - lr: 1.2500e-06\n",
      "Epoch 63/100\n",
      "100/100 - 37s - loss: -6.7682e-01 - val_loss: -6.9558e-01 - lr: 6.2500e-07\n",
      "Epoch 64/100\n",
      "100/100 - 37s - loss: -6.7652e-01 - val_loss: -6.8911e-01 - lr: 6.2500e-07\n",
      "Epoch 65/100\n",
      "100/100 - 37s - loss: -6.8069e-01 - val_loss: -6.3232e-01 - lr: 6.2500e-07\n",
      "Epoch 66/100\n",
      "100/100 - 37s - loss: -6.8082e-01 - val_loss: -6.8583e-01 - lr: 6.2500e-07\n",
      "Epoch 67/100\n",
      "100/100 - 37s - loss: -6.7964e-01 - val_loss: -6.7484e-01 - lr: 6.2500e-07\n",
      "Epoch 68/100\n",
      "100/100 - 37s - loss: -6.7754e-01 - val_loss: -6.6419e-01 - lr: 6.2500e-07\n",
      "Epoch 69/100\n",
      "100/100 - 37s - loss: -6.7592e-01 - val_loss: -7.2296e-01 - lr: 6.2500e-07\n",
      "Epoch 70/100\n",
      "100/100 - 37s - loss: -6.7766e-01 - val_loss: -6.4488e-01 - lr: 6.2500e-07\n",
      "Epoch 71/100\n",
      "100/100 - 37s - loss: -6.7522e-01 - val_loss: -6.6157e-01 - lr: 6.2500e-07\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "100/100 - 37s - loss: -6.7518e-01 - val_loss: -7.0138e-01 - lr: 6.2500e-07\n",
      "Epoch 73/100\n",
      "100/100 - 37s - loss: -6.7785e-01 - val_loss: -6.6042e-01 - lr: 3.1250e-07\n",
      "Epoch 74/100\n",
      "100/100 - 37s - loss: -6.7851e-01 - val_loss: -7.1194e-01 - lr: 3.1250e-07\n",
      "Epoch 75/100\n",
      "100/100 - 37s - loss: -6.7750e-01 - val_loss: -6.7192e-01 - lr: 3.1250e-07\n",
      "Epoch 76/100\n",
      "100/100 - 37s - loss: -6.7970e-01 - val_loss: -6.6288e-01 - lr: 3.1250e-07\n",
      "Epoch 77/100\n",
      "100/100 - 37s - loss: -6.7887e-01 - val_loss: -6.8434e-01 - lr: 3.1250e-07\n",
      "Epoch 78/100\n",
      "100/100 - 37s - loss: -6.7689e-01 - val_loss: -6.7630e-01 - lr: 3.1250e-07\n",
      "Epoch 79/100\n",
      "100/100 - 37s - loss: -6.7966e-01 - val_loss: -6.7276e-01 - lr: 3.1250e-07\n",
      "Epoch 80/100\n",
      "100/100 - 37s - loss: -6.7840e-01 - val_loss: -6.6270e-01 - lr: 3.1250e-07\n",
      "Epoch 81/100\n",
      "100/100 - 37s - loss: -6.8019e-01 - val_loss: -6.4537e-01 - lr: 3.1250e-07\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "100/100 - 37s - loss: -6.7547e-01 - val_loss: -6.7377e-01 - lr: 3.1250e-07\n",
      "Epoch 83/100\n",
      "100/100 - 37s - loss: -6.7813e-01 - val_loss: -6.7914e-01 - lr: 1.5625e-07\n",
      "Epoch 84/100\n",
      "100/100 - 37s - loss: -6.7622e-01 - val_loss: -6.6297e-01 - lr: 1.5625e-07\n",
      "Epoch 85/100\n",
      "100/100 - 37s - loss: -6.7993e-01 - val_loss: -6.9710e-01 - lr: 1.5625e-07\n",
      "Epoch 86/100\n",
      "100/100 - 37s - loss: -6.8133e-01 - val_loss: -6.5618e-01 - lr: 1.5625e-07\n",
      "Epoch 87/100\n",
      "100/100 - 37s - loss: -6.8197e-01 - val_loss: -6.8521e-01 - lr: 1.5625e-07\n",
      "Epoch 88/100\n",
      "100/100 - 37s - loss: -6.8318e-01 - val_loss: -6.4498e-01 - lr: 1.5625e-07\n",
      "Epoch 89/100\n",
      "100/100 - 37s - loss: -6.7412e-01 - val_loss: -7.2448e-01 - lr: 1.5625e-07\n",
      "Epoch 90/100\n",
      "100/100 - 37s - loss: -6.8031e-01 - val_loss: -6.0272e-01 - lr: 1.5625e-07\n",
      "Epoch 91/100\n",
      "100/100 - 37s - loss: -6.8180e-01 - val_loss: -6.8114e-01 - lr: 1.5625e-07\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "100/100 - 37s - loss: -6.7721e-01 - val_loss: -7.1299e-01 - lr: 1.5625e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100\n",
      "100/100 - 37s - loss: -6.7838e-01 - val_loss: -6.2970e-01 - lr: 7.8125e-08\n",
      "Epoch 94/100\n",
      "100/100 - 37s - loss: -6.7467e-01 - val_loss: -6.8483e-01 - lr: 7.8125e-08\n",
      "Epoch 95/100\n",
      "100/100 - 37s - loss: -6.7770e-01 - val_loss: -6.8437e-01 - lr: 7.8125e-08\n",
      "Epoch 96/100\n",
      "100/100 - 37s - loss: -6.8039e-01 - val_loss: -6.6969e-01 - lr: 7.8125e-08\n",
      "Epoch 97/100\n",
      "100/100 - 37s - loss: -6.7735e-01 - val_loss: -6.5413e-01 - lr: 7.8125e-08\n",
      "Epoch 98/100\n",
      "100/100 - 37s - loss: -6.8394e-01 - val_loss: -6.9275e-01 - lr: 7.8125e-08\n",
      "Epoch 99/100\n",
      "100/100 - 37s - loss: -6.7880e-01 - val_loss: -6.5984e-01 - lr: 7.8125e-08\n",
      "Epoch 100/100\n",
      "100/100 - 37s - loss: -6.7506e-01 - val_loss: -6.8730e-01 - lr: 7.8125e-08\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"long_surgery_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_1 (Conv3D)          (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_batch_1 (BatchNormalizatio (None, 48, 64, 64, 4 192         long_conv3D_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_1 (Activation)  (None, 48, 64, 64, 4 0           long_batch_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_2 (Conv3D)          (None, 48, 64, 64, 4 62256       long_activation_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "long_batch_2 (BatchNormalizatio (None, 48, 64, 64, 4 192         long_conv3D_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_2 (Activation)  (None, 48, 64, 64, 4 0           long_batch_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_3 (Conv3D)          (None, 1, 64, 64, 48 49          long_activation_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_3 (Activation)  (None, 1, 64, 64, 48 0           long_conv3D_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 436,225\n",
      "Non-trainable params: 32,592\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "100/100 - 31s - loss: -4.0432e-01 - val_loss: -5.4137e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "100/100 - 30s - loss: -5.0051e-01 - val_loss: -5.6011e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "100/100 - 30s - loss: -5.3272e-01 - val_loss: -5.4701e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "100/100 - 30s - loss: -5.5674e-01 - val_loss: -6.0278e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "100/100 - 30s - loss: -5.6920e-01 - val_loss: -5.6509e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "100/100 - 30s - loss: -5.7947e-01 - val_loss: -5.8564e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "100/100 - 30s - loss: -5.8825e-01 - val_loss: -6.1940e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "100/100 - 31s - loss: -5.9648e-01 - val_loss: -6.2851e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "100/100 - 30s - loss: -6.0020e-01 - val_loss: -5.6138e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "100/100 - 31s - loss: -6.0318e-01 - val_loss: -6.4976e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "100/100 - 31s - loss: -6.0471e-01 - val_loss: -6.4363e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "100/100 - 31s - loss: -6.0748e-01 - val_loss: -5.7244e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "100/100 - 31s - loss: -6.1788e-01 - val_loss: -6.8947e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "100/100 - 31s - loss: -6.1304e-01 - val_loss: -5.9345e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "100/100 - 31s - loss: -6.1667e-01 - val_loss: -6.0692e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "100/100 - 31s - loss: -6.2795e-01 - val_loss: -6.4491e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "100/100 - 31s - loss: -6.2157e-01 - val_loss: -6.4147e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "100/100 - 30s - loss: -6.2493e-01 - val_loss: -6.8890e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "100/100 - 31s - loss: -6.3236e-01 - val_loss: -6.4202e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "100/100 - 31s - loss: -6.3446e-01 - val_loss: -6.1728e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "100/100 - 31s - loss: -6.2617e-01 - val_loss: -6.6669e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "100/100 - 31s - loss: -6.3299e-01 - val_loss: -6.6203e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "100/100 - 31s - loss: -6.3385e-01 - val_loss: -6.5286e-01 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      "100/100 - 31s - loss: -6.4064e-01 - val_loss: -6.8734e-01 - lr: 5.0000e-06\n",
      "Epoch 25/100\n",
      "100/100 - 31s - loss: -6.3957e-01 - val_loss: -6.4235e-01 - lr: 5.0000e-06\n",
      "Epoch 26/100\n",
      "100/100 - 31s - loss: -6.4018e-01 - val_loss: -6.3870e-01 - lr: 5.0000e-06\n",
      "Epoch 27/100\n",
      "100/100 - 31s - loss: -6.4411e-01 - val_loss: -6.7405e-01 - lr: 5.0000e-06\n",
      "Epoch 28/100\n",
      "100/100 - 31s - loss: -6.4163e-01 - val_loss: -6.4865e-01 - lr: 5.0000e-06\n",
      "Epoch 29/100\n",
      "100/100 - 31s - loss: -6.4085e-01 - val_loss: -6.9246e-01 - lr: 5.0000e-06\n",
      "Epoch 30/100\n",
      "100/100 - 31s - loss: -6.4293e-01 - val_loss: -6.5201e-01 - lr: 5.0000e-06\n",
      "Epoch 31/100\n",
      "100/100 - 31s - loss: -6.4473e-01 - val_loss: -6.8040e-01 - lr: 5.0000e-06\n",
      "Epoch 32/100\n",
      "100/100 - 31s - loss: -6.3937e-01 - val_loss: -6.3404e-01 - lr: 5.0000e-06\n",
      "Epoch 33/100\n",
      "100/100 - 31s - loss: -6.4849e-01 - val_loss: -6.7481e-01 - lr: 5.0000e-06\n",
      "Epoch 34/100\n",
      "100/100 - 31s - loss: -6.4912e-01 - val_loss: -6.4523e-01 - lr: 5.0000e-06\n",
      "Epoch 35/100\n",
      "100/100 - 31s - loss: -6.4076e-01 - val_loss: -6.6082e-01 - lr: 5.0000e-06\n",
      "Epoch 36/100\n",
      "100/100 - 31s - loss: -6.4847e-01 - val_loss: -6.4515e-01 - lr: 5.0000e-06\n",
      "Epoch 37/100\n",
      "100/100 - 31s - loss: -6.4675e-01 - val_loss: -6.2802e-01 - lr: 5.0000e-06\n",
      "Epoch 38/100\n",
      "100/100 - 31s - loss: -6.4486e-01 - val_loss: -6.7942e-01 - lr: 5.0000e-06\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "100/100 - 31s - loss: -6.4856e-01 - val_loss: -6.5420e-01 - lr: 5.0000e-06\n",
      "Epoch 40/100\n",
      "100/100 - 31s - loss: -6.4961e-01 - val_loss: -6.7319e-01 - lr: 2.5000e-06\n",
      "Epoch 41/100\n",
      "100/100 - 31s - loss: -6.5031e-01 - val_loss: -6.8432e-01 - lr: 2.5000e-06\n",
      "Epoch 42/100\n",
      "100/100 - 31s - loss: -6.4819e-01 - val_loss: -6.5472e-01 - lr: 2.5000e-06\n",
      "Epoch 43/100\n",
      "100/100 - 31s - loss: -6.5272e-01 - val_loss: -6.5443e-01 - lr: 2.5000e-06\n",
      "Epoch 44/100\n",
      "100/100 - 31s - loss: -6.4918e-01 - val_loss: -6.6167e-01 - lr: 2.5000e-06\n",
      "Epoch 45/100\n",
      "100/100 - 31s - loss: -6.4588e-01 - val_loss: -6.9613e-01 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "100/100 - 31s - loss: -6.5265e-01 - val_loss: -6.6594e-01 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "100/100 - 31s - loss: -6.5273e-01 - val_loss: -7.3157e-01 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "100/100 - 31s - loss: -6.4977e-01 - val_loss: -6.7435e-01 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "100/100 - 30s - loss: -6.4781e-01 - val_loss: -6.2876e-01 - lr: 2.5000e-06\n",
      "Epoch 50/100\n",
      "100/100 - 30s - loss: -6.5141e-01 - val_loss: -6.8452e-01 - lr: 2.5000e-06\n",
      "Epoch 51/100\n",
      "100/100 - 31s - loss: -6.4953e-01 - val_loss: -6.9257e-01 - lr: 2.5000e-06\n",
      "Epoch 52/100\n",
      "100/100 - 31s - loss: -6.4954e-01 - val_loss: -6.5710e-01 - lr: 2.5000e-06\n",
      "Epoch 53/100\n",
      "100/100 - 31s - loss: -6.5155e-01 - val_loss: -6.4087e-01 - lr: 2.5000e-06\n",
      "Epoch 54/100\n",
      "100/100 - 31s - loss: -6.5389e-01 - val_loss: -6.5932e-01 - lr: 2.5000e-06\n",
      "Epoch 55/100\n",
      "100/100 - 31s - loss: -6.5329e-01 - val_loss: -6.7321e-01 - lr: 2.5000e-06\n",
      "Epoch 56/100\n",
      "100/100 - 31s - loss: -6.5408e-01 - val_loss: -6.3151e-01 - lr: 2.5000e-06\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "100/100 - 31s - loss: -6.5526e-01 - val_loss: -6.9301e-01 - lr: 2.5000e-06\n",
      "Epoch 58/100\n",
      "100/100 - 31s - loss: -6.5385e-01 - val_loss: -6.9599e-01 - lr: 1.2500e-06\n",
      "Epoch 59/100\n",
      "100/100 - 31s - loss: -6.5415e-01 - val_loss: -6.5608e-01 - lr: 1.2500e-06\n",
      "Epoch 60/100\n",
      "100/100 - 31s - loss: -6.5636e-01 - val_loss: -6.6966e-01 - lr: 1.2500e-06\n",
      "Epoch 61/100\n",
      "100/100 - 31s - loss: -6.5226e-01 - val_loss: -6.5759e-01 - lr: 1.2500e-06\n",
      "Epoch 62/100\n",
      "100/100 - 31s - loss: -6.5164e-01 - val_loss: -7.0123e-01 - lr: 1.2500e-06\n",
      "Epoch 63/100\n",
      "100/100 - 31s - loss: -6.5554e-01 - val_loss: -6.4495e-01 - lr: 1.2500e-06\n",
      "Epoch 64/100\n",
      "100/100 - 31s - loss: -6.5701e-01 - val_loss: -6.6466e-01 - lr: 1.2500e-06\n",
      "Epoch 65/100\n",
      "100/100 - 31s - loss: -6.5218e-01 - val_loss: -6.7602e-01 - lr: 1.2500e-06\n",
      "Epoch 66/100\n",
      "100/100 - 31s - loss: -6.5820e-01 - val_loss: -6.7036e-01 - lr: 1.2500e-06\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "100/100 - 31s - loss: -6.5252e-01 - val_loss: -6.4799e-01 - lr: 1.2500e-06\n",
      "Epoch 68/100\n",
      "100/100 - 31s - loss: -6.5875e-01 - val_loss: -6.1834e-01 - lr: 6.2500e-07\n",
      "Epoch 69/100\n",
      "100/100 - 31s - loss: -6.5092e-01 - val_loss: -7.1497e-01 - lr: 6.2500e-07\n",
      "Epoch 70/100\n",
      "100/100 - 31s - loss: -6.5339e-01 - val_loss: -6.3945e-01 - lr: 6.2500e-07\n",
      "Epoch 71/100\n",
      "100/100 - 31s - loss: -6.5573e-01 - val_loss: -6.6123e-01 - lr: 6.2500e-07\n",
      "Epoch 72/100\n",
      "100/100 - 31s - loss: -6.4992e-01 - val_loss: -6.7612e-01 - lr: 6.2500e-07\n",
      "Epoch 73/100\n",
      "100/100 - 31s - loss: -6.5661e-01 - val_loss: -6.7471e-01 - lr: 6.2500e-07\n",
      "Epoch 74/100\n",
      "100/100 - 31s - loss: -6.5410e-01 - val_loss: -6.5291e-01 - lr: 6.2500e-07\n",
      "Epoch 75/100\n",
      "100/100 - 31s - loss: -6.5615e-01 - val_loss: -6.7553e-01 - lr: 6.2500e-07\n",
      "Epoch 76/100\n",
      "100/100 - 31s - loss: -6.5375e-01 - val_loss: -6.2780e-01 - lr: 6.2500e-07\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "100/100 - 31s - loss: -6.5189e-01 - val_loss: -6.4785e-01 - lr: 6.2500e-07\n",
      "Epoch 78/100\n",
      "100/100 - 31s - loss: -6.5459e-01 - val_loss: -6.8821e-01 - lr: 3.1250e-07\n",
      "Epoch 79/100\n",
      "100/100 - 31s - loss: -6.5700e-01 - val_loss: -6.6256e-01 - lr: 3.1250e-07\n",
      "Epoch 80/100\n",
      "100/100 - 31s - loss: -6.5503e-01 - val_loss: -6.4576e-01 - lr: 3.1250e-07\n",
      "Epoch 81/100\n",
      "100/100 - 31s - loss: -6.5708e-01 - val_loss: -6.1150e-01 - lr: 3.1250e-07\n",
      "Epoch 82/100\n",
      "100/100 - 31s - loss: -6.5633e-01 - val_loss: -6.9674e-01 - lr: 3.1250e-07\n",
      "Epoch 83/100\n",
      "100/100 - 31s - loss: -6.5185e-01 - val_loss: -6.6004e-01 - lr: 3.1250e-07\n",
      "Epoch 84/100\n",
      "100/100 - 31s - loss: -6.5703e-01 - val_loss: -6.8309e-01 - lr: 3.1250e-07\n",
      "Epoch 85/100\n",
      "100/100 - 31s - loss: -6.5426e-01 - val_loss: -6.7133e-01 - lr: 3.1250e-07\n",
      "Epoch 86/100\n",
      "100/100 - 31s - loss: -6.6215e-01 - val_loss: -6.4594e-01 - lr: 3.1250e-07\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "100/100 - 31s - loss: -6.5556e-01 - val_loss: -6.7990e-01 - lr: 3.1250e-07\n",
      "Epoch 88/100\n",
      "100/100 - 31s - loss: -6.5573e-01 - val_loss: -6.7399e-01 - lr: 1.5625e-07\n",
      "Epoch 89/100\n",
      "100/100 - 31s - loss: -6.5811e-01 - val_loss: -6.5876e-01 - lr: 1.5625e-07\n",
      "Epoch 90/100\n",
      "100/100 - 31s - loss: -6.5536e-01 - val_loss: -6.4586e-01 - lr: 1.5625e-07\n",
      "Epoch 91/100\n",
      "100/100 - 31s - loss: -6.5732e-01 - val_loss: -6.9683e-01 - lr: 1.5625e-07\n",
      "Epoch 92/100\n",
      "100/100 - 31s - loss: -6.6170e-01 - val_loss: -6.9200e-01 - lr: 1.5625e-07\n",
      "Epoch 93/100\n",
      "100/100 - 30s - loss: -6.5500e-01 - val_loss: -6.6370e-01 - lr: 1.5625e-07\n",
      "Epoch 94/100\n",
      "100/100 - 30s - loss: -6.5179e-01 - val_loss: -6.3278e-01 - lr: 1.5625e-07\n",
      "Epoch 95/100\n",
      "100/100 - 30s - loss: -6.5813e-01 - val_loss: -6.9005e-01 - lr: 1.5625e-07\n",
      "Epoch 96/100\n",
      "100/100 - 31s - loss: -6.5466e-01 - val_loss: -6.7520e-01 - lr: 1.5625e-07\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "100/100 - 31s - loss: -6.5261e-01 - val_loss: -6.4246e-01 - lr: 1.5625e-07\n",
      "Epoch 98/100\n",
      "100/100 - 31s - loss: -6.5551e-01 - val_loss: -6.4067e-01 - lr: 7.8125e-08\n",
      "Epoch 99/100\n",
      "100/100 - 30s - loss: -6.5482e-01 - val_loss: -6.9989e-01 - lr: 7.8125e-08\n",
      "Epoch 100/100\n",
      "100/100 - 31s - loss: -6.5837e-01 - val_loss: -6.5336e-01 - lr: 7.8125e-08\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"long_surgery_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_1 (Conv3D)          (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_batch_1 (BatchNormalizatio (None, 48, 64, 64, 4 192         long_conv3D_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_1 (Activation)  (None, 48, 64, 64, 4 0           long_batch_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_2 (Conv3D)          (None, 48, 64, 64, 4 62256       long_activation_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "long_batch_2 (BatchNormalizatio (None, 48, 64, 64, 4 192         long_conv3D_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_2 (Activation)  (None, 48, 64, 64, 4 0           long_batch_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "long_conv3D_3 (Conv3D)          (None, 1, 64, 64, 48 49          long_activation_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "long_activation_3 (Activation)  (None, 1, 64, 64, 48 0           long_conv3D_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 373,873\n",
      "Non-trainable params: 94,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 30s - loss: -4.3118e-01 - val_loss: -4.8639e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "100/100 - 29s - loss: -5.1273e-01 - val_loss: -5.6165e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "100/100 - 29s - loss: -5.2334e-01 - val_loss: -5.8524e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "100/100 - 29s - loss: -5.5351e-01 - val_loss: -5.6223e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "100/100 - 30s - loss: -5.6527e-01 - val_loss: -5.8996e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "100/100 - 30s - loss: -5.6924e-01 - val_loss: -6.0642e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "100/100 - 29s - loss: -5.8292e-01 - val_loss: -5.6345e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "100/100 - 30s - loss: -5.8879e-01 - val_loss: -6.1073e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "100/100 - 29s - loss: -5.9337e-01 - val_loss: -5.9072e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "100/100 - 29s - loss: -5.9829e-01 - val_loss: -6.0853e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "100/100 - 30s - loss: -6.0305e-01 - val_loss: -6.2047e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "100/100 - 29s - loss: -6.0170e-01 - val_loss: -5.9492e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "100/100 - 30s - loss: -6.0985e-01 - val_loss: -6.5847e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "100/100 - 30s - loss: -6.1357e-01 - val_loss: -5.8871e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "100/100 - 30s - loss: -6.1920e-01 - val_loss: -6.7535e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "100/100 - 29s - loss: -6.0880e-01 - val_loss: -6.1030e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "100/100 - 29s - loss: -6.1950e-01 - val_loss: -6.2888e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "100/100 - 29s - loss: -6.2331e-01 - val_loss: -6.4660e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "100/100 - 29s - loss: -6.2352e-01 - val_loss: -5.9524e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "100/100 - 30s - loss: -6.2695e-01 - val_loss: -6.7736e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "100/100 - 29s - loss: -6.2495e-01 - val_loss: -6.5009e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "100/100 - 29s - loss: -6.2423e-01 - val_loss: -6.0086e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "100/100 - 29s - loss: -6.3127e-01 - val_loss: -6.5250e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "100/100 - 29s - loss: -6.3145e-01 - val_loss: -6.1378e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "100/100 - 30s - loss: -6.2746e-01 - val_loss: -6.5782e-01 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "100/100 - 29s - loss: -6.3365e-01 - val_loss: -6.2831e-01 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "100/100 - 29s - loss: -6.3090e-01 - val_loss: -6.3783e-01 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "100/100 - 30s - loss: -6.3736e-01 - val_loss: -6.8083e-01 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "100/100 - 29s - loss: -6.3869e-01 - val_loss: -6.3509e-01 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "100/100 - 30s - loss: -6.3062e-01 - val_loss: -6.3837e-01 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "100/100 - 29s - loss: -6.3933e-01 - val_loss: -6.2540e-01 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "100/100 - 30s - loss: -6.4116e-01 - val_loss: -6.6842e-01 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "100/100 - 29s - loss: -6.3646e-01 - val_loss: -6.2622e-01 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "100/100 - 29s - loss: -6.3897e-01 - val_loss: -6.6666e-01 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "100/100 - 29s - loss: -6.3617e-01 - val_loss: -6.4951e-01 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "100/100 - 29s - loss: -6.4683e-01 - val_loss: -6.4642e-01 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "100/100 - 29s - loss: -6.4295e-01 - val_loss: -6.4245e-01 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "100/100 - 30s - loss: -6.4405e-01 - val_loss: -6.8393e-01 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "100/100 - 29s - loss: -6.4264e-01 - val_loss: -6.1262e-01 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "100/100 - 29s - loss: -6.4262e-01 - val_loss: -6.6108e-01 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "100/100 - 29s - loss: -6.4832e-01 - val_loss: -6.7036e-01 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "100/100 - 29s - loss: -6.4603e-01 - val_loss: -6.2682e-01 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "100/100 - 29s - loss: -6.4233e-01 - val_loss: -6.3542e-01 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "100/100 - 29s - loss: -6.4481e-01 - val_loss: -6.5401e-01 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "100/100 - 29s - loss: -6.4834e-01 - val_loss: -6.7916e-01 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "100/100 - 29s - loss: -6.5000e-01 - val_loss: -6.5387e-01 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "100/100 - 30s - loss: -6.4746e-01 - val_loss: -7.0038e-01 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "100/100 - 29s - loss: -6.4760e-01 - val_loss: -6.5416e-01 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "100/100 - 29s - loss: -6.4886e-01 - val_loss: -6.3364e-01 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "100/100 - 29s - loss: -6.4838e-01 - val_loss: -6.8545e-01 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "100/100 - 29s - loss: -6.5126e-01 - val_loss: -6.3710e-01 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "100/100 - 29s - loss: -6.4486e-01 - val_loss: -6.6780e-01 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "100/100 - 29s - loss: -6.5065e-01 - val_loss: -6.6393e-01 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "100/100 - 29s - loss: -6.4935e-01 - val_loss: -6.6015e-01 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "100/100 - 30s - loss: -6.5455e-01 - val_loss: -6.7909e-01 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "100/100 - 29s - loss: -6.5620e-01 - val_loss: -6.3800e-01 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "100/100 - 29s - loss: -6.5201e-01 - val_loss: -6.5342e-01 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "100/100 - 29s - loss: -6.4596e-01 - val_loss: -6.6915e-01 - lr: 5.0000e-06\n",
      "Epoch 59/100\n",
      "100/100 - 30s - loss: -6.5370e-01 - val_loss: -6.3014e-01 - lr: 5.0000e-06\n",
      "Epoch 60/100\n",
      "100/100 - 29s - loss: -6.5125e-01 - val_loss: -6.6060e-01 - lr: 5.0000e-06\n",
      "Epoch 61/100\n",
      "100/100 - 29s - loss: -6.5397e-01 - val_loss: -6.9617e-01 - lr: 5.0000e-06\n",
      "Epoch 62/100\n",
      "100/100 - 29s - loss: -6.5656e-01 - val_loss: -6.4327e-01 - lr: 5.0000e-06\n",
      "Epoch 63/100\n",
      "100/100 - 30s - loss: -6.5361e-01 - val_loss: -6.7029e-01 - lr: 5.0000e-06\n",
      "Epoch 64/100\n",
      "100/100 - 29s - loss: -6.5405e-01 - val_loss: -6.5381e-01 - lr: 5.0000e-06\n",
      "Epoch 65/100\n",
      "100/100 - 29s - loss: -6.5235e-01 - val_loss: -6.8428e-01 - lr: 5.0000e-06\n",
      "Epoch 66/100\n",
      "100/100 - 30s - loss: -6.5423e-01 - val_loss: -6.7082e-01 - lr: 5.0000e-06\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "100/100 - 29s - loss: -6.6238e-01 - val_loss: -6.8250e-01 - lr: 5.0000e-06\n",
      "Epoch 68/100\n",
      "100/100 - 29s - loss: -6.5049e-01 - val_loss: -6.3152e-01 - lr: 2.5000e-06\n",
      "Epoch 69/100\n",
      "100/100 - 30s - loss: -6.5284e-01 - val_loss: -6.5509e-01 - lr: 2.5000e-06\n",
      "Epoch 70/100\n",
      "100/100 - 30s - loss: -6.5936e-01 - val_loss: -6.7288e-01 - lr: 2.5000e-06\n",
      "Epoch 71/100\n",
      "100/100 - 30s - loss: -6.5452e-01 - val_loss: -6.4479e-01 - lr: 2.5000e-06\n",
      "Epoch 72/100\n",
      "100/100 - 30s - loss: -6.5972e-01 - val_loss: -6.6889e-01 - lr: 2.5000e-06\n",
      "Epoch 73/100\n",
      "100/100 - 29s - loss: -6.5575e-01 - val_loss: -6.7842e-01 - lr: 2.5000e-06\n",
      "Epoch 74/100\n",
      "100/100 - 30s - loss: -6.5408e-01 - val_loss: -6.5845e-01 - lr: 2.5000e-06\n",
      "Epoch 75/100\n",
      "100/100 - 30s - loss: -6.5043e-01 - val_loss: -6.5706e-01 - lr: 2.5000e-06\n",
      "Epoch 76/100\n",
      "100/100 - 30s - loss: -6.5748e-01 - val_loss: -6.4807e-01 - lr: 2.5000e-06\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "100/100 - 30s - loss: -6.5814e-01 - val_loss: -6.5709e-01 - lr: 2.5000e-06\n",
      "Epoch 78/100\n",
      "100/100 - 30s - loss: -6.6054e-01 - val_loss: -6.6539e-01 - lr: 1.2500e-06\n",
      "Epoch 79/100\n",
      "100/100 - 29s - loss: -6.5344e-01 - val_loss: -6.8152e-01 - lr: 1.2500e-06\n",
      "Epoch 80/100\n",
      "100/100 - 29s - loss: -6.5862e-01 - val_loss: -6.3189e-01 - lr: 1.2500e-06\n",
      "Epoch 81/100\n",
      "100/100 - 29s - loss: -6.5743e-01 - val_loss: -6.6763e-01 - lr: 1.2500e-06\n",
      "Epoch 82/100\n",
      "100/100 - 29s - loss: -6.5679e-01 - val_loss: -6.8337e-01 - lr: 1.2500e-06\n",
      "Epoch 83/100\n",
      "100/100 - 29s - loss: -6.5217e-01 - val_loss: -6.7103e-01 - lr: 1.2500e-06\n",
      "Epoch 84/100\n",
      "100/100 - 29s - loss: -6.5377e-01 - val_loss: -5.9125e-01 - lr: 1.2500e-06\n",
      "Epoch 85/100\n",
      "100/100 - 29s - loss: -6.5577e-01 - val_loss: -6.8423e-01 - lr: 1.2500e-06\n",
      "Epoch 86/100\n",
      "100/100 - 29s - loss: -6.5866e-01 - val_loss: -6.9654e-01 - lr: 1.2500e-06\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "100/100 - 29s - loss: -6.5594e-01 - val_loss: -6.7538e-01 - lr: 1.2500e-06\n",
      "Epoch 88/100\n",
      "100/100 - 29s - loss: -6.6156e-01 - val_loss: -6.8414e-01 - lr: 6.2500e-07\n",
      "Epoch 89/100\n",
      "100/100 - 29s - loss: -6.5313e-01 - val_loss: -6.7352e-01 - lr: 6.2500e-07\n",
      "Epoch 90/100\n",
      "100/100 - 30s - loss: -6.5995e-01 - val_loss: -6.6445e-01 - lr: 6.2500e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      "100/100 - 29s - loss: -6.6125e-01 - val_loss: -6.3184e-01 - lr: 6.2500e-07\n",
      "Epoch 92/100\n",
      "100/100 - 29s - loss: -6.5996e-01 - val_loss: -6.8398e-01 - lr: 6.2500e-07\n",
      "Epoch 93/100\n",
      "100/100 - 29s - loss: -6.5442e-01 - val_loss: -6.3996e-01 - lr: 6.2500e-07\n",
      "Epoch 94/100\n",
      "100/100 - 29s - loss: -6.5596e-01 - val_loss: -6.4215e-01 - lr: 6.2500e-07\n",
      "Epoch 95/100\n",
      "100/100 - 30s - loss: -6.5792e-01 - val_loss: -6.5991e-01 - lr: 6.2500e-07\n",
      "Epoch 96/100\n",
      "100/100 - 29s - loss: -6.6279e-01 - val_loss: -6.5148e-01 - lr: 6.2500e-07\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "100/100 - 30s - loss: -6.5702e-01 - val_loss: -6.6658e-01 - lr: 6.2500e-07\n",
      "Epoch 98/100\n",
      "100/100 - 29s - loss: -6.5894e-01 - val_loss: -6.4948e-01 - lr: 3.1250e-07\n",
      "Epoch 99/100\n",
      "100/100 - 29s - loss: -6.5971e-01 - val_loss: -6.4967e-01 - lr: 3.1250e-07\n",
      "Epoch 100/100\n",
      "100/100 - 29s - loss: -6.5205e-01 - val_loss: -6.1634e-01 - lr: 3.1250e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=long_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='pretraining_unfreeze_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "\n",
    "for i in range(3):\n",
    "    pretrained_model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/transferLearned_models/best_pretraining_unfreeze_test.h5')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = True\n",
    "    for layer in pretrained_model.layers[:depth[i]]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    pretrained_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(pretrained_model.optimizer.learning_rate,0.00001)\n",
    "    print(pretrained_model.summary())\n",
    "    \n",
    "    history = train_model(model=pretrained_model,\n",
    "        path2save=config[\"path2save_finetuned\"],\n",
    "        model_name=_.join(['unfreeze_test_depth',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_unfreeze[i,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/unfreeze_2.npy\",val_loss_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_unfreeze = np.load(main_dir+\"parameters_analysis/unfreeze_2.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((3,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_unfreeze[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_unfreeze[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_unfreeze[2,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'frozen parameters = 0')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'frozen parameters = 32 112')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'frozen parameters = 94 560')\n",
    "\n",
    "plt.title('Evolution of the validation loss for different unfreezing possibilities')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNFREEZING --> LEARNING RATE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_array = [0.0001,0.00001,0.000001]\n",
    "val_loss_learning = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "Epoch 1/100\n",
      "105/105 - 22s - loss: -3.0390e-01 - val_loss: -4.4810e-01 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "105/105 - 22s - loss: -3.9835e-01 - val_loss: -4.7125e-01 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "105/105 - 22s - loss: -4.2187e-01 - val_loss: -4.6105e-01 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "105/105 - 22s - loss: -4.5228e-01 - val_loss: -4.2294e-01 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "105/105 - 22s - loss: -4.7623e-01 - val_loss: -4.7431e-01 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "105/105 - 22s - loss: -4.7352e-01 - val_loss: -4.7239e-01 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "105/105 - 22s - loss: -4.8318e-01 - val_loss: -5.0367e-01 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "105/105 - 22s - loss: -5.0124e-01 - val_loss: -4.5863e-01 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "105/105 - 22s - loss: -5.0782e-01 - val_loss: -5.3182e-01 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "105/105 - 22s - loss: -5.1959e-01 - val_loss: -5.1849e-01 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "105/105 - 22s - loss: -5.2668e-01 - val_loss: -5.3202e-01 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "105/105 - 22s - loss: -5.3098e-01 - val_loss: -5.9435e-01 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "105/105 - 22s - loss: -5.2330e-01 - val_loss: -4.7110e-01 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "105/105 - 22s - loss: -5.3724e-01 - val_loss: -5.8399e-01 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "105/105 - 22s - loss: -5.3275e-01 - val_loss: -5.7198e-01 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "105/105 - 22s - loss: -5.4747e-01 - val_loss: -5.6597e-01 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "105/105 - 22s - loss: -5.5768e-01 - val_loss: -3.7328e-01 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "105/105 - 22s - loss: -5.5124e-01 - val_loss: -5.8129e-01 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "105/105 - 22s - loss: -5.5531e-01 - val_loss: -5.6734e-01 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "105/105 - 22s - loss: -5.5451e-01 - val_loss: -5.3692e-01 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "105/105 - 22s - loss: -5.4846e-01 - val_loss: -5.9278e-01 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "105/105 - 22s - loss: -5.5688e-01 - val_loss: -5.9398e-01 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "105/105 - 22s - loss: -5.7453e-01 - val_loss: -5.6596e-01 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "105/105 - 22s - loss: -5.7936e-01 - val_loss: -5.5288e-01 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "105/105 - 22s - loss: -5.8163e-01 - val_loss: -5.8631e-01 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "105/105 - 22s - loss: -5.7702e-01 - val_loss: -5.5234e-01 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "105/105 - 22s - loss: -5.7821e-01 - val_loss: -5.5007e-01 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "105/105 - 22s - loss: -5.8437e-01 - val_loss: -5.3215e-01 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "105/105 - 22s - loss: -5.8295e-01 - val_loss: -6.8161e-01 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "105/105 - 22s - loss: -5.8813e-01 - val_loss: -5.8456e-01 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "105/105 - 22s - loss: -5.8546e-01 - val_loss: -5.8155e-01 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "105/105 - 22s - loss: -5.8433e-01 - val_loss: -6.4181e-01 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "105/105 - 22s - loss: -5.8976e-01 - val_loss: -5.9201e-01 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "105/105 - 22s - loss: -5.8591e-01 - val_loss: -4.9609e-01 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "105/105 - 22s - loss: -5.8772e-01 - val_loss: -5.8638e-01 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "105/105 - 22s - loss: -5.9345e-01 - val_loss: -6.3075e-01 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "105/105 - 22s - loss: -5.9457e-01 - val_loss: -5.6088e-01 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "105/105 - 22s - loss: -5.9781e-01 - val_loss: -5.7505e-01 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "105/105 - 22s - loss: -5.9296e-01 - val_loss: -6.1017e-01 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "105/105 - 22s - loss: -5.9953e-01 - val_loss: -6.4268e-01 - lr: 2.5000e-04\n",
      "Epoch 41/100\n",
      "105/105 - 22s - loss: -6.0298e-01 - val_loss: -5.5374e-01 - lr: 2.5000e-04\n",
      "Epoch 42/100\n",
      "105/105 - 22s - loss: -6.0669e-01 - val_loss: -6.2898e-01 - lr: 2.5000e-04\n",
      "Epoch 43/100\n",
      "105/105 - 22s - loss: -6.0808e-01 - val_loss: -6.6969e-01 - lr: 2.5000e-04\n",
      "Epoch 44/100\n",
      "105/105 - 22s - loss: -6.1161e-01 - val_loss: -6.0022e-01 - lr: 2.5000e-04\n",
      "Epoch 45/100\n",
      "105/105 - 22s - loss: -6.1039e-01 - val_loss: -5.8306e-01 - lr: 2.5000e-04\n",
      "Epoch 46/100\n",
      "105/105 - 22s - loss: -6.1157e-01 - val_loss: -6.2085e-01 - lr: 2.5000e-04\n",
      "Epoch 47/100\n",
      "105/105 - 22s - loss: -6.1061e-01 - val_loss: -6.4087e-01 - lr: 2.5000e-04\n",
      "Epoch 48/100\n",
      "105/105 - 22s - loss: -6.1737e-01 - val_loss: -5.9645e-01 - lr: 2.5000e-04\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "105/105 - 22s - loss: -6.0963e-01 - val_loss: -6.0130e-01 - lr: 2.5000e-04\n",
      "Epoch 50/100\n",
      "105/105 - 22s - loss: -6.1399e-01 - val_loss: -6.6854e-01 - lr: 1.2500e-04\n",
      "Epoch 51/100\n",
      "105/105 - 22s - loss: -6.1822e-01 - val_loss: -5.6732e-01 - lr: 1.2500e-04\n",
      "Epoch 52/100\n",
      "105/105 - 22s - loss: -6.1851e-01 - val_loss: -6.1924e-01 - lr: 1.2500e-04\n",
      "Epoch 53/100\n",
      "105/105 - 22s - loss: -6.2137e-01 - val_loss: -6.8412e-01 - lr: 1.2500e-04\n",
      "Epoch 54/100\n",
      "105/105 - 22s - loss: -6.2205e-01 - val_loss: -5.8204e-01 - lr: 1.2500e-04\n",
      "Epoch 55/100\n",
      "105/105 - 22s - loss: -6.2158e-01 - val_loss: -6.2312e-01 - lr: 1.2500e-04\n",
      "Epoch 56/100\n",
      "105/105 - 22s - loss: -6.1847e-01 - val_loss: -6.5011e-01 - lr: 1.2500e-04\n",
      "Epoch 57/100\n",
      "105/105 - 22s - loss: -6.1690e-01 - val_loss: -6.1460e-01 - lr: 1.2500e-04\n",
      "Epoch 58/100\n",
      "105/105 - 22s - loss: -6.2388e-01 - val_loss: -6.2201e-01 - lr: 1.2500e-04\n",
      "Epoch 59/100\n",
      "105/105 - 22s - loss: -6.2203e-01 - val_loss: -6.2472e-01 - lr: 1.2500e-04\n",
      "Epoch 60/100\n",
      "105/105 - 22s - loss: -6.2156e-01 - val_loss: -5.8923e-01 - lr: 1.2500e-04\n",
      "Epoch 61/100\n",
      "105/105 - 22s - loss: -6.2059e-01 - val_loss: -6.2861e-01 - lr: 1.2500e-04\n",
      "Epoch 62/100\n",
      "105/105 - 22s - loss: -6.2282e-01 - val_loss: -6.6954e-01 - lr: 1.2500e-04\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "105/105 - 22s - loss: -6.2656e-01 - val_loss: -5.6559e-01 - lr: 1.2500e-04\n",
      "Epoch 64/100\n",
      "105/105 - 22s - loss: -6.3035e-01 - val_loss: -6.2937e-01 - lr: 6.2500e-05\n",
      "Epoch 65/100\n",
      "105/105 - 22s - loss: -6.2357e-01 - val_loss: -6.6061e-01 - lr: 6.2500e-05\n",
      "Epoch 66/100\n",
      "105/105 - 22s - loss: -6.2559e-01 - val_loss: -6.7278e-01 - lr: 6.2500e-05\n",
      "Epoch 67/100\n",
      "105/105 - 22s - loss: -6.2714e-01 - val_loss: -6.1685e-01 - lr: 6.2500e-05\n",
      "Epoch 68/100\n",
      "105/105 - 22s - loss: -6.2616e-01 - val_loss: -6.2876e-01 - lr: 6.2500e-05\n",
      "Epoch 69/100\n",
      "105/105 - 22s - loss: -6.2744e-01 - val_loss: -6.7566e-01 - lr: 6.2500e-05\n",
      "Epoch 70/100\n",
      "105/105 - 22s - loss: -6.3049e-01 - val_loss: -6.1369e-01 - lr: 6.2500e-05\n",
      "Epoch 71/100\n",
      "105/105 - 22s - loss: -6.2813e-01 - val_loss: -6.9137e-01 - lr: 6.2500e-05\n",
      "Epoch 72/100\n",
      "105/105 - 22s - loss: -6.2929e-01 - val_loss: -5.6180e-01 - lr: 6.2500e-05\n",
      "Epoch 73/100\n",
      "105/105 - 22s - loss: -6.2536e-01 - val_loss: -6.6472e-01 - lr: 6.2500e-05\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 - 22s - loss: -6.2735e-01 - val_loss: -6.4227e-01 - lr: 6.2500e-05\n",
      "Epoch 75/100\n",
      "105/105 - 22s - loss: -6.2971e-01 - val_loss: -6.4476e-01 - lr: 6.2500e-05\n",
      "Epoch 76/100\n",
      "105/105 - 22s - loss: -6.3160e-01 - val_loss: -6.3764e-01 - lr: 6.2500e-05\n",
      "Epoch 77/100\n",
      "105/105 - 22s - loss: -6.2426e-01 - val_loss: -6.3978e-01 - lr: 6.2500e-05\n",
      "Epoch 78/100\n",
      "105/105 - 22s - loss: -6.2779e-01 - val_loss: -6.3376e-01 - lr: 6.2500e-05\n",
      "Epoch 79/100\n",
      "105/105 - 22s - loss: -6.3360e-01 - val_loss: -6.3588e-01 - lr: 6.2500e-05\n",
      "Epoch 80/100\n",
      "105/105 - 22s - loss: -6.2938e-01 - val_loss: -6.3565e-01 - lr: 6.2500e-05\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "105/105 - 22s - loss: -6.2797e-01 - val_loss: -6.5004e-01 - lr: 6.2500e-05\n",
      "Epoch 82/100\n",
      "105/105 - 22s - loss: -6.3058e-01 - val_loss: -6.2713e-01 - lr: 3.1250e-05\n",
      "Epoch 83/100\n",
      "105/105 - 22s - loss: -6.3389e-01 - val_loss: -6.4322e-01 - lr: 3.1250e-05\n",
      "Epoch 84/100\n",
      "105/105 - 22s - loss: -6.3337e-01 - val_loss: -6.9695e-01 - lr: 3.1250e-05\n",
      "Epoch 85/100\n",
      "105/105 - 22s - loss: -6.3734e-01 - val_loss: -6.4322e-01 - lr: 3.1250e-05\n",
      "Epoch 86/100\n",
      "105/105 - 22s - loss: -6.3623e-01 - val_loss: -6.4330e-01 - lr: 3.1250e-05\n",
      "Epoch 87/100\n",
      "105/105 - 22s - loss: -6.3679e-01 - val_loss: -5.9481e-01 - lr: 3.1250e-05\n",
      "Epoch 88/100\n",
      "105/105 - 22s - loss: -6.3283e-01 - val_loss: -6.5191e-01 - lr: 3.1250e-05\n",
      "Epoch 89/100\n",
      "105/105 - 22s - loss: -6.3497e-01 - val_loss: -6.6033e-01 - lr: 3.1250e-05\n",
      "Epoch 90/100\n",
      "105/105 - 22s - loss: -6.3282e-01 - val_loss: -6.4367e-01 - lr: 3.1250e-05\n",
      "Epoch 91/100\n",
      "105/105 - 22s - loss: -6.3591e-01 - val_loss: -6.3589e-01 - lr: 3.1250e-05\n",
      "Epoch 92/100\n",
      "105/105 - 22s - loss: -6.3502e-01 - val_loss: -6.5520e-01 - lr: 3.1250e-05\n",
      "Epoch 93/100\n",
      "105/105 - 22s - loss: -6.3567e-01 - val_loss: -6.2606e-01 - lr: 3.1250e-05\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "105/105 - 22s - loss: -6.3837e-01 - val_loss: -6.2006e-01 - lr: 3.1250e-05\n",
      "Epoch 95/100\n",
      "105/105 - 22s - loss: -6.3708e-01 - val_loss: -6.5697e-01 - lr: 1.5625e-05\n",
      "Epoch 96/100\n",
      "105/105 - 22s - loss: -6.3533e-01 - val_loss: -6.3674e-01 - lr: 1.5625e-05\n",
      "Epoch 97/100\n",
      "105/105 - 22s - loss: -6.3261e-01 - val_loss: -6.3048e-01 - lr: 1.5625e-05\n",
      "Epoch 98/100\n",
      "105/105 - 22s - loss: -6.3422e-01 - val_loss: -6.5580e-01 - lr: 1.5625e-05\n",
      "Epoch 99/100\n",
      "105/105 - 22s - loss: -6.3634e-01 - val_loss: -6.6424e-01 - lr: 1.5625e-05\n",
      "Epoch 100/100\n",
      "105/105 - 22s - loss: -6.3253e-01 - val_loss: -6.2133e-01 - lr: 1.5625e-05\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Epoch 1/100\n",
      "105/105 - 39s - loss: -5.7412e-01 - val_loss: -3.6659e-01 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "105/105 - 38s - loss: -6.2205e-01 - val_loss: -6.4486e-01 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "105/105 - 38s - loss: -6.3600e-01 - val_loss: -5.6779e-01 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "105/105 - 38s - loss: -6.4852e-01 - val_loss: -6.0137e-01 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "105/105 - 38s - loss: -6.6214e-01 - val_loss: -6.5010e-01 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "105/105 - 38s - loss: -6.6808e-01 - val_loss: -6.0877e-01 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "105/105 - 38s - loss: -6.6807e-01 - val_loss: -4.1980e-01 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "105/105 - 38s - loss: -6.7772e-01 - val_loss: -6.8567e-01 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "105/105 - 38s - loss: -6.8044e-01 - val_loss: -3.6199e-01 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "105/105 - 38s - loss: -6.8641e-01 - val_loss: -6.8276e-01 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "105/105 - 38s - loss: -6.8606e-01 - val_loss: -5.9406e-01 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "105/105 - 38s - loss: -6.9899e-01 - val_loss: -6.5920e-01 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "105/105 - 38s - loss: -6.9606e-01 - val_loss: -5.7590e-01 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "105/105 - 38s - loss: -6.9909e-01 - val_loss: -6.3761e-01 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "105/105 - 38s - loss: -6.9987e-01 - val_loss: -3.2922e-01 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "105/105 - 38s - loss: -7.0396e-01 - val_loss: -6.7733e-01 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "105/105 - 38s - loss: -7.0793e-01 - val_loss: -4.9669e-01 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "105/105 - 38s - loss: -7.1051e-01 - val_loss: -7.2098e-01 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "105/105 - 38s - loss: -7.0978e-01 - val_loss: -8.0848e-02 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "105/105 - 38s - loss: -7.0904e-01 - val_loss: -4.8332e-01 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "105/105 - 38s - loss: -7.1181e-01 - val_loss: -6.3229e-01 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "105/105 - 38s - loss: -7.1596e-01 - val_loss: -6.7189e-01 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "105/105 - 38s - loss: -7.1842e-01 - val_loss: -6.8084e-01 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "105/105 - 38s - loss: -7.1860e-01 - val_loss: -5.7233e-01 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "105/105 - 38s - loss: -7.1649e-01 - val_loss: -6.8049e-01 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "105/105 - 38s - loss: -7.2470e-01 - val_loss: -6.6695e-02 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "105/105 - 38s - loss: -7.2304e-01 - val_loss: -6.9394e-01 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "105/105 - 38s - loss: -7.2160e-01 - val_loss: -5.4103e-01 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "105/105 - 38s - loss: -7.3408e-01 - val_loss: -4.6658e-01 - lr: 5.0000e-05\n",
      "Epoch 30/100\n",
      "105/105 - 38s - loss: -7.3198e-01 - val_loss: -6.9773e-01 - lr: 5.0000e-05\n",
      "Epoch 31/100\n",
      "105/105 - 38s - loss: -7.3608e-01 - val_loss: -5.6465e-01 - lr: 5.0000e-05\n",
      "Epoch 32/100\n",
      "105/105 - 38s - loss: -7.3463e-01 - val_loss: -6.5558e-01 - lr: 5.0000e-05\n",
      "Epoch 33/100\n",
      "105/105 - 38s - loss: -7.3947e-01 - val_loss: -7.2732e-01 - lr: 5.0000e-05\n",
      "Epoch 34/100\n",
      "105/105 - 38s - loss: -7.4166e-01 - val_loss: -6.5111e-01 - lr: 5.0000e-05\n",
      "Epoch 35/100\n",
      "105/105 - 38s - loss: -7.4036e-01 - val_loss: -6.0270e-01 - lr: 5.0000e-05\n",
      "Epoch 36/100\n",
      "105/105 - 38s - loss: -7.4197e-01 - val_loss: -6.2694e-01 - lr: 5.0000e-05\n",
      "Epoch 37/100\n",
      "105/105 - 38s - loss: -7.4369e-01 - val_loss: -7.4346e-01 - lr: 5.0000e-05\n",
      "Epoch 38/100\n",
      "105/105 - 38s - loss: -7.4443e-01 - val_loss: -6.1514e-01 - lr: 5.0000e-05\n",
      "Epoch 39/100\n",
      "105/105 - 38s - loss: -7.4616e-01 - val_loss: -6.9705e-01 - lr: 5.0000e-05\n",
      "Epoch 40/100\n",
      "105/105 - 38s - loss: -7.4247e-01 - val_loss: -6.6351e-01 - lr: 5.0000e-05\n",
      "Epoch 41/100\n",
      "105/105 - 38s - loss: -7.5150e-01 - val_loss: -7.0774e-01 - lr: 5.0000e-05\n",
      "Epoch 42/100\n",
      "105/105 - 38s - loss: -7.4632e-01 - val_loss: -6.0653e-01 - lr: 5.0000e-05\n",
      "Epoch 43/100\n",
      "105/105 - 38s - loss: -7.4694e-01 - val_loss: -5.7272e-01 - lr: 5.0000e-05\n",
      "Epoch 44/100\n",
      "105/105 - 38s - loss: -7.4905e-01 - val_loss: -6.2698e-01 - lr: 5.0000e-05\n",
      "Epoch 45/100\n",
      "105/105 - 38s - loss: -7.5014e-01 - val_loss: -4.7857e-01 - lr: 5.0000e-05\n",
      "Epoch 46/100\n",
      "105/105 - 38s - loss: -7.4795e-01 - val_loss: -7.1595e-01 - lr: 5.0000e-05\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "105/105 - 38s - loss: -7.5011e-01 - val_loss: -6.6416e-01 - lr: 5.0000e-05\n",
      "Epoch 48/100\n",
      "105/105 - 38s - loss: -7.5570e-01 - val_loss: -6.9702e-01 - lr: 2.5000e-05\n",
      "Epoch 49/100\n",
      "105/105 - 38s - loss: -7.5396e-01 - val_loss: -7.0482e-01 - lr: 2.5000e-05\n",
      "Epoch 50/100\n",
      "105/105 - 38s - loss: -7.5917e-01 - val_loss: -7.0395e-01 - lr: 2.5000e-05\n",
      "Epoch 51/100\n",
      "105/105 - 38s - loss: -7.5765e-01 - val_loss: -6.8308e-01 - lr: 2.5000e-05\n",
      "Epoch 52/100\n",
      "105/105 - 38s - loss: -7.5957e-01 - val_loss: -7.0597e-01 - lr: 2.5000e-05\n",
      "Epoch 53/100\n",
      "105/105 - 38s - loss: -7.6102e-01 - val_loss: -6.4496e-01 - lr: 2.5000e-05\n",
      "Epoch 54/100\n",
      "105/105 - 38s - loss: -7.5880e-01 - val_loss: -6.3838e-01 - lr: 2.5000e-05\n",
      "Epoch 55/100\n",
      "105/105 - 38s - loss: -7.5849e-01 - val_loss: -6.8848e-01 - lr: 2.5000e-05\n",
      "Epoch 56/100\n",
      "105/105 - 38s - loss: -7.5791e-01 - val_loss: -6.6136e-01 - lr: 2.5000e-05\n",
      "Epoch 57/100\n",
      "105/105 - 38s - loss: -7.5844e-01 - val_loss: -7.5086e-01 - lr: 2.5000e-05\n",
      "Epoch 58/100\n",
      "105/105 - 38s - loss: -7.5922e-01 - val_loss: -6.2498e-01 - lr: 2.5000e-05\n",
      "Epoch 59/100\n",
      "105/105 - 38s - loss: -7.6121e-01 - val_loss: -6.5622e-01 - lr: 2.5000e-05\n",
      "Epoch 60/100\n",
      "105/105 - 38s - loss: -7.6140e-01 - val_loss: -6.6420e-01 - lr: 2.5000e-05\n",
      "Epoch 61/100\n",
      "105/105 - 38s - loss: -7.6358e-01 - val_loss: -7.1404e-01 - lr: 2.5000e-05\n",
      "Epoch 62/100\n",
      "105/105 - 38s - loss: -7.6460e-01 - val_loss: -6.2849e-01 - lr: 2.5000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100\n",
      "105/105 - 38s - loss: -7.6371e-01 - val_loss: -6.2169e-01 - lr: 2.5000e-05\n",
      "Epoch 64/100\n",
      "105/105 - 38s - loss: -7.6801e-01 - val_loss: -6.6045e-01 - lr: 2.5000e-05\n",
      "Epoch 65/100\n",
      "105/105 - 38s - loss: -7.6352e-01 - val_loss: -6.8719e-01 - lr: 2.5000e-05\n",
      "Epoch 66/100\n",
      "105/105 - 38s - loss: -7.6564e-01 - val_loss: -7.4761e-01 - lr: 2.5000e-05\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "105/105 - 38s - loss: -7.6777e-01 - val_loss: -6.5335e-01 - lr: 2.5000e-05\n",
      "Epoch 68/100\n",
      "105/105 - 38s - loss: -7.6519e-01 - val_loss: -7.1406e-01 - lr: 1.2500e-05\n",
      "Epoch 69/100\n",
      "105/105 - 38s - loss: -7.6704e-01 - val_loss: -7.0483e-01 - lr: 1.2500e-05\n",
      "Epoch 70/100\n",
      "105/105 - 38s - loss: -7.6931e-01 - val_loss: -6.6099e-01 - lr: 1.2500e-05\n",
      "Epoch 71/100\n",
      "105/105 - 38s - loss: -7.6774e-01 - val_loss: -6.3153e-01 - lr: 1.2500e-05\n",
      "Epoch 72/100\n",
      "105/105 - 38s - loss: -7.6851e-01 - val_loss: -7.0397e-01 - lr: 1.2500e-05\n",
      "Epoch 73/100\n",
      "105/105 - 38s - loss: -7.6712e-01 - val_loss: -7.1906e-01 - lr: 1.2500e-05\n",
      "Epoch 74/100\n",
      "105/105 - 38s - loss: -7.6928e-01 - val_loss: -6.7719e-01 - lr: 1.2500e-05\n",
      "Epoch 75/100\n",
      "105/105 - 38s - loss: -7.7262e-01 - val_loss: -7.0295e-01 - lr: 1.2500e-05\n",
      "Epoch 76/100\n",
      "105/105 - 38s - loss: -7.6593e-01 - val_loss: -7.0036e-01 - lr: 1.2500e-05\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "105/105 - 38s - loss: -7.7186e-01 - val_loss: -7.0863e-01 - lr: 1.2500e-05\n",
      "Epoch 78/100\n",
      "105/105 - 38s - loss: -7.7453e-01 - val_loss: -7.2424e-01 - lr: 6.2500e-06\n",
      "Epoch 79/100\n",
      "105/105 - 38s - loss: -7.7502e-01 - val_loss: -6.7016e-01 - lr: 6.2500e-06\n",
      "Epoch 80/100\n",
      "105/105 - 38s - loss: -7.7214e-01 - val_loss: -7.1152e-01 - lr: 6.2500e-06\n",
      "Epoch 81/100\n",
      "105/105 - 38s - loss: -7.7079e-01 - val_loss: -7.1195e-01 - lr: 6.2500e-06\n",
      "Epoch 82/100\n",
      "105/105 - 38s - loss: -7.7078e-01 - val_loss: -7.2791e-01 - lr: 6.2500e-06\n",
      "Epoch 83/100\n",
      "105/105 - 38s - loss: -7.7068e-01 - val_loss: -6.4340e-01 - lr: 6.2500e-06\n",
      "Epoch 84/100\n",
      "105/105 - 38s - loss: -7.7322e-01 - val_loss: -7.0182e-01 - lr: 6.2500e-06\n",
      "Epoch 85/100\n",
      "105/105 - 38s - loss: -7.7159e-01 - val_loss: -7.2244e-01 - lr: 6.2500e-06\n",
      "Epoch 86/100\n",
      "105/105 - 38s - loss: -7.7080e-01 - val_loss: -6.7655e-01 - lr: 6.2500e-06\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
      "105/105 - 38s - loss: -7.7293e-01 - val_loss: -7.3233e-01 - lr: 6.2500e-06\n",
      "Epoch 88/100\n",
      "105/105 - 38s - loss: -7.6936e-01 - val_loss: -6.5785e-01 - lr: 3.1250e-06\n",
      "Epoch 89/100\n",
      "105/105 - 38s - loss: -7.7104e-01 - val_loss: -7.1363e-01 - lr: 3.1250e-06\n",
      "Epoch 90/100\n",
      "105/105 - 38s - loss: -7.7303e-01 - val_loss: -7.0058e-01 - lr: 3.1250e-06\n",
      "Epoch 91/100\n",
      "105/105 - 38s - loss: -7.7208e-01 - val_loss: -6.9890e-01 - lr: 3.1250e-06\n",
      "Epoch 92/100\n",
      "105/105 - 38s - loss: -7.7561e-01 - val_loss: -7.2363e-01 - lr: 3.1250e-06\n",
      "Epoch 93/100\n",
      "105/105 - 38s - loss: -7.7372e-01 - val_loss: -6.8259e-01 - lr: 3.1250e-06\n",
      "Epoch 94/100\n",
      "105/105 - 38s - loss: -7.7509e-01 - val_loss: -7.1510e-01 - lr: 3.1250e-06\n",
      "Epoch 95/100\n",
      "105/105 - 38s - loss: -7.7226e-01 - val_loss: -6.6884e-01 - lr: 3.1250e-06\n",
      "Epoch 96/100\n",
      "105/105 - 38s - loss: -7.7367e-01 - val_loss: -7.0797e-01 - lr: 3.1250e-06\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
      "105/105 - 38s - loss: -7.7542e-01 - val_loss: -7.3407e-01 - lr: 3.1250e-06\n",
      "Epoch 98/100\n",
      "105/105 - 38s - loss: -7.7365e-01 - val_loss: -7.5846e-01 - lr: 1.5625e-06\n",
      "Epoch 99/100\n",
      "105/105 - 38s - loss: -7.7310e-01 - val_loss: -6.8315e-01 - lr: 1.5625e-06\n",
      "Epoch 100/100\n",
      "105/105 - 38s - loss: -7.7375e-01 - val_loss: -6.6593e-01 - lr: 1.5625e-06\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Epoch 1/100\n",
      "105/105 - 39s - loss: -4.2542e-01 - val_loss: -3.8984e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "105/105 - 38s - loss: -5.5990e-01 - val_loss: -5.5236e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "105/105 - 38s - loss: -5.8464e-01 - val_loss: -6.2621e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "105/105 - 38s - loss: -5.9680e-01 - val_loss: -6.3262e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "105/105 - 38s - loss: -6.1226e-01 - val_loss: -6.1578e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "105/105 - 38s - loss: -6.1158e-01 - val_loss: -5.5474e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "105/105 - 38s - loss: -6.1763e-01 - val_loss: -6.4054e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "105/105 - 38s - loss: -6.2339e-01 - val_loss: -6.4470e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "105/105 - 38s - loss: -6.2206e-01 - val_loss: -6.6823e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "105/105 - 38s - loss: -6.2659e-01 - val_loss: -6.0519e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "105/105 - 38s - loss: -6.3303e-01 - val_loss: -6.7786e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "105/105 - 38s - loss: -6.3476e-01 - val_loss: -6.1969e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "105/105 - 38s - loss: -6.3443e-01 - val_loss: -6.0725e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "105/105 - 38s - loss: -6.4014e-01 - val_loss: -7.0194e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "105/105 - 38s - loss: -6.4463e-01 - val_loss: -6.8817e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "105/105 - 38s - loss: -6.4235e-01 - val_loss: -6.5870e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "105/105 - 38s - loss: -6.4583e-01 - val_loss: -6.9176e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "105/105 - 38s - loss: -6.4043e-01 - val_loss: -6.6570e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "105/105 - 38s - loss: -6.5199e-01 - val_loss: -6.6130e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "105/105 - 38s - loss: -6.4913e-01 - val_loss: -6.5680e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "105/105 - 38s - loss: -6.5325e-01 - val_loss: -6.3102e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "105/105 - 38s - loss: -6.5314e-01 - val_loss: -6.8370e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "105/105 - 38s - loss: -6.5289e-01 - val_loss: -6.4363e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "105/105 - 38s - loss: -6.5794e-01 - val_loss: -6.8357e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "105/105 - 38s - loss: -6.5742e-01 - val_loss: -6.7961e-01 - lr: 5.0000e-06\n",
      "Epoch 26/100\n",
      "105/105 - 38s - loss: -6.6066e-01 - val_loss: -6.7842e-01 - lr: 5.0000e-06\n",
      "Epoch 27/100\n",
      "105/105 - 39s - loss: -6.5797e-01 - val_loss: -6.5666e-01 - lr: 5.0000e-06\n",
      "Epoch 28/100\n",
      "105/105 - 38s - loss: -6.6463e-01 - val_loss: -6.8535e-01 - lr: 5.0000e-06\n",
      "Epoch 29/100\n",
      "105/105 - 38s - loss: -6.5929e-01 - val_loss: -6.6868e-01 - lr: 5.0000e-06\n",
      "Epoch 30/100\n",
      "105/105 - 38s - loss: -6.6781e-01 - val_loss: -6.7143e-01 - lr: 5.0000e-06\n",
      "Epoch 31/100\n",
      "105/105 - 38s - loss: -6.5700e-01 - val_loss: -6.5122e-01 - lr: 5.0000e-06\n",
      "Epoch 32/100\n",
      "105/105 - 38s - loss: -6.6777e-01 - val_loss: -6.7431e-01 - lr: 5.0000e-06\n",
      "Epoch 33/100\n",
      "105/105 - 38s - loss: -6.6741e-01 - val_loss: -6.8768e-01 - lr: 5.0000e-06\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "105/105 - 38s - loss: -6.6720e-01 - val_loss: -6.6019e-01 - lr: 5.0000e-06\n",
      "Epoch 35/100\n",
      "105/105 - 38s - loss: -6.6548e-01 - val_loss: -6.8355e-01 - lr: 2.5000e-06\n",
      "Epoch 36/100\n",
      "105/105 - 38s - loss: -6.6889e-01 - val_loss: -6.7143e-01 - lr: 2.5000e-06\n",
      "Epoch 37/100\n",
      "105/105 - 38s - loss: -6.6183e-01 - val_loss: -7.0563e-01 - lr: 2.5000e-06\n",
      "Epoch 38/100\n",
      "105/105 - 38s - loss: -6.7062e-01 - val_loss: -6.2330e-01 - lr: 2.5000e-06\n",
      "Epoch 39/100\n",
      "105/105 - 38s - loss: -6.6408e-01 - val_loss: -6.9264e-01 - lr: 2.5000e-06\n",
      "Epoch 40/100\n",
      "105/105 - 38s - loss: -6.6795e-01 - val_loss: -6.9760e-01 - lr: 2.5000e-06\n",
      "Epoch 41/100\n",
      "105/105 - 38s - loss: -6.7267e-01 - val_loss: -7.0258e-01 - lr: 2.5000e-06\n",
      "Epoch 42/100\n",
      "105/105 - 38s - loss: -6.6913e-01 - val_loss: -6.9983e-01 - lr: 2.5000e-06\n",
      "Epoch 43/100\n",
      "105/105 - 38s - loss: -6.6497e-01 - val_loss: -6.1338e-01 - lr: 2.5000e-06\n",
      "Epoch 44/100\n",
      "105/105 - 38s - loss: -6.7239e-01 - val_loss: -7.2485e-01 - lr: 2.5000e-06\n",
      "Epoch 45/100\n",
      "105/105 - 38s - loss: -6.6877e-01 - val_loss: -6.8507e-01 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "105/105 - 38s - loss: -6.6732e-01 - val_loss: -6.6706e-01 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "105/105 - 38s - loss: -6.6860e-01 - val_loss: -7.1080e-01 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "105/105 - 38s - loss: -6.7443e-01 - val_loss: -6.9606e-01 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "105/105 - 38s - loss: -6.6917e-01 - val_loss: -6.7989e-01 - lr: 2.5000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "105/105 - 38s - loss: -6.7006e-01 - val_loss: -7.0572e-01 - lr: 2.5000e-06\n",
      "Epoch 51/100\n",
      "105/105 - 38s - loss: -6.7199e-01 - val_loss: -6.7620e-01 - lr: 2.5000e-06\n",
      "Epoch 52/100\n",
      "105/105 - 38s - loss: -6.6936e-01 - val_loss: -6.7058e-01 - lr: 2.5000e-06\n",
      "Epoch 53/100\n",
      "105/105 - 38s - loss: -6.7071e-01 - val_loss: -6.9084e-01 - lr: 2.5000e-06\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "105/105 - 38s - loss: -6.7187e-01 - val_loss: -7.2083e-01 - lr: 2.5000e-06\n",
      "Epoch 55/100\n",
      "105/105 - 38s - loss: -6.7204e-01 - val_loss: -6.8301e-01 - lr: 1.2500e-06\n",
      "Epoch 56/100\n",
      "105/105 - 38s - loss: -6.7034e-01 - val_loss: -7.0281e-01 - lr: 1.2500e-06\n",
      "Epoch 57/100\n",
      "105/105 - 38s - loss: -6.7510e-01 - val_loss: -6.2794e-01 - lr: 1.2500e-06\n",
      "Epoch 58/100\n",
      "105/105 - 38s - loss: -6.6985e-01 - val_loss: -6.6843e-01 - lr: 1.2500e-06\n",
      "Epoch 59/100\n",
      "105/105 - 38s - loss: -6.7511e-01 - val_loss: -7.5054e-01 - lr: 1.2500e-06\n",
      "Epoch 60/100\n",
      "105/105 - 38s - loss: -6.7178e-01 - val_loss: -6.3241e-01 - lr: 1.2500e-06\n",
      "Epoch 61/100\n",
      "105/105 - 38s - loss: -6.7575e-01 - val_loss: -7.3403e-01 - lr: 1.2500e-06\n",
      "Epoch 62/100\n",
      "105/105 - 38s - loss: -6.7033e-01 - val_loss: -6.7276e-01 - lr: 1.2500e-06\n",
      "Epoch 63/100\n",
      "105/105 - 38s - loss: -6.7440e-01 - val_loss: -6.2943e-01 - lr: 1.2500e-06\n",
      "Epoch 64/100\n",
      "105/105 - 38s - loss: -6.7049e-01 - val_loss: -7.0878e-01 - lr: 1.2500e-06\n",
      "Epoch 65/100\n",
      "105/105 - 38s - loss: -6.7168e-01 - val_loss: -6.5504e-01 - lr: 1.2500e-06\n",
      "Epoch 66/100\n",
      "105/105 - 38s - loss: -6.7126e-01 - val_loss: -6.7299e-01 - lr: 1.2500e-06\n",
      "Epoch 67/100\n",
      "105/105 - 38s - loss: -6.7501e-01 - val_loss: -6.8391e-01 - lr: 1.2500e-06\n",
      "Epoch 68/100\n",
      "105/105 - 38s - loss: -6.7067e-01 - val_loss: -6.6215e-01 - lr: 1.2500e-06\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "105/105 - 38s - loss: -6.6927e-01 - val_loss: -6.7563e-01 - lr: 1.2500e-06\n",
      "Epoch 70/100\n",
      "105/105 - 38s - loss: -6.7258e-01 - val_loss: -6.9544e-01 - lr: 6.2500e-07\n",
      "Epoch 71/100\n",
      "105/105 - 38s - loss: -6.7175e-01 - val_loss: -6.7787e-01 - lr: 6.2500e-07\n",
      "Epoch 72/100\n",
      "105/105 - 38s - loss: -6.7179e-01 - val_loss: -6.7276e-01 - lr: 6.2500e-07\n",
      "Epoch 73/100\n",
      "105/105 - 38s - loss: -6.7372e-01 - val_loss: -6.9155e-01 - lr: 6.2500e-07\n",
      "Epoch 74/100\n",
      "105/105 - 38s - loss: -6.7179e-01 - val_loss: -6.5424e-01 - lr: 6.2500e-07\n",
      "Epoch 75/100\n",
      "105/105 - 38s - loss: -6.7555e-01 - val_loss: -6.7430e-01 - lr: 6.2500e-07\n",
      "Epoch 76/100\n",
      "105/105 - 38s - loss: -6.7008e-01 - val_loss: -6.8898e-01 - lr: 6.2500e-07\n",
      "Epoch 77/100\n",
      "105/105 - 38s - loss: -6.7574e-01 - val_loss: -6.6278e-01 - lr: 6.2500e-07\n",
      "Epoch 78/100\n",
      "105/105 - 38s - loss: -6.7579e-01 - val_loss: -6.8546e-01 - lr: 6.2500e-07\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "105/105 - 38s - loss: -6.6976e-01 - val_loss: -6.4796e-01 - lr: 6.2500e-07\n",
      "Epoch 80/100\n",
      "105/105 - 38s - loss: -6.7532e-01 - val_loss: -6.8991e-01 - lr: 3.1250e-07\n",
      "Epoch 81/100\n",
      "105/105 - 38s - loss: -6.7433e-01 - val_loss: -7.2614e-01 - lr: 3.1250e-07\n",
      "Epoch 82/100\n",
      "105/105 - 38s - loss: -6.6866e-01 - val_loss: -6.6744e-01 - lr: 3.1250e-07\n",
      "Epoch 83/100\n",
      "105/105 - 38s - loss: -6.7185e-01 - val_loss: -6.7402e-01 - lr: 3.1250e-07\n",
      "Epoch 84/100\n",
      "105/105 - 38s - loss: -6.7380e-01 - val_loss: -7.0630e-01 - lr: 3.1250e-07\n",
      "Epoch 85/100\n",
      "105/105 - 38s - loss: -6.7754e-01 - val_loss: -6.8499e-01 - lr: 3.1250e-07\n",
      "Epoch 86/100\n",
      "105/105 - 38s - loss: -6.7642e-01 - val_loss: -6.9128e-01 - lr: 3.1250e-07\n",
      "Epoch 87/100\n",
      "105/105 - 38s - loss: -6.7679e-01 - val_loss: -6.5787e-01 - lr: 3.1250e-07\n",
      "Epoch 88/100\n",
      "105/105 - 38s - loss: -6.7149e-01 - val_loss: -6.9861e-01 - lr: 3.1250e-07\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "105/105 - 38s - loss: -6.7160e-01 - val_loss: -6.9306e-01 - lr: 3.1250e-07\n",
      "Epoch 90/100\n",
      "105/105 - 38s - loss: -6.7555e-01 - val_loss: -6.7336e-01 - lr: 1.5625e-07\n",
      "Epoch 91/100\n",
      "105/105 - 38s - loss: -6.7529e-01 - val_loss: -6.6251e-01 - lr: 1.5625e-07\n",
      "Epoch 92/100\n",
      "105/105 - 38s - loss: -6.7758e-01 - val_loss: -6.8139e-01 - lr: 1.5625e-07\n",
      "Epoch 93/100\n",
      "105/105 - 38s - loss: -6.7158e-01 - val_loss: -6.4635e-01 - lr: 1.5625e-07\n",
      "Epoch 94/100\n",
      "105/105 - 38s - loss: -6.7681e-01 - val_loss: -7.1846e-01 - lr: 1.5625e-07\n",
      "Epoch 95/100\n",
      "105/105 - 38s - loss: -6.7317e-01 - val_loss: -6.8243e-01 - lr: 1.5625e-07\n",
      "Epoch 96/100\n",
      "105/105 - 38s - loss: -6.7395e-01 - val_loss: -6.8428e-01 - lr: 1.5625e-07\n",
      "Epoch 97/100\n",
      "105/105 - 38s - loss: -6.7697e-01 - val_loss: -6.6959e-01 - lr: 1.5625e-07\n",
      "Epoch 98/100\n",
      "105/105 - 38s - loss: -6.7385e-01 - val_loss: -7.0590e-01 - lr: 1.5625e-07\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "105/105 - 38s - loss: -6.7447e-01 - val_loss: -6.4581e-01 - lr: 1.5625e-07\n",
      "Epoch 100/100\n",
      "105/105 - 38s - loss: -6.7713e-01 - val_loss: -6.4765e-01 - lr: 7.8125e-08\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Epoch 1/100\n",
      "105/105 - 39s - loss: -2.9731e-01 - val_loss: -4.5265e-01 - lr: 1.0000e-06\n",
      "Epoch 2/100\n",
      "105/105 - 38s - loss: -3.3136e-01 - val_loss: -4.0105e-01 - lr: 1.0000e-06\n",
      "Epoch 3/100\n",
      "105/105 - 38s - loss: -3.6815e-01 - val_loss: -4.1493e-01 - lr: 1.0000e-06\n",
      "Epoch 4/100\n",
      "105/105 - 38s - loss: -3.9755e-01 - val_loss: -4.1136e-01 - lr: 1.0000e-06\n",
      "Epoch 5/100\n",
      "105/105 - 38s - loss: -4.3257e-01 - val_loss: -4.6197e-01 - lr: 1.0000e-06\n",
      "Epoch 6/100\n",
      "105/105 - 38s - loss: -4.6202e-01 - val_loss: -4.8160e-01 - lr: 1.0000e-06\n",
      "Epoch 7/100\n",
      "105/105 - 38s - loss: -4.8487e-01 - val_loss: -4.7995e-01 - lr: 1.0000e-06\n",
      "Epoch 8/100\n",
      "105/105 - 38s - loss: -4.9478e-01 - val_loss: -5.4375e-01 - lr: 1.0000e-06\n",
      "Epoch 9/100\n",
      "105/105 - 38s - loss: -5.1947e-01 - val_loss: -5.4240e-01 - lr: 1.0000e-06\n",
      "Epoch 10/100\n",
      "105/105 - 38s - loss: -5.2783e-01 - val_loss: -5.3793e-01 - lr: 1.0000e-06\n",
      "Epoch 11/100\n",
      "105/105 - 38s - loss: -5.3545e-01 - val_loss: -5.4121e-01 - lr: 1.0000e-06\n",
      "Epoch 12/100\n",
      "105/105 - 38s - loss: -5.4578e-01 - val_loss: -6.0376e-01 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "105/105 - 38s - loss: -5.5003e-01 - val_loss: -5.0692e-01 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "105/105 - 38s - loss: -5.6282e-01 - val_loss: -5.9250e-01 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "105/105 - 38s - loss: -5.6423e-01 - val_loss: -5.6635e-01 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "105/105 - 38s - loss: -5.6859e-01 - val_loss: -5.4004e-01 - lr: 1.0000e-06\n",
      "Epoch 17/100\n",
      "105/105 - 38s - loss: -5.7171e-01 - val_loss: -5.8947e-01 - lr: 1.0000e-06\n",
      "Epoch 18/100\n",
      "105/105 - 38s - loss: -5.7464e-01 - val_loss: -6.1059e-01 - lr: 1.0000e-06\n",
      "Epoch 19/100\n",
      "105/105 - 38s - loss: -5.7821e-01 - val_loss: -5.9598e-01 - lr: 1.0000e-06\n",
      "Epoch 20/100\n",
      "105/105 - 38s - loss: -5.7533e-01 - val_loss: -5.9797e-01 - lr: 1.0000e-06\n",
      "Epoch 21/100\n",
      "105/105 - 38s - loss: -5.8620e-01 - val_loss: -5.8694e-01 - lr: 1.0000e-06\n",
      "Epoch 22/100\n",
      "105/105 - 38s - loss: -5.8656e-01 - val_loss: -5.5740e-01 - lr: 1.0000e-06\n",
      "Epoch 23/100\n",
      "105/105 - 38s - loss: -5.8405e-01 - val_loss: -6.1114e-01 - lr: 1.0000e-06\n",
      "Epoch 24/100\n",
      "105/105 - 38s - loss: -5.8529e-01 - val_loss: -5.8954e-01 - lr: 1.0000e-06\n",
      "Epoch 25/100\n",
      "105/105 - 38s - loss: -5.8597e-01 - val_loss: -6.1934e-01 - lr: 1.0000e-06\n",
      "Epoch 26/100\n",
      "105/105 - 38s - loss: -5.9149e-01 - val_loss: -6.2246e-01 - lr: 1.0000e-06\n",
      "Epoch 27/100\n",
      "105/105 - 38s - loss: -5.9015e-01 - val_loss: -6.0504e-01 - lr: 1.0000e-06\n",
      "Epoch 28/100\n",
      "105/105 - 38s - loss: -5.9038e-01 - val_loss: -6.0208e-01 - lr: 1.0000e-06\n",
      "Epoch 29/100\n",
      "105/105 - 38s - loss: -5.9073e-01 - val_loss: -5.4893e-01 - lr: 1.0000e-06\n",
      "Epoch 30/100\n",
      "105/105 - 38s - loss: -6.0067e-01 - val_loss: -6.2778e-01 - lr: 1.0000e-06\n",
      "Epoch 31/100\n",
      "105/105 - 38s - loss: -5.9542e-01 - val_loss: -6.0872e-01 - lr: 1.0000e-06\n",
      "Epoch 32/100\n",
      "105/105 - 38s - loss: -5.9678e-01 - val_loss: -6.4813e-01 - lr: 1.0000e-06\n",
      "Epoch 33/100\n",
      "105/105 - 38s - loss: -6.0269e-01 - val_loss: -5.8522e-01 - lr: 1.0000e-06\n",
      "Epoch 34/100\n",
      "105/105 - 38s - loss: -5.9964e-01 - val_loss: -6.5183e-01 - lr: 1.0000e-06\n",
      "Epoch 35/100\n",
      "105/105 - 38s - loss: -6.0396e-01 - val_loss: -5.9100e-01 - lr: 1.0000e-06\n",
      "Epoch 36/100\n",
      "105/105 - 38s - loss: -6.0584e-01 - val_loss: -6.1751e-01 - lr: 1.0000e-06\n",
      "Epoch 37/100\n",
      "105/105 - 38s - loss: -6.0125e-01 - val_loss: -6.0942e-01 - lr: 1.0000e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "105/105 - 38s - loss: -6.0351e-01 - val_loss: -6.0669e-01 - lr: 1.0000e-06\n",
      "Epoch 39/100\n",
      "105/105 - 38s - loss: -6.0933e-01 - val_loss: -6.2129e-01 - lr: 1.0000e-06\n",
      "Epoch 40/100\n",
      "105/105 - 38s - loss: -6.0303e-01 - val_loss: -6.0300e-01 - lr: 1.0000e-06\n",
      "Epoch 41/100\n",
      "105/105 - 38s - loss: -6.0728e-01 - val_loss: -6.2804e-01 - lr: 1.0000e-06\n",
      "Epoch 42/100\n",
      "105/105 - 38s - loss: -6.0284e-01 - val_loss: -6.3057e-01 - lr: 1.0000e-06\n",
      "Epoch 43/100\n",
      "105/105 - 38s - loss: -6.1191e-01 - val_loss: -6.0893e-01 - lr: 1.0000e-06\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.999999987376214e-07.\n",
      "105/105 - 38s - loss: -6.0915e-01 - val_loss: -6.3801e-01 - lr: 1.0000e-06\n",
      "Epoch 45/100\n",
      "105/105 - 38s - loss: -6.1072e-01 - val_loss: -6.5190e-01 - lr: 5.0000e-07\n",
      "Epoch 46/100\n",
      "105/105 - 38s - loss: -6.0866e-01 - val_loss: -6.4105e-01 - lr: 5.0000e-07\n",
      "Epoch 47/100\n",
      "105/105 - 38s - loss: -6.0872e-01 - val_loss: -6.4015e-01 - lr: 5.0000e-07\n",
      "Epoch 48/100\n",
      "105/105 - 38s - loss: -6.1717e-01 - val_loss: -6.0749e-01 - lr: 5.0000e-07\n",
      "Epoch 49/100\n",
      "105/105 - 38s - loss: -6.0969e-01 - val_loss: -6.0207e-01 - lr: 5.0000e-07\n",
      "Epoch 50/100\n",
      "105/105 - 38s - loss: -6.0969e-01 - val_loss: -6.5770e-01 - lr: 5.0000e-07\n",
      "Epoch 51/100\n",
      "105/105 - 38s - loss: -6.0925e-01 - val_loss: -6.2530e-01 - lr: 5.0000e-07\n",
      "Epoch 52/100\n",
      "105/105 - 38s - loss: -6.1719e-01 - val_loss: -5.9380e-01 - lr: 5.0000e-07\n",
      "Epoch 53/100\n",
      "105/105 - 38s - loss: -6.1351e-01 - val_loss: -6.0050e-01 - lr: 5.0000e-07\n",
      "Epoch 54/100\n",
      "105/105 - 38s - loss: -6.1311e-01 - val_loss: -6.4944e-01 - lr: 5.0000e-07\n",
      "Epoch 55/100\n",
      "105/105 - 38s - loss: -6.1132e-01 - val_loss: -6.3028e-01 - lr: 5.0000e-07\n",
      "Epoch 56/100\n",
      "105/105 - 38s - loss: -6.0416e-01 - val_loss: -6.5220e-01 - lr: 5.0000e-07\n",
      "Epoch 57/100\n",
      "105/105 - 38s - loss: -6.1414e-01 - val_loss: -6.5749e-01 - lr: 5.0000e-07\n",
      "Epoch 58/100\n",
      "105/105 - 38s - loss: -6.1533e-01 - val_loss: -6.2724e-01 - lr: 5.0000e-07\n",
      "Epoch 59/100\n",
      "105/105 - 38s - loss: -6.1501e-01 - val_loss: -6.3021e-01 - lr: 5.0000e-07\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 2.499999993688107e-07.\n",
      "105/105 - 38s - loss: -6.1829e-01 - val_loss: -6.2414e-01 - lr: 5.0000e-07\n",
      "Epoch 61/100\n",
      "105/105 - 38s - loss: -6.1437e-01 - val_loss: -6.2450e-01 - lr: 2.5000e-07\n",
      "Epoch 62/100\n",
      "105/105 - 38s - loss: -6.1556e-01 - val_loss: -6.3111e-01 - lr: 2.5000e-07\n",
      "Epoch 63/100\n",
      "105/105 - 38s - loss: -6.1678e-01 - val_loss: -6.2426e-01 - lr: 2.5000e-07\n",
      "Epoch 64/100\n",
      "105/105 - 38s - loss: -6.1575e-01 - val_loss: -6.0481e-01 - lr: 2.5000e-07\n",
      "Epoch 65/100\n",
      "105/105 - 38s - loss: -6.1584e-01 - val_loss: -6.2996e-01 - lr: 2.5000e-07\n",
      "Epoch 66/100\n",
      "105/105 - 38s - loss: -6.1671e-01 - val_loss: -6.6897e-01 - lr: 2.5000e-07\n",
      "Epoch 67/100\n",
      "105/105 - 38s - loss: -6.1414e-01 - val_loss: -6.4289e-01 - lr: 2.5000e-07\n",
      "Epoch 68/100\n",
      "105/105 - 38s - loss: -6.1776e-01 - val_loss: -6.1447e-01 - lr: 2.5000e-07\n",
      "Epoch 69/100\n",
      "105/105 - 38s - loss: -6.1055e-01 - val_loss: -6.0303e-01 - lr: 2.5000e-07\n",
      "Epoch 70/100\n",
      "105/105 - 39s - loss: -6.1240e-01 - val_loss: -6.7757e-01 - lr: 2.5000e-07\n",
      "Epoch 71/100\n",
      "105/105 - 38s - loss: -6.1806e-01 - val_loss: -6.0968e-01 - lr: 2.5000e-07\n",
      "Epoch 72/100\n",
      "105/105 - 38s - loss: -6.1711e-01 - val_loss: -6.0709e-01 - lr: 2.5000e-07\n",
      "Epoch 73/100\n",
      "105/105 - 38s - loss: -6.1331e-01 - val_loss: -6.4332e-01 - lr: 2.5000e-07\n",
      "Epoch 74/100\n",
      "105/105 - 38s - loss: -6.1319e-01 - val_loss: -6.6051e-01 - lr: 2.5000e-07\n",
      "Epoch 75/100\n",
      "105/105 - 38s - loss: -6.1766e-01 - val_loss: -5.8212e-01 - lr: 2.5000e-07\n",
      "Epoch 76/100\n",
      "105/105 - 38s - loss: -6.1505e-01 - val_loss: -6.9805e-01 - lr: 2.5000e-07\n",
      "Epoch 77/100\n",
      "105/105 - 38s - loss: -6.1810e-01 - val_loss: -6.0345e-01 - lr: 2.5000e-07\n",
      "Epoch 78/100\n",
      "105/105 - 38s - loss: -6.1963e-01 - val_loss: -6.6258e-01 - lr: 2.5000e-07\n",
      "Epoch 79/100\n",
      "105/105 - 38s - loss: -6.1296e-01 - val_loss: -6.4282e-01 - lr: 2.5000e-07\n",
      "Epoch 80/100\n",
      "105/105 - 38s - loss: -6.1749e-01 - val_loss: -6.4781e-01 - lr: 2.5000e-07\n",
      "Epoch 81/100\n",
      "105/105 - 38s - loss: -6.1848e-01 - val_loss: -6.2718e-01 - lr: 2.5000e-07\n",
      "Epoch 82/100\n",
      "105/105 - 38s - loss: -6.1725e-01 - val_loss: -6.4190e-01 - lr: 2.5000e-07\n",
      "Epoch 83/100\n",
      "105/105 - 38s - loss: -6.1892e-01 - val_loss: -6.6980e-01 - lr: 2.5000e-07\n",
      "Epoch 84/100\n",
      "105/105 - 38s - loss: -6.1737e-01 - val_loss: -6.0097e-01 - lr: 2.5000e-07\n",
      "Epoch 85/100\n",
      "105/105 - 38s - loss: -6.1888e-01 - val_loss: -6.3450e-01 - lr: 2.5000e-07\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.2499999968440534e-07.\n",
      "105/105 - 38s - loss: -6.1566e-01 - val_loss: -6.5833e-01 - lr: 2.5000e-07\n",
      "Epoch 87/100\n",
      "105/105 - 38s - loss: -6.1905e-01 - val_loss: -6.2314e-01 - lr: 1.2500e-07\n",
      "Epoch 88/100\n",
      "105/105 - 38s - loss: -6.1932e-01 - val_loss: -6.3873e-01 - lr: 1.2500e-07\n",
      "Epoch 89/100\n",
      "105/105 - 38s - loss: -6.2381e-01 - val_loss: -6.8035e-01 - lr: 1.2500e-07\n",
      "Epoch 90/100\n",
      "105/105 - 38s - loss: -6.1932e-01 - val_loss: -5.9984e-01 - lr: 1.2500e-07\n",
      "Epoch 91/100\n",
      "105/105 - 38s - loss: -6.1668e-01 - val_loss: -6.4732e-01 - lr: 1.2500e-07\n",
      "Epoch 92/100\n",
      "105/105 - 38s - loss: -6.1908e-01 - val_loss: -6.7956e-01 - lr: 1.2500e-07\n",
      "Epoch 93/100\n",
      "105/105 - 38s - loss: -6.2042e-01 - val_loss: -6.4941e-01 - lr: 1.2500e-07\n",
      "Epoch 94/100\n",
      "105/105 - 38s - loss: -6.1920e-01 - val_loss: -6.2834e-01 - lr: 1.2500e-07\n",
      "Epoch 95/100\n",
      "105/105 - 38s - loss: -6.1730e-01 - val_loss: -6.4592e-01 - lr: 1.2500e-07\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 6.249999984220267e-08.\n",
      "105/105 - 38s - loss: -6.1951e-01 - val_loss: -6.4816e-01 - lr: 1.2500e-07\n",
      "Epoch 97/100\n",
      "105/105 - 38s - loss: -6.1860e-01 - val_loss: -6.1405e-01 - lr: 6.2500e-08\n",
      "Epoch 98/100\n",
      "105/105 - 38s - loss: -6.1989e-01 - val_loss: -5.9791e-01 - lr: 6.2500e-08\n",
      "Epoch 99/100\n",
      "105/105 - 38s - loss: -6.2090e-01 - val_loss: -6.8178e-01 - lr: 6.2500e-08\n",
      "Epoch 100/100\n",
      "105/105 - 38s - loss: -6.2140e-01 - val_loss: -6.2528e-01 - lr: 6.2500e-08\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss_unfreeze' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2731d2e04644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/nidebroux/parameters_analysis/unfreeze_learning_rate.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss_unfreeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss_unfreeze' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=long_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='pretraining_unfreeze_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "val_loss_learning[0,:] = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    pretrained_model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/transferLearned_models/best_pretraining_unfreeze_test.h5')\n",
    "    for layer in pretrained_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    pretrained_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(pretrained_model.optimizer.learning_rate,learning_array[i])\n",
    "    \n",
    "    history = train_model(model=pretrained_model,\n",
    "        path2save=config[\"path2save_finetuned\"],\n",
    "        model_name=_.join(['unfreeze_test_learning',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_learning[i+1,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/unfreeze_learning_rate.npy\",val_loss_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_learning = np.load(main_dir+\"parameters_analysis/unfreeze_learning_rate.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((4,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_learning[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_learning[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_learning[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_learning[3,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'First training results (before unfreezing)')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'learning rate = 0.0001')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'learning rate = 0.00001')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'learning rate = 0.000001')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different learning rates during unfreezing phase ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIRECT RETRAINING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test here which layers we freeze in case of direct retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [0,6,10,16]\n",
    "val_loss_unfreeze = np.zeros((4,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 48, 64, 64, 4 192         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 48, 64, 64, 4 62256       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 48, 64, 64, 4 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 1, 64, 64, 48 49          activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 64, 48 0           conv3d_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 468,193\n",
      "Non-trainable params: 624\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "107/107 - 39s - loss: -4.6870e-01 - val_loss: -3.0050e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "107/107 - 38s - loss: -5.7224e-01 - val_loss: -5.2676e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "107/107 - 39s - loss: -5.9787e-01 - val_loss: -5.8439e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "107/107 - 39s - loss: -6.1670e-01 - val_loss: -5.7787e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "107/107 - 39s - loss: -6.2167e-01 - val_loss: -5.6329e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "107/107 - 39s - loss: -6.3217e-01 - val_loss: -6.0503e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "107/107 - 39s - loss: -6.3652e-01 - val_loss: -5.4928e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "107/107 - 39s - loss: -6.4125e-01 - val_loss: -5.3528e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "107/107 - 39s - loss: -6.4674e-01 - val_loss: -5.9722e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "107/107 - 39s - loss: -6.5464e-01 - val_loss: -6.0116e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "107/107 - 39s - loss: -6.5513e-01 - val_loss: -5.7549e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "107/107 - 39s - loss: -6.5946e-01 - val_loss: -5.5089e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "107/107 - 39s - loss: -6.5926e-01 - val_loss: -6.0876e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "107/107 - 39s - loss: -6.6274e-01 - val_loss: -5.5262e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "107/107 - 39s - loss: -6.6548e-01 - val_loss: -6.1240e-01 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "107/107 - 39s - loss: -6.7073e-01 - val_loss: -6.0992e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "107/107 - 39s - loss: -6.7493e-01 - val_loss: -5.3498e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "107/107 - 39s - loss: -6.7570e-01 - val_loss: -6.2035e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "107/107 - 39s - loss: -6.7858e-01 - val_loss: -6.0118e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "107/107 - 39s - loss: -6.7883e-01 - val_loss: -6.0508e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "107/107 - 39s - loss: -6.7733e-01 - val_loss: -5.5824e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "107/107 - 39s - loss: -6.8316e-01 - val_loss: -5.9373e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "107/107 - 39s - loss: -6.8362e-01 - val_loss: -5.6031e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "107/107 - 39s - loss: -6.8991e-01 - val_loss: -6.2290e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "107/107 - 39s - loss: -6.8604e-01 - val_loss: -5.8988e-01 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "107/107 - 39s - loss: -6.9279e-01 - val_loss: -5.9743e-01 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "107/107 - 39s - loss: -6.9025e-01 - val_loss: -5.9469e-01 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "107/107 - 39s - loss: -6.8963e-01 - val_loss: -6.0214e-01 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "107/107 - 39s - loss: -6.9563e-01 - val_loss: -6.0650e-01 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "107/107 - 39s - loss: -6.9730e-01 - val_loss: -5.8491e-01 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "107/107 - 39s - loss: -6.9628e-01 - val_loss: -6.0458e-01 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "107/107 - 39s - loss: -6.9983e-01 - val_loss: -6.2460e-01 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "107/107 - 39s - loss: -6.9850e-01 - val_loss: -5.6196e-01 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "107/107 - 39s - loss: -6.9793e-01 - val_loss: -6.4009e-01 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "107/107 - 39s - loss: -7.0494e-01 - val_loss: -5.6742e-01 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "107/107 - 39s - loss: -7.0346e-01 - val_loss: -6.0592e-01 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "107/107 - 39s - loss: -7.0204e-01 - val_loss: -5.9696e-01 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "107/107 - 39s - loss: -7.0727e-01 - val_loss: -6.1714e-01 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "107/107 - 39s - loss: -7.0376e-01 - val_loss: -5.7673e-01 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "107/107 - 39s - loss: -7.0494e-01 - val_loss: -5.9850e-01 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "107/107 - 39s - loss: -7.0815e-01 - val_loss: -5.3495e-01 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "107/107 - 39s - loss: -7.1120e-01 - val_loss: -6.4159e-01 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "107/107 - 39s - loss: -7.0811e-01 - val_loss: -6.3460e-01 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "107/107 - 39s - loss: -7.0739e-01 - val_loss: -6.5902e-01 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "107/107 - 39s - loss: -7.1267e-01 - val_loss: -5.5694e-01 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "107/107 - 39s - loss: -7.1120e-01 - val_loss: -6.4454e-01 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "107/107 - 39s - loss: -7.1370e-01 - val_loss: -5.2672e-01 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "107/107 - 39s - loss: -7.1042e-01 - val_loss: -7.3334e-01 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "107/107 - 39s - loss: -7.1632e-01 - val_loss: -5.3610e-01 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "107/107 - 39s - loss: -7.1135e-01 - val_loss: -6.5313e-01 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "107/107 - 39s - loss: -7.1432e-01 - val_loss: -6.6474e-01 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "107/107 - 39s - loss: -7.1425e-01 - val_loss: -6.0175e-01 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "107/107 - 39s - loss: -7.1394e-01 - val_loss: -6.2414e-01 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "107/107 - 39s - loss: -7.1688e-01 - val_loss: -5.9683e-01 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "107/107 - 39s - loss: -7.1783e-01 - val_loss: -6.3168e-01 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "107/107 - 39s - loss: -7.1810e-01 - val_loss: -6.2057e-01 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "107/107 - 39s - loss: -7.1694e-01 - val_loss: -5.8952e-01 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "107/107 - 39s - loss: -7.1897e-01 - val_loss: -5.8818e-01 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "107/107 - 39s - loss: -7.1854e-01 - val_loss: -6.4353e-01 - lr: 5.0000e-06\n",
      "Epoch 60/100\n",
      "107/107 - 39s - loss: -7.2030e-01 - val_loss: -6.3538e-01 - lr: 5.0000e-06\n",
      "Epoch 61/100\n",
      "107/107 - 39s - loss: -7.1867e-01 - val_loss: -5.8506e-01 - lr: 5.0000e-06\n",
      "Epoch 62/100\n",
      "107/107 - 39s - loss: -7.2046e-01 - val_loss: -6.1930e-01 - lr: 5.0000e-06\n",
      "Epoch 63/100\n",
      "107/107 - 39s - loss: -7.2031e-01 - val_loss: -5.9578e-01 - lr: 5.0000e-06\n",
      "Epoch 64/100\n",
      "107/107 - 39s - loss: -7.2086e-01 - val_loss: -6.6980e-01 - lr: 5.0000e-06\n",
      "Epoch 65/100\n",
      "107/107 - 39s - loss: -7.2430e-01 - val_loss: -5.6103e-01 - lr: 5.0000e-06\n",
      "Epoch 66/100\n",
      "107/107 - 39s - loss: -7.2185e-01 - val_loss: -6.4544e-01 - lr: 5.0000e-06\n",
      "Epoch 67/100\n",
      "107/107 - 39s - loss: -7.2150e-01 - val_loss: -6.4923e-01 - lr: 5.0000e-06\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "107/107 - 39s - loss: -7.2145e-01 - val_loss: -5.6828e-01 - lr: 5.0000e-06\n",
      "Epoch 69/100\n",
      "107/107 - 39s - loss: -7.2577e-01 - val_loss: -6.2222e-01 - lr: 2.5000e-06\n",
      "Epoch 70/100\n",
      "107/107 - 39s - loss: -7.2390e-01 - val_loss: -6.2171e-01 - lr: 2.5000e-06\n",
      "Epoch 71/100\n",
      "107/107 - 39s - loss: -7.2540e-01 - val_loss: -6.1088e-01 - lr: 2.5000e-06\n",
      "Epoch 72/100\n",
      "107/107 - 39s - loss: -7.2188e-01 - val_loss: -6.4868e-01 - lr: 2.5000e-06\n",
      "Epoch 73/100\n",
      "107/107 - 39s - loss: -7.2262e-01 - val_loss: -6.0844e-01 - lr: 2.5000e-06\n",
      "Epoch 74/100\n",
      "107/107 - 39s - loss: -7.2326e-01 - val_loss: -5.8139e-01 - lr: 2.5000e-06\n",
      "Epoch 75/100\n",
      "107/107 - 39s - loss: -7.2197e-01 - val_loss: -6.0974e-01 - lr: 2.5000e-06\n",
      "Epoch 76/100\n",
      "107/107 - 39s - loss: -7.2521e-01 - val_loss: -5.8593e-01 - lr: 2.5000e-06\n",
      "Epoch 77/100\n",
      "107/107 - 39s - loss: -7.2615e-01 - val_loss: -6.6559e-01 - lr: 2.5000e-06\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "107/107 - 39s - loss: -7.2566e-01 - val_loss: -6.5475e-01 - lr: 2.5000e-06\n",
      "Epoch 79/100\n",
      "107/107 - 39s - loss: -7.2281e-01 - val_loss: -5.8161e-01 - lr: 1.2500e-06\n",
      "Epoch 80/100\n",
      "107/107 - 39s - loss: -7.2293e-01 - val_loss: -7.0037e-01 - lr: 1.2500e-06\n",
      "Epoch 81/100\n",
      "107/107 - 39s - loss: -7.2534e-01 - val_loss: -5.1866e-01 - lr: 1.2500e-06\n",
      "Epoch 82/100\n",
      "107/107 - 39s - loss: -7.2570e-01 - val_loss: -6.3734e-01 - lr: 1.2500e-06\n",
      "Epoch 83/100\n",
      "107/107 - 39s - loss: -7.2570e-01 - val_loss: -6.6333e-01 - lr: 1.2500e-06\n",
      "Epoch 84/100\n",
      "107/107 - 39s - loss: -7.2603e-01 - val_loss: -6.2413e-01 - lr: 1.2500e-06\n",
      "Epoch 85/100\n",
      "107/107 - 39s - loss: -7.2381e-01 - val_loss: -6.2552e-01 - lr: 1.2500e-06\n",
      "Epoch 86/100\n",
      "107/107 - 39s - loss: -7.2410e-01 - val_loss: -6.4454e-01 - lr: 1.2500e-06\n",
      "Epoch 87/100\n",
      "107/107 - 39s - loss: -7.2509e-01 - val_loss: -6.3914e-01 - lr: 1.2500e-06\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "107/107 - 39s - loss: -7.2428e-01 - val_loss: -5.9529e-01 - lr: 1.2500e-06\n",
      "Epoch 89/100\n",
      "107/107 - 39s - loss: -7.2670e-01 - val_loss: -6.1676e-01 - lr: 6.2500e-07\n",
      "Epoch 90/100\n",
      "107/107 - 39s - loss: -7.2573e-01 - val_loss: -6.5068e-01 - lr: 6.2500e-07\n",
      "Epoch 91/100\n",
      "107/107 - 39s - loss: -7.2165e-01 - val_loss: -6.3093e-01 - lr: 6.2500e-07\n",
      "Epoch 92/100\n",
      "107/107 - 39s - loss: -7.2330e-01 - val_loss: -6.0347e-01 - lr: 6.2500e-07\n",
      "Epoch 93/100\n",
      "107/107 - 39s - loss: -7.2433e-01 - val_loss: -6.5834e-01 - lr: 6.2500e-07\n",
      "Epoch 94/100\n",
      "107/107 - 39s - loss: -7.2854e-01 - val_loss: -5.8034e-01 - lr: 6.2500e-07\n",
      "Epoch 95/100\n",
      "107/107 - 39s - loss: -7.2406e-01 - val_loss: -6.7054e-01 - lr: 6.2500e-07\n",
      "Epoch 96/100\n",
      "107/107 - 39s - loss: -7.2129e-01 - val_loss: -6.0066e-01 - lr: 6.2500e-07\n",
      "Epoch 97/100\n",
      "107/107 - 39s - loss: -7.2638e-01 - val_loss: -6.3357e-01 - lr: 6.2500e-07\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "107/107 - 39s - loss: -7.2514e-01 - val_loss: -6.9711e-01 - lr: 6.2500e-07\n",
      "Epoch 99/100\n",
      "107/107 - 39s - loss: -7.2053e-01 - val_loss: -5.6097e-01 - lr: 3.1250e-07\n",
      "Epoch 100/100\n",
      "107/107 - 39s - loss: -7.2231e-01 - val_loss: -5.7843e-01 - lr: 3.1250e-07\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 48, 64, 64, 4 192         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 48, 64, 64, 4 62256       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 48, 64, 64, 4 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 1, 64, 64, 48 49          activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 64, 48 0           conv3d_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 436,225\n",
      "Non-trainable params: 32,592\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 - 33s - loss: -3.4518e-01 - val_loss: -2.1472e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "107/107 - 33s - loss: -4.8154e-01 - val_loss: -5.3663e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "107/107 - 33s - loss: -5.2825e-01 - val_loss: -5.3803e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "107/107 - 33s - loss: -5.4927e-01 - val_loss: -5.8076e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "107/107 - 33s - loss: -5.6553e-01 - val_loss: -6.1155e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "107/107 - 33s - loss: -5.7700e-01 - val_loss: -5.3510e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "107/107 - 33s - loss: -5.8478e-01 - val_loss: -5.5471e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "107/107 - 33s - loss: -5.9323e-01 - val_loss: -5.6631e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "107/107 - 33s - loss: -5.9508e-01 - val_loss: -5.7407e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "107/107 - 33s - loss: -6.0432e-01 - val_loss: -6.0452e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "107/107 - 33s - loss: -6.1009e-01 - val_loss: -5.9376e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "107/107 - 33s - loss: -6.0973e-01 - val_loss: -5.2339e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "107/107 - 33s - loss: -6.1676e-01 - val_loss: -6.0003e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "107/107 - 33s - loss: -6.1537e-01 - val_loss: -6.1739e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "107/107 - 33s - loss: -6.2381e-01 - val_loss: -5.9115e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "107/107 - 33s - loss: -6.2330e-01 - val_loss: -5.9226e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "107/107 - 33s - loss: -6.2992e-01 - val_loss: -4.9650e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "107/107 - 33s - loss: -6.3081e-01 - val_loss: -5.8951e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "107/107 - 33s - loss: -6.3779e-01 - val_loss: -5.6828e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "107/107 - 33s - loss: -6.3275e-01 - val_loss: -5.9371e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "107/107 - 33s - loss: -6.4038e-01 - val_loss: -5.9424e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "107/107 - 33s - loss: -6.3868e-01 - val_loss: -5.7595e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "107/107 - 33s - loss: -6.4166e-01 - val_loss: -5.7249e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "107/107 - 33s - loss: -6.4439e-01 - val_loss: -6.0038e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "107/107 - 33s - loss: -6.4928e-01 - val_loss: -6.1685e-01 - lr: 5.0000e-06\n",
      "Epoch 26/100\n",
      "107/107 - 33s - loss: -6.4650e-01 - val_loss: -6.2764e-01 - lr: 5.0000e-06\n",
      "Epoch 27/100\n",
      "107/107 - 33s - loss: -6.5063e-01 - val_loss: -5.9827e-01 - lr: 5.0000e-06\n",
      "Epoch 28/100\n",
      "107/107 - 33s - loss: -6.5102e-01 - val_loss: -6.0848e-01 - lr: 5.0000e-06\n",
      "Epoch 29/100\n",
      "107/107 - 33s - loss: -6.5217e-01 - val_loss: -5.5984e-01 - lr: 5.0000e-06\n",
      "Epoch 30/100\n",
      "107/107 - 33s - loss: -6.5002e-01 - val_loss: -5.7863e-01 - lr: 5.0000e-06\n",
      "Epoch 31/100\n",
      "107/107 - 33s - loss: -6.5637e-01 - val_loss: -5.8504e-01 - lr: 5.0000e-06\n",
      "Epoch 32/100\n",
      "107/107 - 33s - loss: -6.5137e-01 - val_loss: -6.3781e-01 - lr: 5.0000e-06\n",
      "Epoch 33/100\n",
      "107/107 - 33s - loss: -6.5523e-01 - val_loss: -6.2108e-01 - lr: 5.0000e-06\n",
      "Epoch 34/100\n",
      "107/107 - 33s - loss: -6.5763e-01 - val_loss: -6.0075e-01 - lr: 5.0000e-06\n",
      "Epoch 35/100\n",
      "107/107 - 33s - loss: -6.5575e-01 - val_loss: -6.2216e-01 - lr: 5.0000e-06\n",
      "Epoch 36/100\n",
      "107/107 - 33s - loss: -6.5615e-01 - val_loss: -6.0639e-01 - lr: 5.0000e-06\n",
      "Epoch 37/100\n",
      "107/107 - 33s - loss: -6.5540e-01 - val_loss: -5.4413e-01 - lr: 5.0000e-06\n",
      "Epoch 38/100\n",
      "107/107 - 33s - loss: -6.6084e-01 - val_loss: -5.9993e-01 - lr: 5.0000e-06\n",
      "Epoch 39/100\n",
      "107/107 - 33s - loss: -6.6068e-01 - val_loss: -5.6949e-01 - lr: 5.0000e-06\n",
      "Epoch 40/100\n",
      "107/107 - 33s - loss: -6.5921e-01 - val_loss: -6.3148e-01 - lr: 5.0000e-06\n",
      "Epoch 41/100\n",
      "107/107 - 33s - loss: -6.6045e-01 - val_loss: -5.8493e-01 - lr: 5.0000e-06\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "107/107 - 33s - loss: -6.6616e-01 - val_loss: -6.2599e-01 - lr: 5.0000e-06\n",
      "Epoch 43/100\n",
      "107/107 - 33s - loss: -6.6561e-01 - val_loss: -6.3594e-01 - lr: 2.5000e-06\n",
      "Epoch 44/100\n",
      "107/107 - 33s - loss: -6.6307e-01 - val_loss: -5.6552e-01 - lr: 2.5000e-06\n",
      "Epoch 45/100\n",
      "107/107 - 33s - loss: -6.5839e-01 - val_loss: -5.9535e-01 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "107/107 - 33s - loss: -6.6522e-01 - val_loss: -6.0967e-01 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "107/107 - 33s - loss: -6.6757e-01 - val_loss: -6.2312e-01 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "107/107 - 33s - loss: -6.6453e-01 - val_loss: -6.0085e-01 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "107/107 - 33s - loss: -6.6287e-01 - val_loss: -6.0775e-01 - lr: 2.5000e-06\n",
      "Epoch 50/100\n",
      "107/107 - 33s - loss: -6.6716e-01 - val_loss: -5.7054e-01 - lr: 2.5000e-06\n",
      "Epoch 51/100\n",
      "107/107 - 33s - loss: -6.6889e-01 - val_loss: -5.5871e-01 - lr: 2.5000e-06\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "107/107 - 33s - loss: -6.6652e-01 - val_loss: -5.8777e-01 - lr: 2.5000e-06\n",
      "Epoch 53/100\n",
      "107/107 - 33s - loss: -6.6874e-01 - val_loss: -5.9767e-01 - lr: 1.2500e-06\n",
      "Epoch 54/100\n",
      "107/107 - 33s - loss: -6.6326e-01 - val_loss: -6.4501e-01 - lr: 1.2500e-06\n",
      "Epoch 55/100\n",
      "107/107 - 33s - loss: -6.6831e-01 - val_loss: -5.9778e-01 - lr: 1.2500e-06\n",
      "Epoch 56/100\n",
      "107/107 - 33s - loss: -6.6705e-01 - val_loss: -5.8442e-01 - lr: 1.2500e-06\n",
      "Epoch 57/100\n",
      "107/107 - 33s - loss: -6.6173e-01 - val_loss: -5.6315e-01 - lr: 1.2500e-06\n",
      "Epoch 58/100\n",
      "107/107 - 33s - loss: -6.7008e-01 - val_loss: -6.0212e-01 - lr: 1.2500e-06\n",
      "Epoch 59/100\n",
      "107/107 - 33s - loss: -6.6955e-01 - val_loss: -5.5583e-01 - lr: 1.2500e-06\n",
      "Epoch 60/100\n",
      "107/107 - 33s - loss: -6.6970e-01 - val_loss: -5.6931e-01 - lr: 1.2500e-06\n",
      "Epoch 61/100\n",
      "107/107 - 33s - loss: -6.6828e-01 - val_loss: -6.4589e-01 - lr: 1.2500e-06\n",
      "Epoch 62/100\n",
      "107/107 - 33s - loss: -6.6854e-01 - val_loss: -6.1345e-01 - lr: 1.2500e-06\n",
      "Epoch 63/100\n",
      "107/107 - 33s - loss: -6.7128e-01 - val_loss: -6.1541e-01 - lr: 1.2500e-06\n",
      "Epoch 64/100\n",
      "107/107 - 33s - loss: -6.7245e-01 - val_loss: -5.9918e-01 - lr: 1.2500e-06\n",
      "Epoch 65/100\n",
      "107/107 - 33s - loss: -6.7039e-01 - val_loss: -6.2155e-01 - lr: 1.2500e-06\n",
      "Epoch 66/100\n",
      "107/107 - 33s - loss: -6.6760e-01 - val_loss: -5.6936e-01 - lr: 1.2500e-06\n",
      "Epoch 67/100\n",
      "107/107 - 33s - loss: -6.7149e-01 - val_loss: -6.2531e-01 - lr: 1.2500e-06\n",
      "Epoch 68/100\n",
      "107/107 - 33s - loss: -6.7116e-01 - val_loss: -6.2096e-01 - lr: 1.2500e-06\n",
      "Epoch 69/100\n",
      "107/107 - 33s - loss: -6.7240e-01 - val_loss: -6.3062e-01 - lr: 1.2500e-06\n",
      "Epoch 70/100\n",
      "107/107 - 33s - loss: -6.6774e-01 - val_loss: -5.9036e-01 - lr: 1.2500e-06\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "107/107 - 33s - loss: -6.7343e-01 - val_loss: -6.1051e-01 - lr: 1.2500e-06\n",
      "Epoch 72/100\n",
      "107/107 - 33s - loss: -6.7128e-01 - val_loss: -6.3700e-01 - lr: 6.2500e-07\n",
      "Epoch 73/100\n",
      "107/107 - 33s - loss: -6.7146e-01 - val_loss: -6.1466e-01 - lr: 6.2500e-07\n",
      "Epoch 74/100\n",
      "107/107 - 33s - loss: -6.7165e-01 - val_loss: -6.1744e-01 - lr: 6.2500e-07\n",
      "Epoch 75/100\n",
      "107/107 - 33s - loss: -6.7101e-01 - val_loss: -5.2432e-01 - lr: 6.2500e-07\n",
      "Epoch 76/100\n",
      "107/107 - 33s - loss: -6.7114e-01 - val_loss: -6.6492e-01 - lr: 6.2500e-07\n",
      "Epoch 77/100\n",
      "107/107 - 33s - loss: -6.7267e-01 - val_loss: -5.6922e-01 - lr: 6.2500e-07\n",
      "Epoch 78/100\n",
      "107/107 - 33s - loss: -6.7167e-01 - val_loss: -6.0670e-01 - lr: 6.2500e-07\n",
      "Epoch 79/100\n",
      "107/107 - 33s - loss: -6.7335e-01 - val_loss: -5.8199e-01 - lr: 6.2500e-07\n",
      "Epoch 80/100\n",
      "107/107 - 33s - loss: -6.7223e-01 - val_loss: -6.3605e-01 - lr: 6.2500e-07\n",
      "Epoch 81/100\n",
      "107/107 - 33s - loss: -6.6591e-01 - val_loss: -6.0230e-01 - lr: 6.2500e-07\n",
      "Epoch 82/100\n",
      "107/107 - 33s - loss: -6.7202e-01 - val_loss: -5.7461e-01 - lr: 6.2500e-07\n",
      "Epoch 83/100\n",
      "107/107 - 33s - loss: -6.7276e-01 - val_loss: -6.1190e-01 - lr: 6.2500e-07\n",
      "Epoch 84/100\n",
      "107/107 - 33s - loss: -6.7287e-01 - val_loss: -6.3648e-01 - lr: 6.2500e-07\n",
      "Epoch 85/100\n",
      "107/107 - 33s - loss: -6.7320e-01 - val_loss: -5.9604e-01 - lr: 6.2500e-07\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "107/107 - 33s - loss: -6.6984e-01 - val_loss: -6.2579e-01 - lr: 6.2500e-07\n",
      "Epoch 87/100\n",
      "107/107 - 33s - loss: -6.7251e-01 - val_loss: -5.5915e-01 - lr: 3.1250e-07\n",
      "Epoch 88/100\n",
      "107/107 - 33s - loss: -6.6939e-01 - val_loss: -6.2562e-01 - lr: 3.1250e-07\n",
      "Epoch 89/100\n",
      "107/107 - 33s - loss: -6.7203e-01 - val_loss: -6.3741e-01 - lr: 3.1250e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "107/107 - 33s - loss: -6.7194e-01 - val_loss: -5.7023e-01 - lr: 3.1250e-07\n",
      "Epoch 91/100\n",
      "107/107 - 33s - loss: -6.7625e-01 - val_loss: -5.4479e-01 - lr: 3.1250e-07\n",
      "Epoch 92/100\n",
      "107/107 - 33s - loss: -6.7227e-01 - val_loss: -6.2350e-01 - lr: 3.1250e-07\n",
      "Epoch 93/100\n",
      "107/107 - 33s - loss: -6.7283e-01 - val_loss: -5.6033e-01 - lr: 3.1250e-07\n",
      "Epoch 94/100\n",
      "107/107 - 33s - loss: -6.6970e-01 - val_loss: -5.9947e-01 - lr: 3.1250e-07\n",
      "Epoch 95/100\n",
      "107/107 - 33s - loss: -6.7211e-01 - val_loss: -6.0914e-01 - lr: 3.1250e-07\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "107/107 - 33s - loss: -6.7272e-01 - val_loss: -5.7263e-01 - lr: 3.1250e-07\n",
      "Epoch 97/100\n",
      "107/107 - 33s - loss: -6.7300e-01 - val_loss: -6.1579e-01 - lr: 1.5625e-07\n",
      "Epoch 98/100\n",
      "107/107 - 33s - loss: -6.7152e-01 - val_loss: -5.8943e-01 - lr: 1.5625e-07\n",
      "Epoch 99/100\n",
      "107/107 - 33s - loss: -6.7048e-01 - val_loss: -6.0670e-01 - lr: 1.5625e-07\n",
      "Epoch 100/100\n",
      "107/107 - 33s - loss: -6.7058e-01 - val_loss: -5.9146e-01 - lr: 1.5625e-07\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 48, 64, 64, 4 192         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 48, 64, 64, 4 62256       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 48, 64, 64, 4 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 1, 64, 64, 48 49          activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 64, 48 0           conv3d_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 373,873\n",
      "Non-trainable params: 94,944\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "107/107 - 32s - loss: -2.5091e-01 - val_loss: -2.1868e-01 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "107/107 - 31s - loss: -4.0379e-01 - val_loss: -4.5964e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "107/107 - 31s - loss: -4.4774e-01 - val_loss: -4.3099e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "107/107 - 31s - loss: -4.7745e-01 - val_loss: -4.9414e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "107/107 - 31s - loss: -4.9509e-01 - val_loss: -4.8859e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "107/107 - 31s - loss: -5.0998e-01 - val_loss: -4.7153e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "107/107 - 31s - loss: -5.2507e-01 - val_loss: -5.5429e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "107/107 - 31s - loss: -5.3330e-01 - val_loss: -5.2541e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "107/107 - 31s - loss: -5.3767e-01 - val_loss: -5.2293e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "107/107 - 31s - loss: -5.4798e-01 - val_loss: -5.1904e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "107/107 - 31s - loss: -5.5317e-01 - val_loss: -6.0521e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "107/107 - 31s - loss: -5.5992e-01 - val_loss: -5.6155e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "107/107 - 32s - loss: -5.6329e-01 - val_loss: -5.5792e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "107/107 - 31s - loss: -5.6363e-01 - val_loss: -4.7496e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "107/107 - 31s - loss: -5.7100e-01 - val_loss: -6.0327e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "107/107 - 31s - loss: -5.7398e-01 - val_loss: -5.6708e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "107/107 - 31s - loss: -5.7590e-01 - val_loss: -5.7761e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "107/107 - 31s - loss: -5.7815e-01 - val_loss: -5.9571e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "107/107 - 31s - loss: -5.8703e-01 - val_loss: -5.5091e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "107/107 - 31s - loss: -5.8301e-01 - val_loss: -5.6037e-01 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "107/107 - 32s - loss: -5.8560e-01 - val_loss: -6.1731e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "107/107 - 31s - loss: -5.9021e-01 - val_loss: -5.5051e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "107/107 - 31s - loss: -5.9596e-01 - val_loss: -5.3282e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "107/107 - 32s - loss: -5.9438e-01 - val_loss: -6.3580e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "107/107 - 31s - loss: -5.9369e-01 - val_loss: -5.0498e-01 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "107/107 - 31s - loss: -5.9522e-01 - val_loss: -5.6120e-01 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "107/107 - 31s - loss: -5.9945e-01 - val_loss: -5.4602e-01 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "107/107 - 31s - loss: -6.0555e-01 - val_loss: -5.6030e-01 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "107/107 - 31s - loss: -6.1093e-01 - val_loss: -5.6099e-01 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "107/107 - 31s - loss: -6.0599e-01 - val_loss: -5.7932e-01 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "107/107 - 31s - loss: -6.0311e-01 - val_loss: -5.9124e-01 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "107/107 - 31s - loss: -6.0822e-01 - val_loss: -5.3450e-01 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "107/107 - 31s - loss: -6.1106e-01 - val_loss: -5.8937e-01 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "107/107 - 31s - loss: -6.1070e-01 - val_loss: -5.6078e-01 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "107/107 - 31s - loss: -6.1572e-01 - val_loss: -5.6489e-01 - lr: 5.0000e-06\n",
      "Epoch 36/100\n",
      "107/107 - 31s - loss: -6.1354e-01 - val_loss: -5.5803e-01 - lr: 5.0000e-06\n",
      "Epoch 37/100\n",
      "107/107 - 31s - loss: -6.1712e-01 - val_loss: -5.2036e-01 - lr: 5.0000e-06\n",
      "Epoch 38/100\n",
      "107/107 - 31s - loss: -6.1417e-01 - val_loss: -6.3225e-01 - lr: 5.0000e-06\n",
      "Epoch 39/100\n",
      "107/107 - 31s - loss: -6.1770e-01 - val_loss: -5.5783e-01 - lr: 5.0000e-06\n",
      "Epoch 40/100\n",
      "107/107 - 31s - loss: -6.2130e-01 - val_loss: -6.2264e-01 - lr: 5.0000e-06\n",
      "Epoch 41/100\n",
      "107/107 - 31s - loss: -6.1998e-01 - val_loss: -5.6291e-01 - lr: 5.0000e-06\n",
      "Epoch 42/100\n",
      "107/107 - 31s - loss: -6.1596e-01 - val_loss: -5.6531e-01 - lr: 5.0000e-06\n",
      "Epoch 43/100\n",
      "107/107 - 31s - loss: -6.2406e-01 - val_loss: -5.6993e-01 - lr: 5.0000e-06\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "107/107 - 31s - loss: -6.2048e-01 - val_loss: -5.7503e-01 - lr: 5.0000e-06\n",
      "Epoch 45/100\n",
      "107/107 - 31s - loss: -6.2263e-01 - val_loss: -5.0113e-01 - lr: 2.5000e-06\n",
      "Epoch 46/100\n",
      "107/107 - 31s - loss: -6.2080e-01 - val_loss: -6.2677e-01 - lr: 2.5000e-06\n",
      "Epoch 47/100\n",
      "107/107 - 31s - loss: -6.2489e-01 - val_loss: -5.0640e-01 - lr: 2.5000e-06\n",
      "Epoch 48/100\n",
      "107/107 - 31s - loss: -6.2131e-01 - val_loss: -6.1938e-01 - lr: 2.5000e-06\n",
      "Epoch 49/100\n",
      "107/107 - 31s - loss: -6.2260e-01 - val_loss: -5.6533e-01 - lr: 2.5000e-06\n",
      "Epoch 50/100\n",
      "107/107 - 31s - loss: -6.2605e-01 - val_loss: -5.8438e-01 - lr: 2.5000e-06\n",
      "Epoch 51/100\n",
      "107/107 - 31s - loss: -6.2625e-01 - val_loss: -5.3056e-01 - lr: 2.5000e-06\n",
      "Epoch 52/100\n",
      "107/107 - 31s - loss: -6.2215e-01 - val_loss: -5.5769e-01 - lr: 2.5000e-06\n",
      "Epoch 53/100\n",
      "107/107 - 31s - loss: -6.2394e-01 - val_loss: -5.7730e-01 - lr: 2.5000e-06\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "107/107 - 31s - loss: -6.1939e-01 - val_loss: -6.0337e-01 - lr: 2.5000e-06\n",
      "Epoch 55/100\n",
      "107/107 - 31s - loss: -6.2448e-01 - val_loss: -5.6266e-01 - lr: 1.2500e-06\n",
      "Epoch 56/100\n",
      "107/107 - 31s - loss: -6.2894e-01 - val_loss: -5.4997e-01 - lr: 1.2500e-06\n",
      "Epoch 57/100\n",
      "107/107 - 31s - loss: -6.2523e-01 - val_loss: -5.6857e-01 - lr: 1.2500e-06\n",
      "Epoch 58/100\n",
      "107/107 - 31s - loss: -6.2692e-01 - val_loss: -5.5330e-01 - lr: 1.2500e-06\n",
      "Epoch 59/100\n",
      "107/107 - 31s - loss: -6.2832e-01 - val_loss: -6.0467e-01 - lr: 1.2500e-06\n",
      "Epoch 60/100\n",
      "107/107 - 31s - loss: -6.2151e-01 - val_loss: -5.9148e-01 - lr: 1.2500e-06\n",
      "Epoch 61/100\n",
      "107/107 - 31s - loss: -6.2602e-01 - val_loss: -5.5800e-01 - lr: 1.2500e-06\n",
      "Epoch 62/100\n",
      "107/107 - 31s - loss: -6.2772e-01 - val_loss: -6.2467e-01 - lr: 1.2500e-06\n",
      "Epoch 63/100\n",
      "107/107 - 31s - loss: -6.2525e-01 - val_loss: -5.3991e-01 - lr: 1.2500e-06\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "107/107 - 31s - loss: -6.2296e-01 - val_loss: -5.5347e-01 - lr: 1.2500e-06\n",
      "Epoch 65/100\n",
      "107/107 - 31s - loss: -6.2742e-01 - val_loss: -6.5642e-01 - lr: 6.2500e-07\n",
      "Epoch 66/100\n",
      "107/107 - 31s - loss: -6.2773e-01 - val_loss: -6.0209e-01 - lr: 6.2500e-07\n",
      "Epoch 67/100\n",
      "107/107 - 31s - loss: -6.2903e-01 - val_loss: -5.9153e-01 - lr: 6.2500e-07\n",
      "Epoch 68/100\n",
      "107/107 - 31s - loss: -6.2634e-01 - val_loss: -5.6753e-01 - lr: 6.2500e-07\n",
      "Epoch 69/100\n",
      "107/107 - 31s - loss: -6.2835e-01 - val_loss: -5.3900e-01 - lr: 6.2500e-07\n",
      "Epoch 70/100\n",
      "107/107 - 31s - loss: -6.2883e-01 - val_loss: -5.7819e-01 - lr: 6.2500e-07\n",
      "Epoch 71/100\n",
      "107/107 - 31s - loss: -6.2975e-01 - val_loss: -5.8145e-01 - lr: 6.2500e-07\n",
      "Epoch 72/100\n",
      "107/107 - 31s - loss: -6.2932e-01 - val_loss: -6.1293e-01 - lr: 6.2500e-07\n",
      "Epoch 73/100\n",
      "107/107 - 31s - loss: -6.2116e-01 - val_loss: -5.5132e-01 - lr: 6.2500e-07\n",
      "Epoch 74/100\n",
      "107/107 - 31s - loss: -6.2973e-01 - val_loss: -6.1337e-01 - lr: 6.2500e-07\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "107/107 - 31s - loss: -6.3013e-01 - val_loss: -5.4408e-01 - lr: 6.2500e-07\n",
      "Epoch 76/100\n",
      "107/107 - 31s - loss: -6.2906e-01 - val_loss: -5.7678e-01 - lr: 3.1250e-07\n",
      "Epoch 77/100\n",
      "107/107 - 31s - loss: -6.3001e-01 - val_loss: -5.7232e-01 - lr: 3.1250e-07\n",
      "Epoch 78/100\n",
      "107/107 - 31s - loss: -6.2713e-01 - val_loss: -5.6808e-01 - lr: 3.1250e-07\n",
      "Epoch 79/100\n",
      "107/107 - 31s - loss: -6.2866e-01 - val_loss: -5.8046e-01 - lr: 3.1250e-07\n",
      "Epoch 80/100\n",
      "107/107 - 31s - loss: -6.2730e-01 - val_loss: -5.9618e-01 - lr: 3.1250e-07\n",
      "Epoch 81/100\n",
      "107/107 - 31s - loss: -6.2782e-01 - val_loss: -6.3617e-01 - lr: 3.1250e-07\n",
      "Epoch 82/100\n",
      "107/107 - 31s - loss: -6.2837e-01 - val_loss: -5.3003e-01 - lr: 3.1250e-07\n",
      "Epoch 83/100\n",
      "107/107 - 31s - loss: -6.2653e-01 - val_loss: -5.8314e-01 - lr: 3.1250e-07\n",
      "Epoch 84/100\n",
      "107/107 - 31s - loss: -6.2711e-01 - val_loss: -5.8094e-01 - lr: 3.1250e-07\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "107/107 - 31s - loss: -6.3158e-01 - val_loss: -5.8537e-01 - lr: 3.1250e-07\n",
      "Epoch 86/100\n",
      "107/107 - 31s - loss: -6.3099e-01 - val_loss: -5.8718e-01 - lr: 1.5625e-07\n",
      "Epoch 87/100\n",
      "107/107 - 31s - loss: -6.2885e-01 - val_loss: -5.6072e-01 - lr: 1.5625e-07\n",
      "Epoch 88/100\n",
      "107/107 - 31s - loss: -6.2773e-01 - val_loss: -5.7075e-01 - lr: 1.5625e-07\n",
      "Epoch 89/100\n",
      "107/107 - 31s - loss: -6.2476e-01 - val_loss: -5.7347e-01 - lr: 1.5625e-07\n",
      "Epoch 90/100\n",
      "107/107 - 31s - loss: -6.2593e-01 - val_loss: -5.7200e-01 - lr: 1.5625e-07\n",
      "Epoch 91/100\n",
      "107/107 - 31s - loss: -6.3125e-01 - val_loss: -5.9165e-01 - lr: 1.5625e-07\n",
      "Epoch 92/100\n",
      "107/107 - 31s - loss: -6.2432e-01 - val_loss: -6.1200e-01 - lr: 1.5625e-07\n",
      "Epoch 93/100\n",
      "107/107 - 31s - loss: -6.3496e-01 - val_loss: -5.1434e-01 - lr: 1.5625e-07\n",
      "Epoch 94/100\n",
      "107/107 - 31s - loss: -6.3091e-01 - val_loss: -5.9532e-01 - lr: 1.5625e-07\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "107/107 - 31s - loss: -6.2151e-01 - val_loss: -5.8827e-01 - lr: 1.5625e-07\n",
      "Epoch 96/100\n",
      "107/107 - 31s - loss: -6.3050e-01 - val_loss: -5.0133e-01 - lr: 7.8125e-08\n",
      "Epoch 97/100\n",
      "107/107 - 31s - loss: -6.2786e-01 - val_loss: -6.0695e-01 - lr: 7.8125e-08\n",
      "Epoch 98/100\n",
      "107/107 - 31s - loss: -6.2772e-01 - val_loss: -5.6362e-01 - lr: 7.8125e-08\n",
      "Epoch 99/100\n",
      "107/107 - 31s - loss: -6.2962e-01 - val_loss: -5.8449e-01 - lr: 7.8125e-08\n",
      "Epoch 100/100\n",
      "107/107 - 31s - loss: -6.2917e-01 - val_loss: -5.5425e-01 - lr: 7.8125e-08\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 48, 64, 64, 4 192         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 48, 64, 64, 4 62256       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 48, 64, 64, 4 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 1, 64, 64, 48 49          activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 64, 48 0           conv3d_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 249,169\n",
      "Non-trainable params: 219,648\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.479532). Check your callbacks.\n",
      "107/107 - 24s - loss: -1.1662e-01 - val_loss: -5.9976e-02 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "107/107 - 22s - loss: -2.6876e-01 - val_loss: -3.1378e-01 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "107/107 - 22s - loss: -3.4546e-01 - val_loss: -4.0928e-01 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "107/107 - 22s - loss: -3.7874e-01 - val_loss: -4.1595e-01 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "107/107 - 22s - loss: -3.9510e-01 - val_loss: -4.0263e-01 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "107/107 - 22s - loss: -4.1780e-01 - val_loss: -4.0998e-01 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "107/107 - 22s - loss: -4.2172e-01 - val_loss: -4.3792e-01 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "107/107 - 22s - loss: -4.4267e-01 - val_loss: -4.3058e-01 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "107/107 - 23s - loss: -4.4802e-01 - val_loss: -4.6470e-01 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "107/107 - 22s - loss: -4.5101e-01 - val_loss: -4.3277e-01 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "107/107 - 22s - loss: -4.6254e-01 - val_loss: -4.5591e-01 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "107/107 - 23s - loss: -4.6812e-01 - val_loss: -4.6963e-01 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "107/107 - 22s - loss: -4.6668e-01 - val_loss: -4.9480e-01 - lr: 1.0000e-05\n",
      "Epoch 14/100\n",
      "107/107 - 22s - loss: -4.7968e-01 - val_loss: -5.2333e-01 - lr: 1.0000e-05\n",
      "Epoch 15/100\n",
      "107/107 - 22s - loss: -4.8101e-01 - val_loss: -4.4621e-01 - lr: 1.0000e-05\n",
      "Epoch 16/100\n",
      "107/107 - 22s - loss: -4.8955e-01 - val_loss: -5.3064e-01 - lr: 1.0000e-05\n",
      "Epoch 17/100\n",
      "107/107 - 22s - loss: -4.9046e-01 - val_loss: -4.6772e-01 - lr: 1.0000e-05\n",
      "Epoch 18/100\n",
      "107/107 - 22s - loss: -4.9292e-01 - val_loss: -5.1152e-01 - lr: 1.0000e-05\n",
      "Epoch 19/100\n",
      "107/107 - 22s - loss: -4.9833e-01 - val_loss: -4.5901e-01 - lr: 1.0000e-05\n",
      "Epoch 20/100\n",
      "107/107 - 22s - loss: -4.9812e-01 - val_loss: -4.9735e-01 - lr: 1.0000e-05\n",
      "Epoch 21/100\n",
      "107/107 - 22s - loss: -5.1141e-01 - val_loss: -5.1570e-01 - lr: 1.0000e-05\n",
      "Epoch 22/100\n",
      "107/107 - 22s - loss: -5.0613e-01 - val_loss: -5.2196e-01 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "107/107 - 22s - loss: -5.1244e-01 - val_loss: -4.7433e-01 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "107/107 - 23s - loss: -5.1086e-01 - val_loss: -5.3603e-01 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "107/107 - 22s - loss: -5.1732e-01 - val_loss: -5.2917e-01 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "107/107 - 22s - loss: -5.2039e-01 - val_loss: -5.4353e-01 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "107/107 - 22s - loss: -5.1984e-01 - val_loss: -5.1393e-01 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      "107/107 - 22s - loss: -5.2043e-01 - val_loss: -4.9200e-01 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "107/107 - 22s - loss: -5.2608e-01 - val_loss: -5.4842e-01 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "107/107 - 22s - loss: -5.2453e-01 - val_loss: -5.2337e-01 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "107/107 - 22s - loss: -5.3117e-01 - val_loss: -5.3666e-01 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "107/107 - 22s - loss: -5.3118e-01 - val_loss: -5.4168e-01 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "107/107 - 22s - loss: -5.3375e-01 - val_loss: -5.0742e-01 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "107/107 - 22s - loss: -5.3624e-01 - val_loss: -5.0299e-01 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "107/107 - 22s - loss: -5.3594e-01 - val_loss: -5.5762e-01 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "107/107 - 22s - loss: -5.3734e-01 - val_loss: -5.0599e-01 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "107/107 - 22s - loss: -5.4097e-01 - val_loss: -5.5877e-01 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "107/107 - 22s - loss: -5.4072e-01 - val_loss: -5.0603e-01 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "107/107 - 22s - loss: -5.4459e-01 - val_loss: -4.6740e-01 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "107/107 - 23s - loss: -5.4477e-01 - val_loss: -5.9157e-01 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "107/107 - 22s - loss: -5.4121e-01 - val_loss: -5.5418e-01 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "107/107 - 22s - loss: -5.5034e-01 - val_loss: -5.0167e-01 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "107/107 - 22s - loss: -5.4730e-01 - val_loss: -5.5051e-01 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "107/107 - 22s - loss: -5.4455e-01 - val_loss: -5.3546e-01 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "107/107 - 22s - loss: -5.5324e-01 - val_loss: -5.4663e-01 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "107/107 - 22s - loss: -5.4910e-01 - val_loss: -5.1322e-01 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "107/107 - 22s - loss: -5.5058e-01 - val_loss: -5.8845e-01 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "107/107 - 22s - loss: -5.5290e-01 - val_loss: -4.8366e-01 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "107/107 - 22s - loss: -5.6149e-01 - val_loss: -5.1076e-01 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "107/107 - 22s - loss: -5.5371e-01 - val_loss: -5.4304e-01 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "107/107 - 23s - loss: -5.5770e-01 - val_loss: -5.7554e-01 - lr: 5.0000e-06\n",
      "Epoch 52/100\n",
      "107/107 - 22s - loss: -5.5468e-01 - val_loss: -5.2474e-01 - lr: 5.0000e-06\n",
      "Epoch 53/100\n",
      "107/107 - 22s - loss: -5.5796e-01 - val_loss: -4.9291e-01 - lr: 5.0000e-06\n",
      "Epoch 54/100\n",
      "107/107 - 22s - loss: -5.5758e-01 - val_loss: -5.2965e-01 - lr: 5.0000e-06\n",
      "Epoch 55/100\n",
      "107/107 - 23s - loss: -5.6082e-01 - val_loss: -6.1481e-01 - lr: 5.0000e-06\n",
      "Epoch 56/100\n",
      "107/107 - 22s - loss: -5.5865e-01 - val_loss: -5.3982e-01 - lr: 5.0000e-06\n",
      "Epoch 57/100\n",
      "107/107 - 22s - loss: -5.5858e-01 - val_loss: -4.6476e-01 - lr: 5.0000e-06\n",
      "Epoch 58/100\n",
      "107/107 - 22s - loss: -5.6141e-01 - val_loss: -5.8917e-01 - lr: 5.0000e-06\n",
      "Epoch 59/100\n",
      "107/107 - 22s - loss: -5.6154e-01 - val_loss: -5.6271e-01 - lr: 5.0000e-06\n",
      "Epoch 60/100\n",
      "107/107 - 22s - loss: -5.6536e-01 - val_loss: -5.1597e-01 - lr: 5.0000e-06\n",
      "Epoch 61/100\n",
      "107/107 - 22s - loss: -5.6440e-01 - val_loss: -5.5501e-01 - lr: 5.0000e-06\n",
      "Epoch 62/100\n",
      "107/107 - 22s - loss: -5.6442e-01 - val_loss: -5.7089e-01 - lr: 5.0000e-06\n",
      "Epoch 63/100\n",
      "107/107 - 22s - loss: -5.6099e-01 - val_loss: -5.3788e-01 - lr: 5.0000e-06\n",
      "Epoch 64/100\n",
      "107/107 - 22s - loss: -5.6173e-01 - val_loss: -5.2905e-01 - lr: 5.0000e-06\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "107/107 - 22s - loss: -5.6432e-01 - val_loss: -5.3893e-01 - lr: 5.0000e-06\n",
      "Epoch 66/100\n",
      "107/107 - 22s - loss: -5.6605e-01 - val_loss: -5.0987e-01 - lr: 2.5000e-06\n",
      "Epoch 67/100\n",
      "107/107 - 22s - loss: -5.5839e-01 - val_loss: -5.2393e-01 - lr: 2.5000e-06\n",
      "Epoch 68/100\n",
      "107/107 - 22s - loss: -5.6965e-01 - val_loss: -5.1948e-01 - lr: 2.5000e-06\n",
      "Epoch 69/100\n",
      "107/107 - 22s - loss: -5.6422e-01 - val_loss: -5.7229e-01 - lr: 2.5000e-06\n",
      "Epoch 70/100\n",
      "107/107 - 22s - loss: -5.6835e-01 - val_loss: -4.9421e-01 - lr: 2.5000e-06\n",
      "Epoch 71/100\n",
      "107/107 - 22s - loss: -5.6770e-01 - val_loss: -5.3499e-01 - lr: 2.5000e-06\n",
      "Epoch 72/100\n",
      "107/107 - 22s - loss: -5.6304e-01 - val_loss: -5.6014e-01 - lr: 2.5000e-06\n",
      "Epoch 73/100\n",
      "107/107 - 22s - loss: -5.7133e-01 - val_loss: -5.5190e-01 - lr: 2.5000e-06\n",
      "Epoch 74/100\n",
      "107/107 - 22s - loss: -5.6357e-01 - val_loss: -5.4080e-01 - lr: 2.5000e-06\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "107/107 - 22s - loss: -5.6780e-01 - val_loss: -5.6311e-01 - lr: 2.5000e-06\n",
      "Epoch 76/100\n",
      "107/107 - 22s - loss: -5.6856e-01 - val_loss: -5.2769e-01 - lr: 1.2500e-06\n",
      "Epoch 77/100\n",
      "107/107 - 22s - loss: -5.6433e-01 - val_loss: -5.2448e-01 - lr: 1.2500e-06\n",
      "Epoch 78/100\n",
      "107/107 - 22s - loss: -5.7256e-01 - val_loss: -5.3725e-01 - lr: 1.2500e-06\n",
      "Epoch 79/100\n",
      "107/107 - 22s - loss: -5.6316e-01 - val_loss: -5.2901e-01 - lr: 1.2500e-06\n",
      "Epoch 80/100\n",
      "107/107 - 22s - loss: -5.6702e-01 - val_loss: -5.2205e-01 - lr: 1.2500e-06\n",
      "Epoch 81/100\n",
      "107/107 - 22s - loss: -5.7136e-01 - val_loss: -5.3117e-01 - lr: 1.2500e-06\n",
      "Epoch 82/100\n",
      "107/107 - 22s - loss: -5.7094e-01 - val_loss: -5.1848e-01 - lr: 1.2500e-06\n",
      "Epoch 83/100\n",
      "107/107 - 22s - loss: -5.6388e-01 - val_loss: -6.0826e-01 - lr: 1.2500e-06\n",
      "Epoch 84/100\n",
      "107/107 - 22s - loss: -5.6986e-01 - val_loss: -5.0237e-01 - lr: 1.2500e-06\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "107/107 - 22s - loss: -5.6642e-01 - val_loss: -5.5968e-01 - lr: 1.2500e-06\n",
      "Epoch 86/100\n",
      "107/107 - 22s - loss: -5.6702e-01 - val_loss: -5.4214e-01 - lr: 6.2500e-07\n",
      "Epoch 87/100\n",
      "107/107 - 22s - loss: -5.7326e-01 - val_loss: -5.9428e-01 - lr: 6.2500e-07\n",
      "Epoch 88/100\n",
      "107/107 - 22s - loss: -5.6486e-01 - val_loss: -5.2265e-01 - lr: 6.2500e-07\n",
      "Epoch 89/100\n",
      "107/107 - 22s - loss: -5.6911e-01 - val_loss: -5.5532e-01 - lr: 6.2500e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100\n",
      "107/107 - 22s - loss: -5.6941e-01 - val_loss: -5.1878e-01 - lr: 6.2500e-07\n",
      "Epoch 91/100\n",
      "107/107 - 22s - loss: -5.6502e-01 - val_loss: -5.5349e-01 - lr: 6.2500e-07\n",
      "Epoch 92/100\n",
      "107/107 - 22s - loss: -5.7004e-01 - val_loss: -5.4974e-01 - lr: 6.2500e-07\n",
      "Epoch 93/100\n",
      "107/107 - 22s - loss: -5.6384e-01 - val_loss: -5.5635e-01 - lr: 6.2500e-07\n",
      "Epoch 94/100\n",
      "107/107 - 22s - loss: -5.7107e-01 - val_loss: -5.3253e-01 - lr: 6.2500e-07\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "107/107 - 22s - loss: -5.7167e-01 - val_loss: -5.3037e-01 - lr: 6.2500e-07\n",
      "Epoch 96/100\n",
      "107/107 - 22s - loss: -5.6823e-01 - val_loss: -5.6540e-01 - lr: 3.1250e-07\n",
      "Epoch 97/100\n",
      "107/107 - 22s - loss: -5.6766e-01 - val_loss: -5.4148e-01 - lr: 3.1250e-07\n",
      "Epoch 98/100\n",
      "107/107 - 22s - loss: -5.6981e-01 - val_loss: -5.3221e-01 - lr: 3.1250e-07\n",
      "Epoch 99/100\n",
      "107/107 - 22s - loss: -5.6970e-01 - val_loss: -5.4907e-01 - lr: 3.1250e-07\n",
      "Epoch 100/100\n",
      "107/107 - 22s - loss: -5.7020e-01 - val_loss: -5.2990e-01 - lr: 3.1250e-07\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=12) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    for layer in model.layers[:depth[i]]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "    K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "    print(model.summary())\n",
    "    \n",
    "    history = train_model(model=model,\n",
    "        path2save=config[\"path2save_retrained\"],\n",
    "        model_name=_.join(['retrained',str(i)]),\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=100,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "    val_loss_unfreeze[i,:] = history.history['val_loss']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/retraining.npy\",val_loss_unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_unfreeze = np.load(main_dir+\"parameters_analysis/retraining.npy\")\n",
    "epochs = range(5,101,5)\n",
    "mean_val_loss = np.zeros((4,20))\n",
    "for i in range(20):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_unfreeze[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_unfreeze[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_unfreeze[2,i:i+5]))\n",
    "    mean_val_loss[3,i]=np.abs(np.mean(val_loss_unfreeze[3,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'frozen parameters = 0')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'frozen parameters = 32 112')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'frozen parameters = 94 560')\n",
    "plt.plot(epochs,mean_val_loss[3,:],'o--',label = 'frozen parameters = 219 024')\n",
    "\n",
    "plt.title('Evolution of the validation loss for different unfreezing possibilities starting from the original model')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_array = [False,True]\n",
    "flip_array = [False,True]\n",
    "val_loss_augment = np.zeros((2,70))\n",
    "train_loss_augment = np.zeros((2,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "Epoch 1/70\n",
      "81/81 - 17s - loss: -3.2698e-01 - val_loss: -3.7344e-01 - lr: 0.0010\n",
      "Epoch 2/70\n",
      "81/81 - 17s - loss: -4.3255e-01 - val_loss: -2.3583e-01 - lr: 0.0010\n",
      "Epoch 3/70\n",
      "81/81 - 17s - loss: -4.8979e-01 - val_loss: -3.5651e-01 - lr: 0.0010\n",
      "Epoch 4/70\n",
      "81/81 - 17s - loss: -5.0962e-01 - val_loss: -4.4597e-01 - lr: 0.0010\n",
      "Epoch 5/70\n",
      "81/81 - 17s - loss: -5.1331e-01 - val_loss: -4.3583e-01 - lr: 0.0010\n",
      "Epoch 6/70\n",
      "81/81 - 18s - loss: -5.2058e-01 - val_loss: -3.4904e-01 - lr: 0.0010\n",
      "Epoch 7/70\n",
      "81/81 - 17s - loss: -5.4033e-01 - val_loss: -3.8580e-01 - lr: 0.0010\n",
      "Epoch 8/70\n",
      "81/81 - 18s - loss: -5.3341e-01 - val_loss: -3.3298e-01 - lr: 0.0010\n",
      "Epoch 9/70\n",
      "81/81 - 17s - loss: -5.3939e-01 - val_loss: -4.3313e-01 - lr: 0.0010\n",
      "Epoch 10/70\n",
      "81/81 - 17s - loss: -5.5587e-01 - val_loss: -4.4067e-01 - lr: 0.0010\n",
      "Epoch 11/70\n",
      "81/81 - 17s - loss: -5.7179e-01 - val_loss: -4.9570e-01 - lr: 0.0010\n",
      "Epoch 12/70\n",
      "81/81 - 17s - loss: -5.7497e-01 - val_loss: -4.7423e-01 - lr: 0.0010\n",
      "Epoch 13/70\n",
      "81/81 - 17s - loss: -5.7104e-01 - val_loss: -4.8369e-01 - lr: 0.0010\n",
      "Epoch 14/70\n",
      "81/81 - 17s - loss: -5.7719e-01 - val_loss: -2.8332e-01 - lr: 0.0010\n",
      "Epoch 15/70\n",
      "81/81 - 18s - loss: -5.8245e-01 - val_loss: -5.0461e-01 - lr: 0.0010\n",
      "Epoch 16/70\n",
      "81/81 - 18s - loss: -5.8101e-01 - val_loss: -3.9988e-01 - lr: 0.0010\n",
      "Epoch 17/70\n",
      "81/81 - 18s - loss: -5.8405e-01 - val_loss: -4.9897e-01 - lr: 0.0010\n",
      "Epoch 18/70\n",
      "81/81 - 17s - loss: -6.0618e-01 - val_loss: -4.9452e-01 - lr: 0.0010\n",
      "Epoch 19/70\n",
      "81/81 - 17s - loss: -6.0355e-01 - val_loss: -5.1424e-01 - lr: 0.0010\n",
      "Epoch 20/70\n",
      "81/81 - 17s - loss: -6.1013e-01 - val_loss: -4.6886e-01 - lr: 0.0010\n",
      "Epoch 21/70\n",
      "81/81 - 17s - loss: -6.0170e-01 - val_loss: -5.1080e-01 - lr: 0.0010\n",
      "Epoch 22/70\n",
      "81/81 - 17s - loss: -6.0191e-01 - val_loss: -4.8696e-01 - lr: 0.0010\n",
      "Epoch 23/70\n",
      "81/81 - 17s - loss: -6.1569e-01 - val_loss: -4.9044e-01 - lr: 0.0010\n",
      "Epoch 24/70\n",
      "81/81 - 18s - loss: -6.2358e-01 - val_loss: -5.7502e-01 - lr: 0.0010\n",
      "Epoch 25/70\n",
      "81/81 - 18s - loss: -6.1824e-01 - val_loss: -3.9101e-01 - lr: 0.0010\n",
      "Epoch 26/70\n",
      "81/81 - 18s - loss: -6.2773e-01 - val_loss: -3.3482e-01 - lr: 0.0010\n",
      "Epoch 27/70\n",
      "81/81 - 17s - loss: -6.2982e-01 - val_loss: -4.8816e-01 - lr: 0.0010\n",
      "Epoch 28/70\n",
      "81/81 - 17s - loss: -6.3411e-01 - val_loss: -4.9848e-01 - lr: 0.0010\n",
      "Epoch 29/70\n",
      "81/81 - 17s - loss: -6.2211e-01 - val_loss: -3.7366e-01 - lr: 0.0010\n",
      "Epoch 30/70\n",
      "81/81 - 17s - loss: -6.1959e-01 - val_loss: -5.4399e-01 - lr: 0.0010\n",
      "Epoch 31/70\n",
      "81/81 - 17s - loss: -6.2105e-01 - val_loss: -5.0605e-01 - lr: 0.0010\n",
      "Epoch 32/70\n",
      "81/81 - 18s - loss: -6.3277e-01 - val_loss: -5.3448e-01 - lr: 0.0010\n",
      "Epoch 33/70\n",
      "81/81 - 18s - loss: -6.4514e-01 - val_loss: -5.5198e-01 - lr: 0.0010\n",
      "Epoch 34/70\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "81/81 - 18s - loss: -6.3616e-01 - val_loss: -5.0000e-01 - lr: 0.0010\n",
      "Epoch 35/70\n",
      "81/81 - 18s - loss: -6.6065e-01 - val_loss: -5.0599e-01 - lr: 5.0000e-04\n",
      "Epoch 36/70\n",
      "81/81 - 17s - loss: -6.6555e-01 - val_loss: -5.2756e-01 - lr: 5.0000e-04\n",
      "Epoch 37/70\n",
      "81/81 - 17s - loss: -6.5783e-01 - val_loss: -5.7067e-01 - lr: 5.0000e-04\n",
      "Epoch 38/70\n",
      "81/81 - 17s - loss: -6.5814e-01 - val_loss: -4.5118e-01 - lr: 5.0000e-04\n",
      "Epoch 39/70\n",
      "81/81 - 17s - loss: -6.6402e-01 - val_loss: -5.1853e-01 - lr: 5.0000e-04\n",
      "Epoch 40/70\n",
      "81/81 - 17s - loss: -6.6179e-01 - val_loss: -5.7987e-01 - lr: 5.0000e-04\n",
      "Epoch 41/70\n",
      "81/81 - 18s - loss: -6.6543e-01 - val_loss: -5.9518e-01 - lr: 5.0000e-04\n",
      "Epoch 42/70\n",
      "81/81 - 18s - loss: -6.7187e-01 - val_loss: -4.2761e-01 - lr: 5.0000e-04\n",
      "Epoch 43/70\n",
      "81/81 - 18s - loss: -6.7488e-01 - val_loss: -5.4408e-01 - lr: 5.0000e-04\n",
      "Epoch 44/70\n",
      "81/81 - 17s - loss: -6.7710e-01 - val_loss: -5.4210e-01 - lr: 5.0000e-04\n",
      "Epoch 45/70\n",
      "81/81 - 17s - loss: -6.7752e-01 - val_loss: -5.2462e-01 - lr: 5.0000e-04\n",
      "Epoch 46/70\n",
      "81/81 - 17s - loss: -6.7972e-01 - val_loss: -5.3557e-01 - lr: 5.0000e-04\n",
      "Epoch 47/70\n",
      "81/81 - 17s - loss: -6.6910e-01 - val_loss: -4.4983e-01 - lr: 5.0000e-04\n",
      "Epoch 48/70\n",
      "81/81 - 17s - loss: -6.7191e-01 - val_loss: -4.4489e-01 - lr: 5.0000e-04\n",
      "Epoch 49/70\n",
      "81/81 - 17s - loss: -6.7729e-01 - val_loss: -4.9915e-01 - lr: 5.0000e-04\n",
      "Epoch 50/70\n",
      "81/81 - 18s - loss: -6.7824e-01 - val_loss: -5.5859e-01 - lr: 5.0000e-04\n",
      "Epoch 51/70\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "81/81 - 18s - loss: -6.8222e-01 - val_loss: -5.1980e-01 - lr: 5.0000e-04\n",
      "Epoch 52/70\n",
      "81/81 - 18s - loss: -6.9262e-01 - val_loss: -5.0498e-01 - lr: 2.5000e-04\n",
      "Epoch 53/70\n",
      "81/81 - 17s - loss: -6.9446e-01 - val_loss: -5.0469e-01 - lr: 2.5000e-04\n",
      "Epoch 54/70\n",
      "81/81 - 17s - loss: -6.9480e-01 - val_loss: -5.3643e-01 - lr: 2.5000e-04\n",
      "Epoch 55/70\n",
      "81/81 - 17s - loss: -6.9080e-01 - val_loss: -5.0192e-01 - lr: 2.5000e-04\n",
      "Epoch 56/70\n",
      "81/81 - 17s - loss: -6.9048e-01 - val_loss: -5.3804e-01 - lr: 2.5000e-04\n",
      "Epoch 57/70\n",
      "81/81 - 17s - loss: -6.9307e-01 - val_loss: -5.9430e-01 - lr: 2.5000e-04\n",
      "Epoch 58/70\n",
      "81/81 - 17s - loss: -6.9783e-01 - val_loss: -5.0142e-01 - lr: 2.5000e-04\n",
      "Epoch 59/70\n",
      "81/81 - 18s - loss: -6.9930e-01 - val_loss: -5.2714e-01 - lr: 2.5000e-04\n",
      "Epoch 60/70\n",
      "81/81 - 18s - loss: -6.9616e-01 - val_loss: -5.6719e-01 - lr: 2.5000e-04\n",
      "Epoch 61/70\n",
      "81/81 - 18s - loss: -6.9770e-01 - val_loss: -6.0744e-01 - lr: 2.5000e-04\n",
      "Epoch 62/70\n",
      "81/81 - 17s - loss: -6.9761e-01 - val_loss: -5.1267e-01 - lr: 2.5000e-04\n",
      "Epoch 63/70\n",
      "81/81 - 17s - loss: -6.9716e-01 - val_loss: -5.7287e-01 - lr: 2.5000e-04\n",
      "Epoch 64/70\n",
      "81/81 - 17s - loss: -7.0370e-01 - val_loss: -5.6324e-01 - lr: 2.5000e-04\n",
      "Epoch 65/70\n",
      "81/81 - 17s - loss: -7.0096e-01 - val_loss: -5.5413e-01 - lr: 2.5000e-04\n",
      "Epoch 66/70\n",
      "81/81 - 17s - loss: -7.0467e-01 - val_loss: -5.8559e-01 - lr: 2.5000e-04\n",
      "Epoch 67/70\n",
      "81/81 - 17s - loss: -7.0870e-01 - val_loss: -5.4300e-01 - lr: 2.5000e-04\n",
      "Epoch 68/70\n",
      "81/81 - 18s - loss: -7.0540e-01 - val_loss: -5.5898e-01 - lr: 2.5000e-04\n",
      "Epoch 69/70\n",
      "81/81 - 18s - loss: -7.0517e-01 - val_loss: -5.5986e-01 - lr: 2.5000e-04\n",
      "Epoch 70/70\n",
      "81/81 - 18s - loss: -7.0659e-01 - val_loss: -5.2116e-01 - lr: 2.5000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "Epoch 1/70\n",
      "81/81 - 17s - loss: -2.9280e-01 - val_loss: -2.6007e-01 - lr: 0.0010\n",
      "Epoch 2/70\n",
      "81/81 - 17s - loss: -3.8883e-01 - val_loss: -3.9329e-01 - lr: 0.0010\n",
      "Epoch 3/70\n",
      "81/81 - 17s - loss: -4.1730e-01 - val_loss: -3.1195e-01 - lr: 0.0010\n",
      "Epoch 4/70\n",
      "81/81 - 17s - loss: -4.7139e-01 - val_loss: -3.1802e-01 - lr: 0.0010\n",
      "Epoch 5/70\n",
      "81/81 - 17s - loss: -4.8575e-01 - val_loss: -3.9230e-01 - lr: 0.0010\n",
      "Epoch 6/70\n",
      "81/81 - 18s - loss: -5.1963e-01 - val_loss: -3.7909e-01 - lr: 0.0010\n",
      "Epoch 7/70\n",
      "81/81 - 18s - loss: -4.9347e-01 - val_loss: -4.3604e-01 - lr: 0.0010\n",
      "Epoch 8/70\n",
      "81/81 - 18s - loss: -5.0684e-01 - val_loss: -2.7741e-01 - lr: 0.0010\n",
      "Epoch 9/70\n",
      "81/81 - 17s - loss: -5.3278e-01 - val_loss: -4.4195e-01 - lr: 0.0010\n",
      "Epoch 10/70\n",
      "81/81 - 17s - loss: -5.3377e-01 - val_loss: -4.2125e-01 - lr: 0.0010\n",
      "Epoch 11/70\n",
      "81/81 - 17s - loss: -5.4157e-01 - val_loss: -4.9256e-01 - lr: 0.0010\n",
      "Epoch 12/70\n",
      "81/81 - 17s - loss: -5.3025e-01 - val_loss: -4.2015e-01 - lr: 0.0010\n",
      "Epoch 13/70\n",
      "81/81 - 17s - loss: -5.4510e-01 - val_loss: -4.5504e-01 - lr: 0.0010\n",
      "Epoch 14/70\n",
      "81/81 - 17s - loss: -5.5676e-01 - val_loss: -4.4442e-01 - lr: 0.0010\n",
      "Epoch 15/70\n",
      "81/81 - 18s - loss: -5.5285e-01 - val_loss: -5.0833e-01 - lr: 0.0010\n",
      "Epoch 16/70\n",
      "81/81 - 18s - loss: -5.5822e-01 - val_loss: -4.7229e-01 - lr: 0.0010\n",
      "Epoch 17/70\n",
      "81/81 - 18s - loss: -5.5897e-01 - val_loss: -2.7339e-01 - lr: 0.0010\n",
      "Epoch 18/70\n",
      "81/81 - 17s - loss: -5.4311e-01 - val_loss: -3.9569e-01 - lr: 0.0010\n",
      "Epoch 19/70\n",
      "81/81 - 17s - loss: -5.6557e-01 - val_loss: -3.5226e-01 - lr: 0.0010\n",
      "Epoch 20/70\n",
      "81/81 - 17s - loss: -5.6468e-01 - val_loss: -3.5138e-01 - lr: 0.0010\n",
      "Epoch 21/70\n",
      "81/81 - 17s - loss: -5.6515e-01 - val_loss: -4.8828e-01 - lr: 0.0010\n",
      "Epoch 22/70\n",
      "81/81 - 17s - loss: -5.7091e-01 - val_loss: -4.5842e-01 - lr: 0.0010\n",
      "Epoch 23/70\n",
      "81/81 - 17s - loss: -5.5730e-01 - val_loss: -3.6451e-01 - lr: 0.0010\n",
      "Epoch 24/70\n",
      "81/81 - 18s - loss: -5.7783e-01 - val_loss: -4.1315e-01 - lr: 0.0010\n",
      "Epoch 25/70\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "81/81 - 18s - loss: -5.7614e-01 - val_loss: -4.1250e-01 - lr: 0.0010\n",
      "Epoch 26/70\n",
      "81/81 - 18s - loss: -5.8111e-01 - val_loss: -5.4968e-01 - lr: 5.0000e-04\n",
      "Epoch 27/70\n",
      "81/81 - 17s - loss: -5.8733e-01 - val_loss: -4.5534e-01 - lr: 5.0000e-04\n",
      "Epoch 28/70\n",
      "81/81 - 17s - loss: -5.8711e-01 - val_loss: -4.7326e-01 - lr: 5.0000e-04\n",
      "Epoch 29/70\n",
      "81/81 - 17s - loss: -5.9024e-01 - val_loss: -4.8482e-01 - lr: 5.0000e-04\n",
      "Epoch 30/70\n",
      "81/81 - 17s - loss: -5.9844e-01 - val_loss: -5.0250e-01 - lr: 5.0000e-04\n",
      "Epoch 31/70\n",
      "81/81 - 17s - loss: -5.9699e-01 - val_loss: -4.4759e-01 - lr: 5.0000e-04\n",
      "Epoch 32/70\n",
      "81/81 - 17s - loss: -5.9642e-01 - val_loss: -5.1349e-01 - lr: 5.0000e-04\n",
      "Epoch 33/70\n",
      "81/81 - 18s - loss: -5.9666e-01 - val_loss: -5.1172e-01 - lr: 5.0000e-04\n",
      "Epoch 34/70\n",
      "81/81 - 18s - loss: -5.9177e-01 - val_loss: -4.7882e-01 - lr: 5.0000e-04\n",
      "Epoch 35/70\n",
      "81/81 - 18s - loss: -5.9970e-01 - val_loss: -5.1718e-01 - lr: 5.0000e-04\n",
      "Epoch 36/70\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "81/81 - 17s - loss: -5.9698e-01 - val_loss: -4.9409e-01 - lr: 5.0000e-04\n",
      "Epoch 37/70\n",
      "81/81 - 17s - loss: -6.0692e-01 - val_loss: -5.2140e-01 - lr: 2.5000e-04\n",
      "Epoch 38/70\n",
      "81/81 - 17s - loss: -6.0875e-01 - val_loss: -4.8706e-01 - lr: 2.5000e-04\n",
      "Epoch 39/70\n",
      "81/81 - 17s - loss: -6.1346e-01 - val_loss: -4.6286e-01 - lr: 2.5000e-04\n",
      "Epoch 40/70\n",
      "81/81 - 17s - loss: -6.0565e-01 - val_loss: -4.7255e-01 - lr: 2.5000e-04\n",
      "Epoch 41/70\n",
      "81/81 - 18s - loss: -6.0851e-01 - val_loss: -4.7446e-01 - lr: 2.5000e-04\n",
      "Epoch 42/70\n",
      "81/81 - 18s - loss: -6.0671e-01 - val_loss: -5.0026e-01 - lr: 2.5000e-04\n",
      "Epoch 43/70\n",
      "81/81 - 18s - loss: -6.0950e-01 - val_loss: -4.8884e-01 - lr: 2.5000e-04\n",
      "Epoch 44/70\n",
      "81/81 - 17s - loss: -6.1151e-01 - val_loss: -5.1921e-01 - lr: 2.5000e-04\n",
      "Epoch 45/70\n",
      "81/81 - 17s - loss: -6.0749e-01 - val_loss: -4.6013e-01 - lr: 2.5000e-04\n",
      "Epoch 46/70\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "81/81 - 17s - loss: -6.0498e-01 - val_loss: -5.3780e-01 - lr: 2.5000e-04\n",
      "Epoch 47/70\n",
      "81/81 - 17s - loss: -6.1505e-01 - val_loss: -4.6935e-01 - lr: 1.2500e-04\n",
      "Epoch 48/70\n",
      "81/81 - 17s - loss: -6.1370e-01 - val_loss: -5.4364e-01 - lr: 1.2500e-04\n",
      "Epoch 49/70\n",
      "81/81 - 17s - loss: -6.1310e-01 - val_loss: -4.9238e-01 - lr: 1.2500e-04\n",
      "Epoch 50/70\n",
      "81/81 - 18s - loss: -6.1761e-01 - val_loss: -5.3898e-01 - lr: 1.2500e-04\n",
      "Epoch 51/70\n",
      "81/81 - 18s - loss: -6.2085e-01 - val_loss: -5.1035e-01 - lr: 1.2500e-04\n",
      "Epoch 52/70\n",
      "81/81 - 18s - loss: -6.1982e-01 - val_loss: -4.8360e-01 - lr: 1.2500e-04\n",
      "Epoch 53/70\n",
      "81/81 - 17s - loss: -6.2021e-01 - val_loss: -5.1871e-01 - lr: 1.2500e-04\n",
      "Epoch 54/70\n",
      "81/81 - 17s - loss: -6.1927e-01 - val_loss: -5.1946e-01 - lr: 1.2500e-04\n",
      "Epoch 55/70\n",
      "81/81 - 17s - loss: -6.1965e-01 - val_loss: -5.1753e-01 - lr: 1.2500e-04\n",
      "Epoch 56/70\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "81/81 - 17s - loss: -6.2083e-01 - val_loss: -5.2415e-01 - lr: 1.2500e-04\n",
      "Epoch 57/70\n",
      "81/81 - 17s - loss: -6.2659e-01 - val_loss: -5.0211e-01 - lr: 6.2500e-05\n",
      "Epoch 58/70\n",
      "81/81 - 18s - loss: -6.2411e-01 - val_loss: -5.3010e-01 - lr: 6.2500e-05\n",
      "Epoch 59/70\n",
      "81/81 - 18s - loss: -6.2137e-01 - val_loss: -5.0078e-01 - lr: 6.2500e-05\n",
      "Epoch 60/70\n",
      "81/81 - 18s - loss: -6.2566e-01 - val_loss: -5.0462e-01 - lr: 6.2500e-05\n",
      "Epoch 61/70\n",
      "81/81 - 18s - loss: -6.2568e-01 - val_loss: -5.2104e-01 - lr: 6.2500e-05\n",
      "Epoch 62/70\n",
      "81/81 - 17s - loss: -6.2794e-01 - val_loss: -4.9308e-01 - lr: 6.2500e-05\n",
      "Epoch 63/70\n",
      "81/81 - 17s - loss: -6.2607e-01 - val_loss: -5.1430e-01 - lr: 6.2500e-05\n",
      "Epoch 64/70\n",
      "81/81 - 17s - loss: -6.2918e-01 - val_loss: -5.4586e-01 - lr: 6.2500e-05\n",
      "Epoch 65/70\n",
      "81/81 - 17s - loss: -6.2605e-01 - val_loss: -4.8212e-01 - lr: 6.2500e-05\n",
      "Epoch 66/70\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "81/81 - 17s - loss: -6.3012e-01 - val_loss: -5.3818e-01 - lr: 6.2500e-05\n",
      "Epoch 67/70\n",
      "81/81 - 18s - loss: -6.2820e-01 - val_loss: -4.8738e-01 - lr: 3.1250e-05\n",
      "Epoch 68/70\n",
      "81/81 - 18s - loss: -6.2993e-01 - val_loss: -5.9360e-01 - lr: 3.1250e-05\n",
      "Epoch 69/70\n",
      "81/81 - 18s - loss: -6.2629e-01 - val_loss: -4.8840e-01 - lr: 3.1250e-05\n",
      "Epoch 70/70\n",
      "81/81 - 17s - loss: -6.2911e-01 - val_loss: -5.2273e-01 - lr: 3.1250e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=16) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=augment_array[i],\n",
    "                                                    augment_flip=flip_array[i])\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_retraining\"],\n",
    "            model_name='_'.join([\"augment_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=70,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_augment[i,:] = history.history['val_loss']\n",
    "    train_loss_augment[i,:] = history.history['loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/augment_val.npy\",val_loss_augment)\n",
    "np.save(main_dir+\"parameters_analysis/augment_train.npy\",train_loss_augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_augment = np.load(main_dir+\"parameters_analysis/augment_val.npy\")\n",
    "loss_augment = np.load(main_dir+\"parameters_analysis/augment_train.npy\")\n",
    "epochs = range(5,71,5)\n",
    "mean_val_loss = np.zeros((2,14))\n",
    "mean_loss = np.zeros((2,14))\n",
    "for i in range(14):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_augment[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_augment[1,i:i+5]))\n",
    "    mean_loss[0,i]=np.abs(np.mean(loss_augment[0,i:i+5]))\n",
    "    mean_loss[1,i]=np.abs(np.mean(loss_augment[1,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'validation loss without data augmentation')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'validation loss with data augmentation')\n",
    "plt.plot(epochs,mean_loss[0,:],'o--',label = 'train loss without data augmentation')\n",
    "plt.plot(epochs,mean_loss[1,:],'o--',label = 'train loss with data augmentation')\n",
    "\n",
    "plt.title('Evolution of the validation and train loss with and without data augmentation')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation/train loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND OVERLAP TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_array = [12,6,3]\n",
    "val_loss_overlap = np.zeros((3,50))\n",
    "loss_overlap = np.zeros((3,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "106/106 - 23s - loss: -3.1376e-01 - val_loss: -3.0279e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "106/106 - 22s - loss: -3.9183e-01 - val_loss: -3.6529e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "106/106 - 22s - loss: -4.4575e-01 - val_loss: -3.5367e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "106/106 - 22s - loss: -4.7427e-01 - val_loss: -3.9776e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "106/106 - 22s - loss: -4.8202e-01 - val_loss: -4.1621e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "106/106 - 23s - loss: -5.0826e-01 - val_loss: -3.7588e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "106/106 - 23s - loss: -4.8146e-01 - val_loss: -4.2171e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "106/106 - 22s - loss: -5.2650e-01 - val_loss: -4.5504e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "106/106 - 22s - loss: -5.3299e-01 - val_loss: -3.5466e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "106/106 - 22s - loss: -5.2661e-01 - val_loss: -4.7768e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "106/106 - 22s - loss: -5.4382e-01 - val_loss: -4.8935e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "106/106 - 23s - loss: -5.5653e-01 - val_loss: -4.6809e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "106/106 - 23s - loss: -5.3656e-01 - val_loss: -3.9286e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "106/106 - 23s - loss: -5.5233e-01 - val_loss: -4.3543e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "106/106 - 22s - loss: -5.5233e-01 - val_loss: -4.3408e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "106/106 - 22s - loss: -5.5425e-01 - val_loss: -4.7591e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "106/106 - 22s - loss: -5.6088e-01 - val_loss: -4.3150e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "106/106 - 22s - loss: -5.8363e-01 - val_loss: -4.6103e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "106/106 - 23s - loss: -5.8514e-01 - val_loss: -4.3547e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "106/106 - 23s - loss: -5.7769e-01 - val_loss: -4.6971e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "106/106 - 23s - loss: -5.8595e-01 - val_loss: -5.0358e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "106/106 - 22s - loss: -5.9273e-01 - val_loss: -4.5167e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "106/106 - 22s - loss: -5.7053e-01 - val_loss: -4.7967e-01 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "106/106 - 22s - loss: -5.9339e-01 - val_loss: -5.0909e-01 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "106/106 - 22s - loss: -5.9998e-01 - val_loss: -4.8903e-01 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "106/106 - 23s - loss: -5.8800e-01 - val_loss: -3.9961e-01 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "106/106 - 23s - loss: -5.8650e-01 - val_loss: -4.8501e-01 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "106/106 - 23s - loss: -6.0759e-01 - val_loss: -5.0396e-01 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "106/106 - 22s - loss: -6.0665e-01 - val_loss: -4.3786e-01 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "106/106 - 22s - loss: -6.1234e-01 - val_loss: -5.5580e-01 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "106/106 - 22s - loss: -6.0473e-01 - val_loss: -4.3494e-01 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "106/106 - 22s - loss: -6.1197e-01 - val_loss: -4.3740e-01 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "106/106 - 23s - loss: -6.1415e-01 - val_loss: -4.7314e-01 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "106/106 - 23s - loss: -6.1570e-01 - val_loss: -4.8021e-01 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "106/106 - 23s - loss: -6.2090e-01 - val_loss: -4.6455e-01 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "106/106 - 22s - loss: -6.2006e-01 - val_loss: -4.4753e-01 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "106/106 - 22s - loss: -6.1724e-01 - val_loss: -4.9074e-01 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "106/106 - 22s - loss: -6.1689e-01 - val_loss: -4.8499e-01 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "106/106 - 22s - loss: -6.2325e-01 - val_loss: -4.8531e-01 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "106/106 - 23s - loss: -6.2406e-01 - val_loss: -4.9395e-01 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "106/106 - 23s - loss: -6.3404e-01 - val_loss: -5.4840e-01 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "106/106 - 23s - loss: -6.3654e-01 - val_loss: -4.8517e-01 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "106/106 - 22s - loss: -6.3748e-01 - val_loss: -4.8128e-01 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "106/106 - 22s - loss: -6.3908e-01 - val_loss: -5.4571e-01 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "106/106 - 22s - loss: -6.4109e-01 - val_loss: -4.5978e-01 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "106/106 - 22s - loss: -6.3500e-01 - val_loss: -4.9443e-01 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "106/106 - 23s - loss: -6.3902e-01 - val_loss: -5.3008e-01 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "106/106 - 23s - loss: -6.4294e-01 - val_loss: -5.2470e-01 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "106/106 - 22s - loss: -6.4151e-01 - val_loss: -4.9578e-01 - lr: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "106/106 - 22s - loss: -6.3608e-01 - val_loss: -5.2062e-01 - lr: 5.0000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "209/209 - 43s - loss: -3.6819e-01 - val_loss: -3.7408e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "209/209 - 45s - loss: -4.7848e-01 - val_loss: -3.9794e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "209/209 - 44s - loss: -5.0837e-01 - val_loss: -3.9220e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "209/209 - 43s - loss: -5.3246e-01 - val_loss: -4.8512e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "209/209 - 44s - loss: -5.4889e-01 - val_loss: -4.4983e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "209/209 - 45s - loss: -5.6019e-01 - val_loss: -4.2492e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "209/209 - 43s - loss: -5.6710e-01 - val_loss: -4.9708e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "209/209 - 43s - loss: -5.7972e-01 - val_loss: -5.4898e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "209/209 - 44s - loss: -5.8610e-01 - val_loss: -3.9572e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "209/209 - 44s - loss: -5.8119e-01 - val_loss: -5.0557e-01 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "209/209 - 43s - loss: -5.9662e-01 - val_loss: -5.0431e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "209/209 - 44s - loss: -5.9412e-01 - val_loss: -4.7272e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "209/209 - 45s - loss: -5.9697e-01 - val_loss: -5.1491e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "209/209 - 43s - loss: -6.0490e-01 - val_loss: -4.9706e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "209/209 - 43s - loss: -6.0515e-01 - val_loss: -5.0011e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "209/209 - 44s - loss: -6.1289e-01 - val_loss: -4.5425e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "209/209 - 44s - loss: -6.1769e-01 - val_loss: -4.8819e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "209/209 - 43s - loss: -6.2174e-01 - val_loss: -5.0545e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "209/209 - 44s - loss: -6.3313e-01 - val_loss: -4.2488e-01 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "209/209 - 45s - loss: -6.3767e-01 - val_loss: -5.2431e-01 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "209/209 - 43s - loss: -6.3704e-01 - val_loss: -4.9921e-01 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "209/209 - 43s - loss: -6.4485e-01 - val_loss: -5.1007e-01 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "209/209 - 44s - loss: -6.3896e-01 - val_loss: -5.1787e-01 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "209/209 - 44s - loss: -6.4573e-01 - val_loss: -5.0790e-01 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "209/209 - 43s - loss: -6.4752e-01 - val_loss: -4.8360e-01 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "209/209 - 44s - loss: -6.4648e-01 - val_loss: -5.1477e-01 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "209/209 - 45s - loss: -6.4919e-01 - val_loss: -4.9601e-01 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "209/209 - 43s - loss: -6.4719e-01 - val_loss: -4.7956e-01 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "209/209 - 43s - loss: -6.5533e-01 - val_loss: -5.1151e-01 - lr: 2.5000e-04\n",
      "Epoch 30/50\n",
      "209/209 - 44s - loss: -6.5693e-01 - val_loss: -5.2582e-01 - lr: 2.5000e-04\n",
      "Epoch 31/50\n",
      "209/209 - 44s - loss: -6.5985e-01 - val_loss: -5.1634e-01 - lr: 2.5000e-04\n",
      "Epoch 32/50\n",
      "209/209 - 43s - loss: -6.5972e-01 - val_loss: -5.4205e-01 - lr: 2.5000e-04\n",
      "Epoch 33/50\n",
      "209/209 - 44s - loss: -6.5771e-01 - val_loss: -5.4973e-01 - lr: 2.5000e-04\n",
      "Epoch 34/50\n",
      "209/209 - 45s - loss: -6.6361e-01 - val_loss: -5.1265e-01 - lr: 2.5000e-04\n",
      "Epoch 35/50\n",
      "209/209 - 43s - loss: -6.6321e-01 - val_loss: -5.0790e-01 - lr: 2.5000e-04\n",
      "Epoch 36/50\n",
      "209/209 - 43s - loss: -6.6130e-01 - val_loss: -5.0081e-01 - lr: 2.5000e-04\n",
      "Epoch 37/50\n",
      "209/209 - 44s - loss: -6.6549e-01 - val_loss: -5.0216e-01 - lr: 2.5000e-04\n",
      "Epoch 38/50\n",
      "209/209 - 44s - loss: -6.6310e-01 - val_loss: -4.6439e-01 - lr: 2.5000e-04\n",
      "Epoch 39/50\n",
      "209/209 - 43s - loss: -6.6514e-01 - val_loss: -5.4576e-01 - lr: 2.5000e-04\n",
      "Epoch 40/50\n",
      "209/209 - 44s - loss: -6.6755e-01 - val_loss: -5.1147e-01 - lr: 2.5000e-04\n",
      "Epoch 41/50\n",
      "209/209 - 45s - loss: -6.6536e-01 - val_loss: -5.0709e-01 - lr: 2.5000e-04\n",
      "Epoch 42/50\n",
      "209/209 - 43s - loss: -6.6662e-01 - val_loss: -5.5604e-01 - lr: 2.5000e-04\n",
      "Epoch 43/50\n",
      "209/209 - 43s - loss: -6.6711e-01 - val_loss: -5.1185e-01 - lr: 2.5000e-04\n",
      "Epoch 44/50\n",
      "209/209 - 45s - loss: -6.6439e-01 - val_loss: -5.0960e-01 - lr: 2.5000e-04\n",
      "Epoch 45/50\n",
      "209/209 - 44s - loss: -6.7060e-01 - val_loss: -5.4750e-01 - lr: 2.5000e-04\n",
      "Epoch 46/50\n",
      "209/209 - 43s - loss: -6.6746e-01 - val_loss: -4.9671e-01 - lr: 2.5000e-04\n",
      "Epoch 47/50\n",
      "209/209 - 44s - loss: -6.6946e-01 - val_loss: -6.2978e-01 - lr: 2.5000e-04\n",
      "Epoch 48/50\n",
      "209/209 - 45s - loss: -6.6962e-01 - val_loss: -4.9409e-01 - lr: 2.5000e-04\n",
      "Epoch 49/50\n",
      "209/209 - 43s - loss: -6.7145e-01 - val_loss: -5.0347e-01 - lr: 2.5000e-04\n",
      "Epoch 50/50\n",
      "209/209 - 43s - loss: -6.7067e-01 - val_loss: -5.5777e-01 - lr: 2.5000e-04\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "Epoch 1/50\n",
      "415/415 - 87s - loss: -4.0832e-01 - val_loss: -3.5971e-01 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "415/415 - 86s - loss: -4.9738e-01 - val_loss: -4.4789e-01 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "415/415 - 87s - loss: -5.3710e-01 - val_loss: -4.1703e-01 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "415/415 - 87s - loss: -5.5538e-01 - val_loss: -4.5077e-01 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "415/415 - 86s - loss: -5.7738e-01 - val_loss: -4.6303e-01 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "415/415 - 87s - loss: -5.9334e-01 - val_loss: -3.9189e-01 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "415/415 - 85s - loss: -5.9795e-01 - val_loss: -5.3216e-01 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "415/415 - 87s - loss: -6.0809e-01 - val_loss: -4.4794e-01 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "415/415 - 86s - loss: -6.1458e-01 - val_loss: -4.7863e-01 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "415/415 - 87s - loss: -6.2289e-01 - val_loss: -4.5084e-01 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "415/415 - 87s - loss: -6.2418e-01 - val_loss: -5.5311e-01 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "415/415 - 86s - loss: -6.3204e-01 - val_loss: -5.2002e-01 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "415/415 - 87s - loss: -6.3584e-01 - val_loss: -5.4523e-01 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "415/415 - 85s - loss: -6.3215e-01 - val_loss: -4.8387e-01 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "415/415 - 87s - loss: -6.3811e-01 - val_loss: -4.7869e-01 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "415/415 - 86s - loss: -6.4238e-01 - val_loss: -5.6230e-01 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "415/415 - 87s - loss: -6.4293e-01 - val_loss: -4.8175e-01 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "415/415 - 87s - loss: -6.4630e-01 - val_loss: -5.2726e-01 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "415/415 - 86s - loss: -6.4949e-01 - val_loss: -5.1510e-01 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "415/415 - 88s - loss: -6.4763e-01 - val_loss: -3.7014e-01 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "415/415 - 85s - loss: -6.5489e-01 - val_loss: -4.6958e-01 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "415/415 - 87s - loss: -6.5894e-01 - val_loss: -5.3546e-01 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "415/415 - 86s - loss: -6.5638e-01 - val_loss: -4.7809e-01 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "415/415 - 86s - loss: -6.6147e-01 - val_loss: -5.2433e-01 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "415/415 - 87s - loss: -6.5982e-01 - val_loss: -4.3097e-01 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "415/415 - 86s - loss: -6.6166e-01 - val_loss: -5.7274e-01 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "415/415 - 88s - loss: -6.6055e-01 - val_loss: -4.9451e-01 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "415/415 - 85s - loss: -6.6776e-01 - val_loss: -5.1139e-01 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "415/415 - 87s - loss: -6.7174e-01 - val_loss: -5.1190e-01 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "415/415 - 86s - loss: -6.6833e-01 - val_loss: -5.2694e-01 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "415/415 - 87s - loss: -6.7170e-01 - val_loss: -5.3452e-01 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "415/415 - 86s - loss: -6.7315e-01 - val_loss: -5.5703e-01 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "415/415 - 86s - loss: -6.7170e-01 - val_loss: -4.9731e-01 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "415/415 - 87s - loss: -6.7587e-01 - val_loss: -5.0015e-01 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "415/415 - 85s - loss: -6.7591e-01 - val_loss: -4.9209e-01 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "415/415 - 87s - loss: -6.7561e-01 - val_loss: -5.4239e-01 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50\n",
      "415/415 - 86s - loss: -6.8796e-01 - val_loss: -5.5731e-01 - lr: 5.0000e-04\n",
      "Epoch 38/50\n",
      "415/415 - 87s - loss: -6.8987e-01 - val_loss: -5.4985e-01 - lr: 5.0000e-04\n",
      "Epoch 39/50\n",
      "415/415 - 86s - loss: -6.9166e-01 - val_loss: -5.2421e-01 - lr: 5.0000e-04\n",
      "Epoch 40/50\n",
      "415/415 - 86s - loss: -6.9007e-01 - val_loss: -5.4930e-01 - lr: 5.0000e-04\n",
      "Epoch 41/50\n",
      "415/415 - 87s - loss: -6.9125e-01 - val_loss: -5.7263e-01 - lr: 5.0000e-04\n",
      "Epoch 42/50\n",
      "415/415 - 86s - loss: -6.9269e-01 - val_loss: -5.2018e-01 - lr: 5.0000e-04\n",
      "Epoch 43/50\n",
      "415/415 - 87s - loss: -6.9173e-01 - val_loss: -5.1996e-01 - lr: 5.0000e-04\n",
      "Epoch 44/50\n",
      "415/415 - 86s - loss: -6.9420e-01 - val_loss: -6.0017e-01 - lr: 5.0000e-04\n",
      "Epoch 45/50\n",
      "415/415 - 87s - loss: -6.9488e-01 - val_loss: -4.6779e-01 - lr: 5.0000e-04\n",
      "Epoch 46/50\n",
      "415/415 - 86s - loss: -6.9762e-01 - val_loss: -5.4163e-01 - lr: 5.0000e-04\n",
      "Epoch 47/50\n",
      "415/415 - 86s - loss: -6.9303e-01 - val_loss: -5.6707e-01 - lr: 5.0000e-04\n",
      "Epoch 48/50\n",
      "415/415 - 87s - loss: -6.9398e-01 - val_loss: -5.3723e-01 - lr: 5.0000e-04\n",
      "Epoch 49/50\n",
      "415/415 - 86s - loss: -6.9770e-01 - val_loss: -5.0401e-01 - lr: 5.0000e-04\n",
      "Epoch 50/50\n",
      "415/415 - 87s - loss: -6.9900e-01 - val_loss: -5.4154e-01 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "\n",
    "    X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=overlap_array[i]) \n",
    "    X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "    \n",
    "    X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                        patch_shape=config[\"patch_size\"],\n",
    "                                        overlap=0)\n",
    "    X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                    [X_train, y_train],\n",
    "                                                    batch_size=5,\n",
    "                                                    augment=True,\n",
    "                                                    augment_flip=True)\n",
    "\n",
    "\n",
    "    validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                    [X_valid, y_valid],\n",
    "                                                    batch_size=2,\n",
    "                                                    augment=False,\n",
    "                                                    augment_flip=False)\n",
    "\n",
    "    init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "    long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "    long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "    long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "    long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "    long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "    long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "    long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "    long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "    long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "    for layer in long_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "    K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "    #fit the long model\n",
    "    history = train_model(model=long_model,\n",
    "            path2save=config[\"path2save_transferlearning\"],\n",
    "            model_name='_'.join([\"second_overlap_test\",str(i)]),\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=nb_train_steps,\n",
    "            validation_steps=nb_valid_steps,\n",
    "            n_epochs=50,\n",
    "            learning_rate_drop=0.5,\n",
    "            learning_rate_patience=10\n",
    "           )\n",
    "\n",
    "    val_loss_overlap[i,:] = history.history['val_loss']\n",
    "    loss_overlap[i,:] = history.history['loss']\n",
    "    \n",
    "np.save(main_dir+\"parameters_analysis/second_overlap.npy\",val_loss_overlap)\n",
    "np.save(main_dir+\"parameters_analysis/second_overlap_train.npy\",loss_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_overlap = np.load(main_dir+\"parameters_analysis/second_overlap.npy\")\n",
    "epochs = range(5,51,5)\n",
    "mean_val_loss = np.zeros((3,10))\n",
    "for i in range(10):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss_overlap[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss_overlap[1,i:i+5]))\n",
    "    mean_val_loss[2,i]=np.abs(np.mean(val_loss_overlap[2,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'overlap of 12')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'overlap of 6')\n",
    "plt.plot(epochs,mean_val_loss[2,:],'o--',label = 'overlap of 3')\n",
    "\n",
    "plt.title('Evolution of the validation loss with different overlap sizes ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFER LEARNING + UNFREEZING VS DIRECT RETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.zeros((2,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-5/t2/iv-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-2/t2/iv-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-9/t2/pm-9_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-4/t2/pm-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-11/t2/pm-11_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-1/t2/iv-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-10/t2/pm-10_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-13/t2/pm-13_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-14/t2/pm-14_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-4/t2/iv-4_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-6/t2/pm-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-5/t2/pm-5_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-12/t2/pm-12_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-7/t2/iv-7_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-2/t2/pm-2_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/pm-1/t2/pm-1_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-6/t2/iv-6_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-3/t2/iv-3_t2_norm.nii.gz\n",
      "/home/nidebroux/data_mask/preprocess/Data_organized/iv-9/t2/iv-9_t2_norm.nii.gz\n",
      "Epoch 1/75\n",
      "193/193 - 40s - loss: -3.6175e-01 - val_loss: -4.6226e-01 - lr: 0.0010\n",
      "Epoch 2/75\n",
      "193/193 - 41s - loss: -4.4191e-01 - val_loss: -4.8830e-01 - lr: 0.0010\n",
      "Epoch 3/75\n",
      "193/193 - 41s - loss: -4.6433e-01 - val_loss: -5.5733e-01 - lr: 0.0010\n",
      "Epoch 4/75\n",
      "193/193 - 40s - loss: -4.9312e-01 - val_loss: -4.8457e-01 - lr: 0.0010\n",
      "Epoch 5/75\n",
      "193/193 - 40s - loss: -5.0622e-01 - val_loss: -5.7788e-01 - lr: 0.0010\n",
      "Epoch 6/75\n",
      "193/193 - 42s - loss: -4.8911e-01 - val_loss: -4.8919e-01 - lr: 0.0010\n",
      "Epoch 7/75\n",
      "193/193 - 41s - loss: -5.2082e-01 - val_loss: -5.7317e-01 - lr: 0.0010\n",
      "Epoch 8/75\n",
      "193/193 - 40s - loss: -5.3826e-01 - val_loss: -5.7367e-01 - lr: 0.0010\n",
      "Epoch 9/75\n",
      "193/193 - 41s - loss: -5.1875e-01 - val_loss: -6.1358e-01 - lr: 0.0010\n",
      "Epoch 10/75\n",
      "193/193 - 42s - loss: -5.4199e-01 - val_loss: -5.3770e-01 - lr: 0.0010\n",
      "Epoch 11/75\n",
      "193/193 - 40s - loss: -5.5397e-01 - val_loss: -5.5875e-01 - lr: 0.0010\n",
      "Epoch 12/75\n",
      "193/193 - 40s - loss: -5.5983e-01 - val_loss: -5.9540e-01 - lr: 0.0010\n",
      "Epoch 13/75\n",
      "193/193 - 41s - loss: -5.6076e-01 - val_loss: -5.6007e-01 - lr: 0.0010\n",
      "Epoch 14/75\n",
      "193/193 - 41s - loss: -5.6492e-01 - val_loss: -5.6052e-01 - lr: 0.0010\n",
      "Epoch 15/75\n",
      "193/193 - 40s - loss: -5.6929e-01 - val_loss: -5.2392e-01 - lr: 0.0010\n",
      "Epoch 16/75\n",
      "193/193 - 40s - loss: -5.7724e-01 - val_loss: -6.6842e-01 - lr: 0.0010\n",
      "Epoch 17/75\n",
      "193/193 - 41s - loss: -5.7472e-01 - val_loss: -5.8669e-01 - lr: 0.0010\n",
      "Epoch 18/75\n",
      "193/193 - 41s - loss: -5.8060e-01 - val_loss: -6.1037e-01 - lr: 0.0010\n",
      "Epoch 19/75\n",
      "193/193 - 40s - loss: -5.8546e-01 - val_loss: -5.9813e-01 - lr: 0.0010\n",
      "Epoch 20/75\n",
      "193/193 - 40s - loss: -5.8863e-01 - val_loss: -6.0394e-01 - lr: 0.0010\n",
      "Epoch 21/75\n",
      "193/193 - 42s - loss: -5.9026e-01 - val_loss: -4.6746e-01 - lr: 0.0010\n",
      "Epoch 22/75\n",
      "193/193 - 41s - loss: -5.9352e-01 - val_loss: -6.3150e-01 - lr: 0.0010\n",
      "Epoch 23/75\n",
      "193/193 - 40s - loss: -5.9036e-01 - val_loss: -6.4000e-01 - lr: 0.0010\n",
      "Epoch 24/75\n",
      "193/193 - 40s - loss: -5.9923e-01 - val_loss: -5.8615e-01 - lr: 0.0010\n",
      "Epoch 25/75\n",
      "193/193 - 42s - loss: -5.9546e-01 - val_loss: -5.7193e-01 - lr: 0.0010\n",
      "Epoch 26/75\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "193/193 - 41s - loss: -5.9714e-01 - val_loss: -6.0215e-01 - lr: 0.0010\n",
      "Epoch 27/75\n",
      "193/193 - 40s - loss: -6.1068e-01 - val_loss: -6.4822e-01 - lr: 5.0000e-04\n",
      "Epoch 28/75\n",
      "193/193 - 41s - loss: -6.1824e-01 - val_loss: -6.2661e-01 - lr: 5.0000e-04\n",
      "Epoch 29/75\n",
      "193/193 - 42s - loss: -6.1588e-01 - val_loss: -6.5846e-01 - lr: 5.0000e-04\n",
      "Epoch 30/75\n",
      "193/193 - 40s - loss: -6.1577e-01 - val_loss: -6.2643e-01 - lr: 5.0000e-04\n",
      "Epoch 31/75\n",
      "193/193 - 40s - loss: -6.1963e-01 - val_loss: -6.4468e-01 - lr: 5.0000e-04\n",
      "Epoch 32/75\n",
      "193/193 - 41s - loss: -6.1968e-01 - val_loss: -6.7290e-01 - lr: 5.0000e-04\n",
      "Epoch 33/75\n",
      "193/193 - 41s - loss: -6.1904e-01 - val_loss: -5.9417e-01 - lr: 5.0000e-04\n",
      "Epoch 34/75\n",
      "193/193 - 40s - loss: -6.2114e-01 - val_loss: -6.4222e-01 - lr: 5.0000e-04\n",
      "Epoch 35/75\n",
      "193/193 - 40s - loss: -6.1757e-01 - val_loss: -6.6994e-01 - lr: 5.0000e-04\n",
      "Epoch 36/75\n",
      "193/193 - 41s - loss: -6.2643e-01 - val_loss: -6.2771e-01 - lr: 5.0000e-04\n",
      "Epoch 37/75\n",
      "193/193 - 41s - loss: -6.2557e-01 - val_loss: -6.8154e-01 - lr: 5.0000e-04\n",
      "Epoch 38/75\n",
      "193/193 - 40s - loss: -6.2499e-01 - val_loss: -6.5677e-01 - lr: 5.0000e-04\n",
      "Epoch 39/75\n",
      "193/193 - 40s - loss: -6.2646e-01 - val_loss: -6.0414e-01 - lr: 5.0000e-04\n",
      "Epoch 40/75\n",
      "193/193 - 42s - loss: -6.2721e-01 - val_loss: -6.5872e-01 - lr: 5.0000e-04\n",
      "Epoch 41/75\n",
      "193/193 - 41s - loss: -6.2882e-01 - val_loss: -6.2089e-01 - lr: 5.0000e-04\n",
      "Epoch 42/75\n",
      "193/193 - 40s - loss: -6.2711e-01 - val_loss: -6.1911e-01 - lr: 5.0000e-04\n",
      "Epoch 43/75\n",
      "193/193 - 41s - loss: -6.3234e-01 - val_loss: -6.6155e-01 - lr: 5.0000e-04\n",
      "Epoch 44/75\n",
      "193/193 - 42s - loss: -6.2824e-01 - val_loss: -6.5212e-01 - lr: 5.0000e-04\n",
      "Epoch 45/75\n",
      "193/193 - 40s - loss: -6.2948e-01 - val_loss: -6.4240e-01 - lr: 5.0000e-04\n",
      "Epoch 46/75\n",
      "193/193 - 40s - loss: -6.2648e-01 - val_loss: -6.2545e-01 - lr: 5.0000e-04\n",
      "Epoch 47/75\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "193/193 - 41s - loss: -6.2913e-01 - val_loss: -6.4311e-01 - lr: 5.0000e-04\n",
      "Epoch 48/75\n",
      "193/193 - 41s - loss: -6.3703e-01 - val_loss: -6.4277e-01 - lr: 2.5000e-04\n",
      "Epoch 49/75\n",
      "193/193 - 40s - loss: -6.3942e-01 - val_loss: -6.2680e-01 - lr: 2.5000e-04\n",
      "Epoch 50/75\n",
      "193/193 - 40s - loss: -6.4059e-01 - val_loss: -6.7383e-01 - lr: 2.5000e-04\n",
      "Epoch 51/75\n",
      "193/193 - 42s - loss: -6.4448e-01 - val_loss: -6.8501e-01 - lr: 2.5000e-04\n",
      "Epoch 52/75\n",
      "193/193 - 41s - loss: -6.4177e-01 - val_loss: -6.4015e-01 - lr: 2.5000e-04\n",
      "Epoch 53/75\n",
      "193/193 - 40s - loss: -6.4354e-01 - val_loss: -6.6599e-01 - lr: 2.5000e-04\n",
      "Epoch 54/75\n",
      "193/193 - 41s - loss: -6.4268e-01 - val_loss: -6.8623e-01 - lr: 2.5000e-04\n",
      "Epoch 55/75\n",
      "193/193 - 42s - loss: -6.4691e-01 - val_loss: -5.9978e-01 - lr: 2.5000e-04\n",
      "Epoch 56/75\n",
      "193/193 - 40s - loss: -6.4274e-01 - val_loss: -6.5624e-01 - lr: 2.5000e-04\n",
      "Epoch 57/75\n",
      "193/193 - 40s - loss: -6.4421e-01 - val_loss: -6.9230e-01 - lr: 2.5000e-04\n",
      "Epoch 58/75\n",
      "193/193 - 41s - loss: -6.4718e-01 - val_loss: -6.5349e-01 - lr: 2.5000e-04\n",
      "Epoch 59/75\n",
      "193/193 - 41s - loss: -6.4800e-01 - val_loss: -5.9589e-01 - lr: 2.5000e-04\n",
      "Epoch 60/75\n",
      "193/193 - 40s - loss: -6.4790e-01 - val_loss: -7.0944e-01 - lr: 2.5000e-04\n",
      "Epoch 61/75\n",
      "193/193 - 40s - loss: -6.4734e-01 - val_loss: -6.0752e-01 - lr: 2.5000e-04\n",
      "Epoch 62/75\n",
      "193/193 - 41s - loss: -6.4780e-01 - val_loss: -6.9671e-01 - lr: 2.5000e-04\n",
      "Epoch 63/75\n",
      "193/193 - 41s - loss: -6.4618e-01 - val_loss: -6.7685e-01 - lr: 2.5000e-04\n",
      "Epoch 64/75\n",
      "193/193 - 40s - loss: -6.5041e-01 - val_loss: -6.6269e-01 - lr: 2.5000e-04\n",
      "Epoch 65/75\n",
      "193/193 - 40s - loss: -6.4975e-01 - val_loss: -5.8403e-01 - lr: 2.5000e-04\n",
      "Epoch 66/75\n",
      "193/193 - 42s - loss: -6.4794e-01 - val_loss: -7.1188e-01 - lr: 2.5000e-04\n",
      "Epoch 67/75\n",
      "193/193 - 40s - loss: -6.5027e-01 - val_loss: -6.3954e-01 - lr: 2.5000e-04\n",
      "Epoch 68/75\n",
      "193/193 - 40s - loss: -6.5019e-01 - val_loss: -6.2561e-01 - lr: 2.5000e-04\n",
      "Epoch 69/75\n",
      "193/193 - 41s - loss: -6.5026e-01 - val_loss: -6.6257e-01 - lr: 2.5000e-04\n",
      "Epoch 70/75\n",
      "193/193 - 42s - loss: -6.4619e-01 - val_loss: -6.6648e-01 - lr: 2.5000e-04\n",
      "Epoch 71/75\n",
      "193/193 - 40s - loss: -6.5406e-01 - val_loss: -6.4723e-01 - lr: 2.5000e-04\n",
      "Epoch 72/75\n",
      "193/193 - 40s - loss: -6.5638e-01 - val_loss: -6.6473e-01 - lr: 2.5000e-04\n",
      "Epoch 73/75\n",
      "193/193 - 41s - loss: -6.5140e-01 - val_loss: -6.7131e-01 - lr: 2.5000e-04\n",
      "Epoch 74/75\n",
      "193/193 - 41s - loss: -6.5145e-01 - val_loss: -6.6812e-01 - lr: 2.5000e-04\n",
      "Epoch 75/75\n",
      "193/193 - 40s - loss: -6.5282e-01 - val_loss: -6.6717e-01 - lr: 2.5000e-04\n",
      "Epoch 1/75\n",
      "193/193 - 72s - loss: -4.1683e-01 - val_loss: -5.6117e-01 - lr: 1.0000e-05\n",
      "Epoch 2/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 - 71s - loss: -5.7586e-01 - val_loss: -6.4923e-01 - lr: 1.0000e-05\n",
      "Epoch 3/75\n",
      "193/193 - 71s - loss: -5.9859e-01 - val_loss: -6.1035e-01 - lr: 1.0000e-05\n",
      "Epoch 4/75\n",
      "193/193 - 71s - loss: -6.1295e-01 - val_loss: -6.2678e-01 - lr: 1.0000e-05\n",
      "Epoch 5/75\n",
      "193/193 - 71s - loss: -6.2253e-01 - val_loss: -6.5637e-01 - lr: 1.0000e-05\n",
      "Epoch 6/75\n",
      "193/193 - 71s - loss: -6.3183e-01 - val_loss: -7.0310e-01 - lr: 1.0000e-05\n",
      "Epoch 7/75\n",
      "193/193 - 70s - loss: -6.3241e-01 - val_loss: -7.1644e-01 - lr: 1.0000e-05\n",
      "Epoch 8/75\n",
      "193/193 - 72s - loss: -6.3926e-01 - val_loss: -6.7570e-01 - lr: 1.0000e-05\n",
      "Epoch 9/75\n",
      "193/193 - 70s - loss: -6.4339e-01 - val_loss: -7.3999e-01 - lr: 1.0000e-05\n",
      "Epoch 10/75\n",
      "193/193 - 72s - loss: -6.4496e-01 - val_loss: -6.8340e-01 - lr: 1.0000e-05\n",
      "Epoch 11/75\n",
      "193/193 - 70s - loss: -6.4989e-01 - val_loss: -7.0203e-01 - lr: 1.0000e-05\n",
      "Epoch 12/75\n",
      "193/193 - 72s - loss: -6.5405e-01 - val_loss: -7.3660e-01 - lr: 1.0000e-05\n",
      "Epoch 13/75\n",
      "193/193 - 71s - loss: -6.5656e-01 - val_loss: -7.0362e-01 - lr: 1.0000e-05\n",
      "Epoch 14/75\n",
      "193/193 - 71s - loss: -6.5475e-01 - val_loss: -6.8412e-01 - lr: 1.0000e-05\n",
      "Epoch 15/75\n",
      "193/193 - 71s - loss: -6.5779e-01 - val_loss: -6.8389e-01 - lr: 1.0000e-05\n",
      "Epoch 16/75\n",
      "193/193 - 71s - loss: -6.5986e-01 - val_loss: -7.2433e-01 - lr: 1.0000e-05\n",
      "Epoch 17/75\n",
      "193/193 - 71s - loss: -6.6741e-01 - val_loss: -6.8549e-01 - lr: 1.0000e-05\n",
      "Epoch 18/75\n",
      "193/193 - 71s - loss: -6.6427e-01 - val_loss: -7.0741e-01 - lr: 1.0000e-05\n",
      "Epoch 19/75\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "193/193 - 71s - loss: -6.6585e-01 - val_loss: -6.8816e-01 - lr: 1.0000e-05\n",
      "Epoch 20/75\n",
      "193/193 - 71s - loss: -6.7107e-01 - val_loss: -7.2332e-01 - lr: 5.0000e-06\n",
      "Epoch 21/75\n",
      "193/193 - 71s - loss: -6.7003e-01 - val_loss: -6.7929e-01 - lr: 5.0000e-06\n",
      "Epoch 22/75\n",
      "193/193 - 71s - loss: -6.7189e-01 - val_loss: -7.0508e-01 - lr: 5.0000e-06\n",
      "Epoch 23/75\n",
      "193/193 - 71s - loss: -6.7297e-01 - val_loss: -7.2075e-01 - lr: 5.0000e-06\n",
      "Epoch 24/75\n",
      "193/193 - 70s - loss: -6.7434e-01 - val_loss: -6.7589e-01 - lr: 5.0000e-06\n",
      "Epoch 25/75\n",
      "193/193 - 72s - loss: -6.7568e-01 - val_loss: -7.3120e-01 - lr: 5.0000e-06\n",
      "Epoch 26/75\n",
      "193/193 - 70s - loss: -6.7554e-01 - val_loss: -6.7525e-01 - lr: 5.0000e-06\n",
      "Epoch 27/75\n",
      "193/193 - 72s - loss: -6.7516e-01 - val_loss: -7.5446e-01 - lr: 5.0000e-06\n",
      "Epoch 28/75\n",
      "193/193 - 70s - loss: -6.7656e-01 - val_loss: -7.4927e-01 - lr: 5.0000e-06\n",
      "Epoch 29/75\n",
      "193/193 - 71s - loss: -6.7594e-01 - val_loss: -6.9965e-01 - lr: 5.0000e-06\n",
      "Epoch 30/75\n",
      "193/193 - 71s - loss: -6.7956e-01 - val_loss: -6.7445e-01 - lr: 5.0000e-06\n",
      "Epoch 31/75\n",
      "193/193 - 71s - loss: -6.8125e-01 - val_loss: -7.5492e-01 - lr: 5.0000e-06\n",
      "Epoch 32/75\n",
      "193/193 - 71s - loss: -6.8054e-01 - val_loss: -6.7058e-01 - lr: 5.0000e-06\n",
      "Epoch 33/75\n",
      "193/193 - 71s - loss: -6.8029e-01 - val_loss: -7.4547e-01 - lr: 5.0000e-06\n",
      "Epoch 34/75\n",
      "193/193 - 71s - loss: -6.8160e-01 - val_loss: -6.9958e-01 - lr: 5.0000e-06\n",
      "Epoch 35/75\n",
      "193/193 - 71s - loss: -6.8212e-01 - val_loss: -7.4852e-01 - lr: 5.0000e-06\n",
      "Epoch 36/75\n",
      "193/193 - 71s - loss: -6.8295e-01 - val_loss: -6.9575e-01 - lr: 5.0000e-06\n",
      "Epoch 37/75\n",
      "193/193 - 71s - loss: -6.8253e-01 - val_loss: -6.9123e-01 - lr: 5.0000e-06\n",
      "Epoch 38/75\n",
      "193/193 - 72s - loss: -6.8485e-01 - val_loss: -6.9862e-01 - lr: 5.0000e-06\n",
      "Epoch 39/75\n",
      "193/193 - 70s - loss: -6.8378e-01 - val_loss: -6.9652e-01 - lr: 5.0000e-06\n",
      "Epoch 40/75\n",
      "193/193 - 72s - loss: -6.8512e-01 - val_loss: -7.2658e-01 - lr: 5.0000e-06\n",
      "Epoch 41/75\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "193/193 - 70s - loss: -6.8524e-01 - val_loss: -7.1675e-01 - lr: 5.0000e-06\n",
      "Epoch 42/75\n",
      "193/193 - 72s - loss: -6.8377e-01 - val_loss: -7.1656e-01 - lr: 2.5000e-06\n",
      "Epoch 43/75\n",
      "193/193 - 70s - loss: -6.8646e-01 - val_loss: -6.9323e-01 - lr: 2.5000e-06\n",
      "Epoch 44/75\n",
      "193/193 - 72s - loss: -6.8556e-01 - val_loss: -7.2448e-01 - lr: 2.5000e-06\n",
      "Epoch 45/75\n",
      "193/193 - 70s - loss: -6.8741e-01 - val_loss: -6.8072e-01 - lr: 2.5000e-06\n",
      "Epoch 46/75\n",
      "193/193 - 71s - loss: -6.8666e-01 - val_loss: -7.2793e-01 - lr: 2.5000e-06\n",
      "Epoch 47/75\n",
      "193/193 - 71s - loss: -6.8879e-01 - val_loss: -7.1054e-01 - lr: 2.5000e-06\n",
      "Epoch 48/75\n",
      "193/193 - 71s - loss: -6.8457e-01 - val_loss: -7.0451e-01 - lr: 2.5000e-06\n",
      "Epoch 49/75\n",
      "193/193 - 71s - loss: -6.8881e-01 - val_loss: -7.1245e-01 - lr: 2.5000e-06\n",
      "Epoch 50/75\n",
      "193/193 - 71s - loss: -6.8899e-01 - val_loss: -7.2549e-01 - lr: 2.5000e-06\n",
      "Epoch 51/75\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "193/193 - 71s - loss: -6.9014e-01 - val_loss: -7.1163e-01 - lr: 2.5000e-06\n",
      "Epoch 52/75\n",
      "193/193 - 71s - loss: -6.8872e-01 - val_loss: -6.9164e-01 - lr: 1.2500e-06\n",
      "Epoch 53/75\n",
      "193/193 - 71s - loss: -6.8670e-01 - val_loss: -6.9947e-01 - lr: 1.2500e-06\n",
      "Epoch 54/75\n",
      "193/193 - 71s - loss: -6.9167e-01 - val_loss: -7.4721e-01 - lr: 1.2500e-06\n",
      "Epoch 55/75\n",
      "193/193 - 71s - loss: -6.8903e-01 - val_loss: -7.1373e-01 - lr: 1.2500e-06\n",
      "Epoch 56/75\n",
      "193/193 - 71s - loss: -6.8891e-01 - val_loss: -6.9697e-01 - lr: 1.2500e-06\n",
      "Epoch 57/75\n",
      "193/193 - 71s - loss: -6.9216e-01 - val_loss: -7.1052e-01 - lr: 1.2500e-06\n",
      "Epoch 58/75\n",
      "193/193 - 70s - loss: -6.8709e-01 - val_loss: -7.1912e-01 - lr: 1.2500e-06\n",
      "Epoch 59/75\n",
      "193/193 - 72s - loss: -6.9167e-01 - val_loss: -7.2694e-01 - lr: 1.2500e-06\n",
      "Epoch 60/75\n",
      "193/193 - 70s - loss: -6.8834e-01 - val_loss: -7.3343e-01 - lr: 1.2500e-06\n",
      "Epoch 61/75\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "193/193 - 72s - loss: -6.9160e-01 - val_loss: -6.8546e-01 - lr: 1.2500e-06\n",
      "Epoch 62/75\n",
      "193/193 - 70s - loss: -6.9103e-01 - val_loss: -6.6856e-01 - lr: 6.2500e-07\n",
      "Epoch 63/75\n",
      "193/193 - 72s - loss: -6.9057e-01 - val_loss: -6.9584e-01 - lr: 6.2500e-07\n",
      "Epoch 64/75\n",
      "193/193 - 70s - loss: -6.8905e-01 - val_loss: -7.1186e-01 - lr: 6.2500e-07\n",
      "Epoch 65/75\n",
      "193/193 - 71s - loss: -6.8895e-01 - val_loss: -7.3087e-01 - lr: 6.2500e-07\n",
      "Epoch 66/75\n",
      "193/193 - 71s - loss: -6.9009e-01 - val_loss: -7.0930e-01 - lr: 6.2500e-07\n",
      "Epoch 67/75\n",
      "193/193 - 71s - loss: -6.8953e-01 - val_loss: -6.8488e-01 - lr: 6.2500e-07\n",
      "Epoch 68/75\n",
      "193/193 - 71s - loss: -6.8986e-01 - val_loss: -7.1203e-01 - lr: 6.2500e-07\n",
      "Epoch 69/75\n",
      "193/193 - 71s - loss: -6.8848e-01 - val_loss: -7.1101e-01 - lr: 6.2500e-07\n",
      "Epoch 70/75\n",
      "193/193 - 71s - loss: -6.9282e-01 - val_loss: -7.2076e-01 - lr: 6.2500e-07\n",
      "Epoch 71/75\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "193/193 - 71s - loss: -6.9561e-01 - val_loss: -7.1754e-01 - lr: 6.2500e-07\n",
      "Epoch 72/75\n",
      "193/193 - 71s - loss: -6.9258e-01 - val_loss: -7.4948e-01 - lr: 3.1250e-07\n",
      "Epoch 73/75\n",
      "193/193 - 71s - loss: -6.9277e-01 - val_loss: -6.7751e-01 - lr: 3.1250e-07\n",
      "Epoch 74/75\n",
      "193/193 - 71s - loss: -6.8933e-01 - val_loss: -7.2764e-01 - lr: 3.1250e-07\n",
      "Epoch 75/75\n",
      "193/193 - 71s - loss: -6.9299e-01 - val_loss: -6.5360e-01 - lr: 3.1250e-07\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 64, 64, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 24, 64, 64, 4 672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 24, 64, 64, 4 96          conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 24, 64, 64, 4 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 48, 64, 64, 4 31152       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 48, 64, 64, 4 192         conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 48, 32, 32, 2 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 48, 32, 32, 2 62256       max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 48, 32, 32, 2 192         conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 48, 32, 32, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 96, 32, 32, 2 124512      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 32, 32, 2 384         conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 96, 32, 32, 2 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3D)  (None, 96, 64, 64, 4 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 144, 64, 64,  0           up_sampling3d_1[0][0]            \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 48, 64, 64, 4 186672      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 48, 64, 64, 4 192         conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 48, 64, 64, 4 62256       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 48, 64, 64, 4 192         conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 48, 64, 64, 4 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 1, 64, 64, 48 49          activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 64, 64, 48 0           conv3d_7[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 468,817\n",
      "Trainable params: 468,193\n",
      "Non-trainable params: 624\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "193/193 - 72s - loss: -5.0399e-01 - val_loss: -6.1022e-01 - lr: 1.0000e-05\n",
      "Epoch 2/150\n",
      "193/193 - 71s - loss: -5.8128e-01 - val_loss: -6.9070e-01 - lr: 1.0000e-05\n",
      "Epoch 3/150\n",
      "193/193 - 72s - loss: -6.0237e-01 - val_loss: -6.1512e-01 - lr: 1.0000e-05\n",
      "Epoch 4/150\n",
      "193/193 - 71s - loss: -6.1343e-01 - val_loss: -6.7807e-01 - lr: 1.0000e-05\n",
      "Epoch 5/150\n",
      "193/193 - 72s - loss: -6.2462e-01 - val_loss: -6.6328e-01 - lr: 1.0000e-05\n",
      "Epoch 6/150\n",
      "193/193 - 71s - loss: -6.2674e-01 - val_loss: -7.0521e-01 - lr: 1.0000e-05\n",
      "Epoch 7/150\n",
      "193/193 - 72s - loss: -6.3341e-01 - val_loss: -7.1542e-01 - lr: 1.0000e-05\n",
      "Epoch 8/150\n",
      "193/193 - 71s - loss: -6.4060e-01 - val_loss: -6.6089e-01 - lr: 1.0000e-05\n",
      "Epoch 9/150\n",
      "193/193 - 72s - loss: -6.4646e-01 - val_loss: -6.5774e-01 - lr: 1.0000e-05\n",
      "Epoch 10/150\n",
      "193/193 - 71s - loss: -6.5158e-01 - val_loss: -6.9479e-01 - lr: 1.0000e-05\n",
      "Epoch 11/150\n",
      "193/193 - 72s - loss: -6.5357e-01 - val_loss: -7.0622e-01 - lr: 1.0000e-05\n",
      "Epoch 12/150\n",
      "193/193 - 71s - loss: -6.5761e-01 - val_loss: -7.2502e-01 - lr: 1.0000e-05\n",
      "Epoch 13/150\n",
      "193/193 - 72s - loss: -6.6104e-01 - val_loss: -7.3141e-01 - lr: 1.0000e-05\n",
      "Epoch 14/150\n",
      "193/193 - 71s - loss: -6.6271e-01 - val_loss: -6.8946e-01 - lr: 1.0000e-05\n",
      "Epoch 15/150\n",
      "193/193 - 71s - loss: -6.6732e-01 - val_loss: -7.4133e-01 - lr: 1.0000e-05\n",
      "Epoch 16/150\n",
      "193/193 - 71s - loss: -6.7021e-01 - val_loss: -6.8994e-01 - lr: 1.0000e-05\n",
      "Epoch 17/150\n",
      "193/193 - 71s - loss: -6.7060e-01 - val_loss: -6.5430e-01 - lr: 1.0000e-05\n",
      "Epoch 18/150\n",
      "193/193 - 71s - loss: -6.7143e-01 - val_loss: -7.8566e-01 - lr: 1.0000e-05\n",
      "Epoch 19/150\n",
      "193/193 - 71s - loss: -6.7780e-01 - val_loss: -7.2933e-01 - lr: 1.0000e-05\n",
      "Epoch 20/150\n",
      "193/193 - 71s - loss: -6.7893e-01 - val_loss: -7.1867e-01 - lr: 1.0000e-05\n",
      "Epoch 21/150\n",
      "193/193 - 71s - loss: -6.8092e-01 - val_loss: -7.5143e-01 - lr: 1.0000e-05\n",
      "Epoch 22/150\n",
      "193/193 - 71s - loss: -6.8177e-01 - val_loss: -7.1963e-01 - lr: 1.0000e-05\n",
      "Epoch 23/150\n",
      "193/193 - 71s - loss: -6.8555e-01 - val_loss: -6.6409e-01 - lr: 1.0000e-05\n",
      "Epoch 24/150\n",
      "193/193 - 72s - loss: -6.8596e-01 - val_loss: -7.4426e-01 - lr: 1.0000e-05\n",
      "Epoch 25/150\n",
      "193/193 - 71s - loss: -6.8910e-01 - val_loss: -7.0379e-01 - lr: 1.0000e-05\n",
      "Epoch 26/150\n",
      "193/193 - 72s - loss: -6.8683e-01 - val_loss: -7.1694e-01 - lr: 1.0000e-05\n",
      "Epoch 27/150\n",
      "193/193 - 71s - loss: -6.9153e-01 - val_loss: -7.0051e-01 - lr: 1.0000e-05\n",
      "Epoch 28/150\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "193/193 - 72s - loss: -6.9293e-01 - val_loss: -7.6796e-01 - lr: 1.0000e-05\n",
      "Epoch 29/150\n",
      "193/193 - 71s - loss: -6.9262e-01 - val_loss: -6.9308e-01 - lr: 5.0000e-06\n",
      "Epoch 30/150\n",
      "193/193 - 72s - loss: -6.9080e-01 - val_loss: -7.1989e-01 - lr: 5.0000e-06\n",
      "Epoch 31/150\n",
      "193/193 - 71s - loss: -6.9787e-01 - val_loss: -7.3996e-01 - lr: 5.0000e-06\n",
      "Epoch 32/150\n",
      "193/193 - 71s - loss: -6.9865e-01 - val_loss: -7.3575e-01 - lr: 5.0000e-06\n",
      "Epoch 33/150\n",
      "193/193 - 71s - loss: -6.9393e-01 - val_loss: -7.2489e-01 - lr: 5.0000e-06\n",
      "Epoch 34/150\n",
      "193/193 - 71s - loss: -6.9691e-01 - val_loss: -7.3347e-01 - lr: 5.0000e-06\n",
      "Epoch 35/150\n",
      "193/193 - 71s - loss: -6.9623e-01 - val_loss: -7.3071e-01 - lr: 5.0000e-06\n",
      "Epoch 36/150\n",
      "193/193 - 71s - loss: -6.9765e-01 - val_loss: -6.8481e-01 - lr: 5.0000e-06\n",
      "Epoch 37/150\n",
      "193/193 - 71s - loss: -6.9851e-01 - val_loss: -7.1854e-01 - lr: 5.0000e-06\n",
      "Epoch 38/150\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
      "193/193 - 71s - loss: -6.9740e-01 - val_loss: -7.3043e-01 - lr: 5.0000e-06\n",
      "Epoch 39/150\n",
      "193/193 - 72s - loss: -7.0044e-01 - val_loss: -7.0637e-01 - lr: 2.5000e-06\n",
      "Epoch 40/150\n",
      "193/193 - 71s - loss: -7.0011e-01 - val_loss: -7.5391e-01 - lr: 2.5000e-06\n",
      "Epoch 41/150\n",
      "193/193 - 72s - loss: -7.0016e-01 - val_loss: -7.3245e-01 - lr: 2.5000e-06\n",
      "Epoch 42/150\n",
      "193/193 - 71s - loss: -6.9876e-01 - val_loss: -7.1537e-01 - lr: 2.5000e-06\n",
      "Epoch 43/150\n",
      "193/193 - 72s - loss: -7.0006e-01 - val_loss: -6.9382e-01 - lr: 2.5000e-06\n",
      "Epoch 44/150\n",
      "193/193 - 71s - loss: -7.0243e-01 - val_loss: -7.1375e-01 - lr: 2.5000e-06\n",
      "Epoch 45/150\n",
      "193/193 - 72s - loss: -7.0002e-01 - val_loss: -7.3723e-01 - lr: 2.5000e-06\n",
      "Epoch 46/150\n",
      "193/193 - 71s - loss: -7.0044e-01 - val_loss: -7.7375e-01 - lr: 2.5000e-06\n",
      "Epoch 47/150\n",
      "193/193 - 72s - loss: -7.0367e-01 - val_loss: -6.6905e-01 - lr: 2.5000e-06\n",
      "Epoch 48/150\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-06.\n",
      "193/193 - 71s - loss: -6.9943e-01 - val_loss: -7.4489e-01 - lr: 2.5000e-06\n",
      "Epoch 49/150\n",
      "193/193 - 72s - loss: -7.0377e-01 - val_loss: -6.9606e-01 - lr: 1.2500e-06\n",
      "Epoch 50/150\n",
      "193/193 - 71s - loss: -7.0213e-01 - val_loss: -7.1922e-01 - lr: 1.2500e-06\n",
      "Epoch 51/150\n",
      "193/193 - 71s - loss: -7.0155e-01 - val_loss: -7.2475e-01 - lr: 1.2500e-06\n",
      "Epoch 52/150\n",
      "193/193 - 71s - loss: -7.0154e-01 - val_loss: -7.6447e-01 - lr: 1.2500e-06\n",
      "Epoch 53/150\n",
      "193/193 - 71s - loss: -7.0137e-01 - val_loss: -7.3903e-01 - lr: 1.2500e-06\n",
      "Epoch 54/150\n",
      "193/193 - 71s - loss: -7.0091e-01 - val_loss: -6.9153e-01 - lr: 1.2500e-06\n",
      "Epoch 55/150\n",
      "193/193 - 71s - loss: -7.0036e-01 - val_loss: -7.1948e-01 - lr: 1.2500e-06\n",
      "Epoch 56/150\n",
      "193/193 - 72s - loss: -7.0290e-01 - val_loss: -7.2275e-01 - lr: 1.2500e-06\n",
      "Epoch 57/150\n",
      "193/193 - 70s - loss: -7.0466e-01 - val_loss: -7.2059e-01 - lr: 1.2500e-06\n",
      "Epoch 58/150\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-07.\n",
      "193/193 - 72s - loss: -7.0246e-01 - val_loss: -7.4294e-01 - lr: 1.2500e-06\n",
      "Epoch 59/150\n",
      "193/193 - 70s - loss: -7.0366e-01 - val_loss: -7.4179e-01 - lr: 6.2500e-07\n",
      "Epoch 60/150\n",
      "193/193 - 72s - loss: -7.0209e-01 - val_loss: -7.3093e-01 - lr: 6.2500e-07\n",
      "Epoch 61/150\n",
      "193/193 - 70s - loss: -7.0306e-01 - val_loss: -7.4517e-01 - lr: 6.2500e-07\n",
      "Epoch 62/150\n",
      "193/193 - 72s - loss: -7.0363e-01 - val_loss: -6.7120e-01 - lr: 6.2500e-07\n",
      "Epoch 63/150\n",
      "193/193 - 71s - loss: -7.0468e-01 - val_loss: -7.1430e-01 - lr: 6.2500e-07\n",
      "Epoch 64/150\n",
      "193/193 - 72s - loss: -7.0540e-01 - val_loss: -7.5432e-01 - lr: 6.2500e-07\n",
      "Epoch 65/150\n",
      "193/193 - 71s - loss: -7.0379e-01 - val_loss: -7.1107e-01 - lr: 6.2500e-07\n",
      "Epoch 66/150\n",
      "193/193 - 71s - loss: -7.0461e-01 - val_loss: -7.2218e-01 - lr: 6.2500e-07\n",
      "Epoch 67/150\n",
      "193/193 - 71s - loss: -7.0658e-01 - val_loss: -7.1998e-01 - lr: 6.2500e-07\n",
      "Epoch 68/150\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-07.\n",
      "193/193 - 71s - loss: -7.0404e-01 - val_loss: -7.2340e-01 - lr: 6.2500e-07\n",
      "Epoch 69/150\n",
      "193/193 - 71s - loss: -7.0464e-01 - val_loss: -7.2895e-01 - lr: 3.1250e-07\n",
      "Epoch 70/150\n",
      "193/193 - 71s - loss: -7.0865e-01 - val_loss: -6.7469e-01 - lr: 3.1250e-07\n",
      "Epoch 71/150\n",
      "193/193 - 71s - loss: -7.0573e-01 - val_loss: -7.5308e-01 - lr: 3.1250e-07\n",
      "Epoch 72/150\n",
      "193/193 - 71s - loss: -7.0872e-01 - val_loss: -7.1718e-01 - lr: 3.1250e-07\n",
      "Epoch 73/150\n",
      "193/193 - 71s - loss: -7.0093e-01 - val_loss: -7.2516e-01 - lr: 3.1250e-07\n",
      "Epoch 74/150\n",
      "193/193 - 71s - loss: -7.0448e-01 - val_loss: -7.2355e-01 - lr: 3.1250e-07\n",
      "Epoch 75/150\n",
      "193/193 - 72s - loss: -7.0395e-01 - val_loss: -7.0228e-01 - lr: 3.1250e-07\n",
      "Epoch 76/150\n",
      "193/193 - 71s - loss: -7.0847e-01 - val_loss: -7.8572e-01 - lr: 3.1250e-07\n",
      "Epoch 77/150\n",
      "193/193 - 72s - loss: -7.0468e-01 - val_loss: -7.1480e-01 - lr: 3.1250e-07\n",
      "Epoch 78/150\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-07.\n",
      "193/193 - 71s - loss: -7.0416e-01 - val_loss: -7.1095e-01 - lr: 3.1250e-07\n",
      "Epoch 79/150\n",
      "193/193 - 72s - loss: -7.0574e-01 - val_loss: -7.4914e-01 - lr: 1.5625e-07\n",
      "Epoch 80/150\n",
      "193/193 - 70s - loss: -7.0501e-01 - val_loss: -6.8564e-01 - lr: 1.5625e-07\n",
      "Epoch 81/150\n",
      "193/193 - 72s - loss: -7.0383e-01 - val_loss: -7.2380e-01 - lr: 1.5625e-07\n",
      "Epoch 82/150\n",
      "193/193 - 71s - loss: -7.0365e-01 - val_loss: -7.0017e-01 - lr: 1.5625e-07\n",
      "Epoch 83/150\n",
      "193/193 - 72s - loss: -7.0454e-01 - val_loss: -7.2293e-01 - lr: 1.5625e-07\n",
      "Epoch 84/150\n",
      "193/193 - 71s - loss: -7.0614e-01 - val_loss: -7.0400e-01 - lr: 1.5625e-07\n",
      "Epoch 85/150\n",
      "193/193 - 72s - loss: -7.0503e-01 - val_loss: -7.4242e-01 - lr: 1.5625e-07\n",
      "Epoch 86/150\n",
      "193/193 - 71s - loss: -7.0424e-01 - val_loss: -7.2617e-01 - lr: 1.5625e-07\n",
      "Epoch 87/150\n",
      "193/193 - 71s - loss: -7.0525e-01 - val_loss: -7.3211e-01 - lr: 1.5625e-07\n",
      "Epoch 88/150\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 7.81249980263965e-08.\n",
      "193/193 - 71s - loss: -7.0775e-01 - val_loss: -6.9590e-01 - lr: 1.5625e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/150\n",
      "193/193 - 71s - loss: -7.0832e-01 - val_loss: -7.1085e-01 - lr: 7.8125e-08\n",
      "Epoch 90/150\n",
      "193/193 - 71s - loss: -7.0273e-01 - val_loss: -7.5657e-01 - lr: 7.8125e-08\n",
      "Epoch 91/150\n",
      "193/193 - 71s - loss: -7.0528e-01 - val_loss: -7.0905e-01 - lr: 7.8125e-08\n",
      "Epoch 92/150\n",
      "193/193 - 71s - loss: -7.0440e-01 - val_loss: -7.2356e-01 - lr: 7.8125e-08\n",
      "Epoch 93/150\n",
      "193/193 - 71s - loss: -7.0025e-01 - val_loss: -7.2803e-01 - lr: 7.8125e-08\n",
      "Epoch 94/150\n",
      "193/193 - 71s - loss: -7.0423e-01 - val_loss: -6.9410e-01 - lr: 7.8125e-08\n",
      "Epoch 95/150\n",
      "193/193 - 71s - loss: -7.0739e-01 - val_loss: -7.5056e-01 - lr: 7.8125e-08\n",
      "Epoch 96/150\n",
      "193/193 - 71s - loss: -7.0585e-01 - val_loss: -7.2443e-01 - lr: 7.8125e-08\n",
      "Epoch 97/150\n",
      "193/193 - 71s - loss: -7.0386e-01 - val_loss: -7.3391e-01 - lr: 7.8125e-08\n",
      "Epoch 98/150\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 3.906249901319825e-08.\n",
      "193/193 - 72s - loss: -7.0481e-01 - val_loss: -6.7164e-01 - lr: 7.8125e-08\n",
      "Epoch 99/150\n",
      "193/193 - 71s - loss: -7.0270e-01 - val_loss: -6.9496e-01 - lr: 3.9062e-08\n",
      "Epoch 100/150\n",
      "193/193 - 72s - loss: -7.0740e-01 - val_loss: -7.0011e-01 - lr: 3.9062e-08\n",
      "Epoch 101/150\n",
      "193/193 - 70s - loss: -7.0694e-01 - val_loss: -7.3815e-01 - lr: 3.9062e-08\n",
      "Epoch 102/150\n",
      "193/193 - 72s - loss: -7.0461e-01 - val_loss: -7.1848e-01 - lr: 3.9062e-08\n",
      "Epoch 103/150\n",
      "193/193 - 70s - loss: -7.0570e-01 - val_loss: -6.9896e-01 - lr: 3.9062e-08\n",
      "Epoch 104/150\n",
      "193/193 - 72s - loss: -7.0387e-01 - val_loss: -7.4488e-01 - lr: 3.9062e-08\n",
      "Epoch 105/150\n",
      "193/193 - 70s - loss: -7.0816e-01 - val_loss: -7.3245e-01 - lr: 3.9062e-08\n",
      "Epoch 106/150\n",
      "193/193 - 72s - loss: -7.0290e-01 - val_loss: -7.2267e-01 - lr: 3.9062e-08\n",
      "Epoch 107/150\n",
      "193/193 - 71s - loss: -7.0588e-01 - val_loss: -7.5132e-01 - lr: 3.9062e-08\n",
      "Epoch 108/150\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1.9531249506599124e-08.\n",
      "193/193 - 72s - loss: -7.0701e-01 - val_loss: -7.0844e-01 - lr: 3.9062e-08\n",
      "Epoch 109/150\n",
      "193/193 - 71s - loss: -7.0663e-01 - val_loss: -7.2920e-01 - lr: 1.9531e-08\n",
      "Epoch 110/150\n",
      "193/193 - 71s - loss: -7.0506e-01 - val_loss: -7.1790e-01 - lr: 1.9531e-08\n",
      "Epoch 111/150\n",
      "193/193 - 71s - loss: -7.0522e-01 - val_loss: -7.3847e-01 - lr: 1.9531e-08\n",
      "Epoch 112/150\n",
      "193/193 - 72s - loss: -7.0508e-01 - val_loss: -6.3758e-01 - lr: 1.9531e-08\n",
      "Epoch 113/150\n",
      "193/193 - 71s - loss: -7.0302e-01 - val_loss: -7.3661e-01 - lr: 1.9531e-08\n",
      "Epoch 114/150\n",
      "193/193 - 71s - loss: -7.0908e-01 - val_loss: -7.2436e-01 - lr: 1.9531e-08\n",
      "Epoch 115/150\n",
      "193/193 - 71s - loss: -7.0116e-01 - val_loss: -7.3305e-01 - lr: 1.9531e-08\n",
      "Epoch 116/150\n",
      "193/193 - 71s - loss: -7.0719e-01 - val_loss: -7.2648e-01 - lr: 1.9531e-08\n",
      "Epoch 117/150\n",
      "193/193 - 71s - loss: -7.0287e-01 - val_loss: -7.4166e-01 - lr: 1.9531e-08\n",
      "Epoch 118/150\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 9.765624753299562e-09.\n",
      "193/193 - 71s - loss: -7.0706e-01 - val_loss: -7.4615e-01 - lr: 1.9531e-08\n",
      "Epoch 119/150\n",
      "193/193 - 71s - loss: -7.0674e-01 - val_loss: -6.8017e-01 - lr: 9.7656e-09\n",
      "Epoch 120/150\n",
      "193/193 - 71s - loss: -7.0543e-01 - val_loss: -7.1476e-01 - lr: 9.7656e-09\n",
      "Epoch 121/150\n",
      "193/193 - 71s - loss: -7.0449e-01 - val_loss: -7.3235e-01 - lr: 9.7656e-09\n",
      "Epoch 122/150\n",
      "193/193 - 71s - loss: -7.0551e-01 - val_loss: -6.9002e-01 - lr: 9.7656e-09\n",
      "Epoch 123/150\n",
      "193/193 - 72s - loss: -7.0603e-01 - val_loss: -7.1348e-01 - lr: 9.7656e-09\n",
      "Epoch 124/150\n",
      "193/193 - 70s - loss: -7.0626e-01 - val_loss: -7.3384e-01 - lr: 9.7656e-09\n",
      "Epoch 125/150\n",
      "193/193 - 72s - loss: -7.0451e-01 - val_loss: -7.3232e-01 - lr: 9.7656e-09\n",
      "Epoch 126/150\n",
      "193/193 - 70s - loss: -7.0632e-01 - val_loss: -7.1199e-01 - lr: 9.7656e-09\n",
      "Epoch 127/150\n",
      "193/193 - 72s - loss: -7.0557e-01 - val_loss: -7.0934e-01 - lr: 9.7656e-09\n",
      "Epoch 128/150\n",
      "\n",
      "Epoch 00128: ReduceLROnPlateau reducing learning rate to 4.882812376649781e-09.\n",
      "193/193 - 70s - loss: -7.0759e-01 - val_loss: -7.5287e-01 - lr: 9.7656e-09\n",
      "Epoch 129/150\n",
      "193/193 - 72s - loss: -7.0390e-01 - val_loss: -6.9864e-01 - lr: 4.8828e-09\n",
      "Epoch 130/150\n",
      "193/193 - 71s - loss: -7.0521e-01 - val_loss: -7.1541e-01 - lr: 4.8828e-09\n",
      "Epoch 131/150\n",
      "193/193 - 72s - loss: -7.0824e-01 - val_loss: -6.5783e-01 - lr: 4.8828e-09\n",
      "Epoch 132/150\n",
      "193/193 - 71s - loss: -7.0416e-01 - val_loss: -7.4872e-01 - lr: 4.8828e-09\n",
      "Epoch 133/150\n",
      "193/193 - 72s - loss: -7.0337e-01 - val_loss: -7.3456e-01 - lr: 4.8828e-09\n",
      "Epoch 134/150\n",
      "193/193 - 71s - loss: -7.0437e-01 - val_loss: -7.2272e-01 - lr: 4.8828e-09\n",
      "Epoch 135/150\n",
      "193/193 - 72s - loss: -7.0862e-01 - val_loss: -7.4741e-01 - lr: 4.8828e-09\n",
      "Epoch 136/150\n",
      "193/193 - 71s - loss: -7.0559e-01 - val_loss: -6.9003e-01 - lr: 4.8828e-09\n",
      "Epoch 137/150\n",
      "193/193 - 71s - loss: -7.0730e-01 - val_loss: -7.3169e-01 - lr: 4.8828e-09\n",
      "Epoch 138/150\n",
      "\n",
      "Epoch 00138: ReduceLROnPlateau reducing learning rate to 2.4414061883248905e-09.\n",
      "193/193 - 71s - loss: -7.0340e-01 - val_loss: -7.0049e-01 - lr: 4.8828e-09\n",
      "Epoch 139/150\n",
      "193/193 - 71s - loss: -7.0813e-01 - val_loss: -7.3698e-01 - lr: 2.4414e-09\n",
      "Epoch 140/150\n",
      "193/193 - 71s - loss: -7.0353e-01 - val_loss: -7.5999e-01 - lr: 2.4414e-09\n",
      "Epoch 141/150\n",
      "193/193 - 71s - loss: -7.0692e-01 - val_loss: -7.1926e-01 - lr: 2.4414e-09\n",
      "Epoch 142/150\n",
      "193/193 - 71s - loss: -7.0765e-01 - val_loss: -7.4820e-01 - lr: 2.4414e-09\n",
      "Epoch 143/150\n",
      "193/193 - 71s - loss: -7.0598e-01 - val_loss: -7.5133e-01 - lr: 2.4414e-09\n",
      "Epoch 144/150\n",
      "193/193 - 71s - loss: -7.0254e-01 - val_loss: -7.1329e-01 - lr: 2.4414e-09\n",
      "Epoch 145/150\n",
      "193/193 - 71s - loss: -7.0319e-01 - val_loss: -7.3579e-01 - lr: 2.4414e-09\n",
      "Epoch 146/150\n",
      "193/193 - 72s - loss: -7.0598e-01 - val_loss: -7.3129e-01 - lr: 2.4414e-09\n",
      "Epoch 147/150\n",
      "193/193 - 71s - loss: -7.0569e-01 - val_loss: -7.0766e-01 - lr: 2.4414e-09\n",
      "Epoch 148/150\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 1.2207030941624453e-09.\n",
      "193/193 - 72s - loss: -7.0463e-01 - val_loss: -7.2312e-01 - lr: 2.4414e-09\n",
      "Epoch 149/150\n",
      "193/193 - 71s - loss: -7.0135e-01 - val_loss: -7.4650e-01 - lr: 1.2207e-09\n",
      "Epoch 150/150\n",
      "193/193 - 72s - loss: -7.0548e-01 - val_loss: -7.2324e-01 - lr: 1.2207e-09\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loss_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3d3b027bfe6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mlearning_rate_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m    )\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mval_loss_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/nidebroux/parameters_analysis/final_test.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_loss_' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_3Dpatches(fname_lst=training_files,patch_shape=config[\"patch_size\"],overlap=6) \n",
    "X_train = normalize_data(X_train, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=2611)\n",
    "\n",
    "X_valid, y_valid = load_3Dpatches(fname_lst=validation_files,\n",
    "                                    patch_shape=config[\"patch_size\"],\n",
    "                                    overlap=0)\n",
    "X_valid = normalize_data(X_valid, MEAN_TRAIN_T2, STD_TRAIN_T2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator, nb_train_steps = get_training_and_validation_generators(\n",
    "                                                [X_train, y_train],\n",
    "                                                batch_size=5,\n",
    "                                                augment=True,\n",
    "                                                augment_flip=True)\n",
    "\n",
    "\n",
    "validation_generator, nb_valid_steps = get_training_and_validation_generators(\n",
    "                                                [X_valid, y_valid],\n",
    "                                                batch_size=2,\n",
    "                                                augment=False,\n",
    "                                                augment_flip=False)\n",
    "\n",
    "init = tf.keras.initializers.VarianceScaling(scale = 1.0, mode = 'fan_avg', distribution = 'uniform', seed= None)\n",
    "long_1 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_1',padding = 'same',kernel_initializer = init)(model.layers[-9].output)\n",
    "long_2 = keras.layers.BatchNormalization(name = 'long_batch_1')(long_1)\n",
    "long_3 = keras.layers.Activation('relu',name = 'long_activation_1')(long_2)\n",
    "long_4 = keras.layers.Conv3D(48,kernel_size=(3,3,3),activation='linear',data_format = 'channels_first', name = 'long_conv3D_2',padding = 'same',kernel_initializer = init)(long_3)\n",
    "long_5 = keras.layers.BatchNormalization(name = 'long_batch_2')(long_4)\n",
    "long_6 = keras.layers.Activation('relu',name = 'long_activation_2')(long_5)\n",
    "long_7 = keras.layers.Conv3D(1,kernel_size=(1,1,1),activation='linear',data_format = 'channels_first', name = 'long_conv3D_3',kernel_initializer = init)(long_6)\n",
    "long_output = keras.layers.Activation('sigmoid',name = 'long_activation_3')(long_7)\n",
    "\n",
    "long_model = keras.models.Model(model.input,long_output,name = \"long_surgery_model\")\n",
    "\n",
    "for layer in long_model.layers[:-9]:\n",
    "    layer.trainable = False\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "\n",
    "K.set_value(long_model.optimizer.learning_rate,0.001)\n",
    "\n",
    "#fit the long model\n",
    "history = train_model(model=long_model,\n",
    "        path2save=config[\"path2save_transferlearning\"],\n",
    "        model_name='final_test',\n",
    "        training_generator=train_generator,\n",
    "        validation_generator=validation_generator,\n",
    "        steps_per_epoch=nb_train_steps,\n",
    "        validation_steps=nb_valid_steps,\n",
    "        n_epochs=75,\n",
    "        learning_rate_drop=0.5,\n",
    "        learning_rate_patience=10\n",
    "       )\n",
    "\n",
    "val_loss[0,:75] = history.history['val_loss']\n",
    "\n",
    "for layer in long_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "long_model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "K.set_value(long_model.optimizer.learning_rate,0.00001)\n",
    "\n",
    "history = train_model(model=long_model,\n",
    "    path2save=config[\"path2save_finetuned\"],\n",
    "    model_name=\"final_test\",\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=75,\n",
    "    learning_rate_drop=0.5,\n",
    "    learning_rate_patience=10\n",
    "   )\n",
    "val_loss[0,75:] = history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = load_trained_model(main_dir+'sct_custom/data/deepseg_sc_models/t2_sc_3D.h5')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model.compile(optimizer = optimizer_class_name, loss=loss)\n",
    "K.set_value(model.optimizer.learning_rate,0.00001)\n",
    "print(model.summary())\n",
    "\n",
    "history = train_model(model=model,\n",
    "    path2save=config[\"path2save_retrained\"],\n",
    "    model_name=\"final_test\",\n",
    "    training_generator=train_generator,\n",
    "    validation_generator=validation_generator,\n",
    "    steps_per_epoch=nb_train_steps,\n",
    "    validation_steps=nb_valid_steps,\n",
    "    n_epochs=150,\n",
    "    learning_rate_drop=0.5,\n",
    "    learning_rate_patience=10\n",
    "   )\n",
    "val_loss[1,:] = history.history['val_loss']\n",
    "\n",
    "np.save(main_dir+\"parameters_analysis/final_test.npy\",val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = np.load(main_dir+\"parameters_analysis/final_test.npy\")\n",
    "epochs = range(5,151,5)\n",
    "mean_val_loss = np.zeros((2,30))\n",
    "for i in range(30):\n",
    "    mean_val_loss[0,i]=np.abs(np.mean(val_loss[0,i:i+5])) \n",
    "    mean_val_loss[1,i]=np.abs(np.mean(val_loss[1,i:i+5]))\n",
    "\n",
    "plt.plot(epochs,mean_val_loss[0,:],'o--',label = 'transfer learning')\n",
    "plt.plot(epochs,mean_val_loss[1,:],'o--',label = 'no  transfer learning')\n",
    "\n",
    "plt.title('Evolution of the validation loss with or without transfer learning ')\n",
    "# show a legend on the plot\n",
    "plt.ylabel('Mean of the validation loss on 5 successive epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(loc = 'lower right')\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
